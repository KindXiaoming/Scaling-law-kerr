{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Epoch: 0 | Loss: 0.4259971752205497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ziming/opt/anaconda3/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1639180852547/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | Loss: 0.0010220951537800549\n",
      "Epoch: 200 | Loss: 0.000386381431285062\n",
      "Epoch: 300 | Loss: 0.00023559945611537054\n",
      "Epoch: 400 | Loss: 0.00017480753418804416\n",
      "Epoch: 500 | Loss: 0.00011149080926617789\n",
      "Epoch: 600 | Loss: 7.974568541872892e-05\n",
      "Epoch: 700 | Loss: 7.727414620086964e-05\n",
      "Epoch: 800 | Loss: 4.368148405515226e-05\n",
      "Epoch: 900 | Loss: 0.0009706463485943603\n",
      "Epoch: 1000 | Loss: 6.808741024383334e-05\n",
      "Epoch: 1100 | Loss: 1.9355143280954128e-05\n",
      "Epoch: 1200 | Loss: 2.009124764315332e-05\n",
      "Epoch: 1300 | Loss: 1.3622711271797847e-05\n",
      "Epoch: 1400 | Loss: 1.2209494131945523e-05\n",
      "Epoch: 1500 | Loss: 6.174126286794492e-05\n",
      "Epoch: 1600 | Loss: 8.706309322099841e-06\n",
      "Epoch: 1700 | Loss: 6.167571481685147e-05\n",
      "Epoch: 1800 | Loss: 1.1492488415443763e-05\n",
      "Epoch: 1900 | Loss: 5.185750414804818e-06\n",
      "Epoch: 2000 | Loss: 2.2512874759217465e-05\n",
      "Epoch: 2100 | Loss: 0.00013139354260093255\n",
      "Epoch: 2200 | Loss: 3.856046703693518e-06\n",
      "Epoch: 2300 | Loss: 0.00015176224129924857\n",
      "Epoch: 2400 | Loss: 4.509340360187743e-05\n",
      "Epoch: 2500 | Loss: 0.00015190884236274767\n",
      "Epoch: 2600 | Loss: 0.0005314765341864343\n",
      "Epoch: 2700 | Loss: 3.771269812872126e-06\n",
      "Epoch: 2800 | Loss: 1.6599602380865256e-05\n",
      "Epoch: 2900 | Loss: 2.0339718621258504e-05\n",
      "Epoch: 3000 | Loss: 3.5234362685801234e-06\n",
      "Epoch: 3100 | Loss: 3.049567051804735e-06\n",
      "Epoch: 3200 | Loss: 2.7215436684761965e-06\n",
      "Epoch: 3300 | Loss: 2.480009476297253e-06\n",
      "Epoch: 3400 | Loss: 2.299328335014512e-06\n",
      "Epoch: 3500 | Loss: 2.162620974203033e-06\n",
      "Epoch: 3600 | Loss: 2.058345043157617e-06\n",
      "Epoch: 3700 | Loss: 1.9783697153631526e-06\n",
      "Epoch: 3800 | Loss: 1.9168252245302935e-06\n",
      "Epoch: 3900 | Loss: 1.8693874577516992e-06\n",
      "Epoch: 4000 | Loss: 1.8328176461899746e-06\n",
      "Epoch: 4100 | Loss: 1.8046571718503953e-06\n",
      "Epoch: 4200 | Loss: 1.7830197307104613e-06\n",
      "Epoch: 4300 | Loss: 1.7664463352756038e-06\n",
      "Epoch: 4400 | Loss: 1.7554380402953986e-06\n",
      "Epoch: 4500 | Loss: 1.9703211089710385e-06\n",
      "Epoch: 4600 | Loss: 2.2455404178600827e-06\n",
      "Epoch: 4700 | Loss: 1.9249323818671047e-06\n",
      "Epoch: 4800 | Loss: 4.762389929797077e-05\n",
      "Epoch: 4900 | Loss: 1.8418772254621534e-06\n",
      "Epoch: 5000 | Loss: 1.7959005401464934e-06\n",
      "Epoch: 5100 | Loss: 8.18338349558437e-06\n",
      "Epoch: 5200 | Loss: 1.880505575596766e-06\n",
      "Epoch: 5300 | Loss: 5.1686091196081936e-05\n",
      "Epoch: 5400 | Loss: 1.8454370125598741e-06\n",
      "Epoch: 5500 | Loss: 6.684871715287223e-06\n",
      "Epoch: 5600 | Loss: 0.00035440324282977114\n",
      "Epoch: 5700 | Loss: 0.00017011982505354555\n",
      "Epoch: 5800 | Loss: 1.8579035848979048e-06\n",
      "Epoch: 5900 | Loss: 2.298609907859896e-06\n",
      "Epoch: 6000 | Loss: 1.976463419735188e-06\n",
      "Epoch: 6100 | Loss: 1.887442584809902e-06\n",
      "Epoch: 6200 | Loss: 1.8317947820193898e-06\n",
      "Epoch: 6300 | Loss: 1.7950034150738553e-06\n",
      "Epoch: 6400 | Loss: 1.770319378960947e-06\n",
      "Epoch: 6500 | Loss: 1.7536015836370543e-06\n",
      "Epoch: 6600 | Loss: 1.7422094524576043e-06\n",
      "Epoch: 6700 | Loss: 1.7344167997657838e-06\n",
      "Epoch: 6800 | Loss: 1.7290742915899217e-06\n",
      "Epoch: 6900 | Loss: 1.725406445173239e-06\n",
      "Epoch: 7000 | Loss: 1.7228851837682506e-06\n",
      "Epoch: 7100 | Loss: 1.7211487802406012e-06\n",
      "Epoch: 7200 | Loss: 1.7199486757236373e-06\n",
      "Epoch: 7300 | Loss: 1.7191139403274807e-06\n",
      "Epoch: 7400 | Loss: 1.7185271847143021e-06\n",
      "Epoch: 7500 | Loss: 1.720284156213413e-06\n",
      "Epoch: 7600 | Loss: 1.7515768594300758e-06\n",
      "Epoch: 7700 | Loss: 2.8404082681769904e-06\n",
      "Epoch: 7800 | Loss: 1.738968131666141e-06\n",
      "Epoch: 7900 | Loss: 1.8460746847710023e-06\n",
      "Epoch: 8000 | Loss: 1.72921806279534e-06\n",
      "Epoch: 8100 | Loss: 1.731885568450727e-06\n",
      "Epoch: 8200 | Loss: 2.012320700030781e-06\n",
      "Epoch: 8300 | Loss: 2.4760123595123267e-06\n",
      "Epoch: 8400 | Loss: 2.527876545147527e-06\n",
      "Epoch: 8500 | Loss: 1.7244871100715438e-06\n",
      "Epoch: 8600 | Loss: 2.129231788302607e-06\n",
      "Epoch: 8700 | Loss: 4.56352674061436e-06\n",
      "Epoch: 8800 | Loss: 3.285673700675547e-06\n",
      "Epoch: 8900 | Loss: 1.1383538215977793e-05\n",
      "Epoch: 9000 | Loss: 2.1025496974386148e-06\n",
      "Epoch: 9100 | Loss: 1.7325146716960454e-06\n",
      "Epoch: 9200 | Loss: 1.724247363205213e-06\n",
      "Epoch: 9300 | Loss: 1.7212681666934492e-06\n",
      "Epoch: 9400 | Loss: 1.7195042014896516e-06\n",
      "Epoch: 9500 | Loss: 1.7184499998090966e-06\n",
      "Epoch: 9600 | Loss: 1.7178119097053228e-06\n",
      "Epoch: 9700 | Loss: 1.7174180239632375e-06\n",
      "Epoch: 9800 | Loss: 1.7171673732676918e-06\n",
      "Epoch: 9900 | Loss: 1.717000732498575e-06\n",
      "Epoch: 10000 | Loss: 1.7168835766199582e-06\n",
      "Epoch: 10100 | Loss: 1.716795996444052e-06\n",
      "Epoch: 10200 | Loss: 1.7167266770075023e-06\n",
      "Epoch: 10300 | Loss: 1.7166692899516448e-06\n",
      "Epoch: 10400 | Loss: 1.7166203408454783e-06\n",
      "Epoch: 10500 | Loss: 1.7165779002593747e-06\n",
      "Epoch: 10600 | Loss: 1.763209154880399e-06\n",
      "Epoch: 10700 | Loss: 1.7177830958773416e-06\n",
      "Epoch: 10800 | Loss: 1.7254173801367899e-06\n",
      "Epoch: 10900 | Loss: 1.7171829308682286e-06\n",
      "Epoch: 11000 | Loss: 1.7167357362741944e-06\n",
      "Epoch: 11100 | Loss: 9.857370932458578e-06\n",
      "Epoch: 11200 | Loss: 1.7230284476294291e-06\n",
      "Epoch: 11300 | Loss: 1.7225178307657844e-06\n",
      "Epoch: 11400 | Loss: 2.731017576902827e-05\n",
      "Epoch: 11500 | Loss: 3.0808977437764713e-06\n",
      "Epoch: 11600 | Loss: 1.718450337511894e-06\n",
      "Epoch: 11700 | Loss: 1.784237672248965e-06\n",
      "Epoch: 11800 | Loss: 1.8577634086366203e-06\n",
      "Epoch: 11900 | Loss: 2.5798631486220866e-06\n",
      "Epoch: 12000 | Loss: 1.717554535692542e-06\n",
      "Epoch: 12100 | Loss: 1.7169769775907098e-06\n",
      "Epoch: 12200 | Loss: 1.7166932728448262e-06\n",
      "Epoch: 12300 | Loss: 1.7165489935965392e-06\n",
      "Epoch: 12400 | Loss: 1.7164736952115342e-06\n",
      "Epoch: 12500 | Loss: 1.7164326925978354e-06\n",
      "Epoch: 12600 | Loss: 1.7164087652203397e-06\n",
      "Epoch: 12700 | Loss: 1.7163934198903101e-06\n",
      "Epoch: 12800 | Loss: 1.7163825336628222e-06\n",
      "Epoch: 12900 | Loss: 1.7163741397706307e-06\n",
      "Epoch: 13000 | Loss: 1.7163673069688413e-06\n",
      "Epoch: 13100 | Loss: 1.716361584278932e-06\n",
      "Epoch: 13200 | Loss: 1.7163567350042174e-06\n",
      "Epoch: 13300 | Loss: 1.716352614702102e-06\n",
      "Epoch: 13400 | Loss: 1.7163491180566272e-06\n",
      "Epoch: 13500 | Loss: 1.8912664467704815e-06\n",
      "Epoch: 13600 | Loss: 1.716510633714988e-06\n",
      "Epoch: 13700 | Loss: 2.91533051762866e-06\n",
      "Epoch: 13800 | Loss: 1.7165615169975297e-06\n",
      "Epoch: 13900 | Loss: 1.7163816953931935e-06\n",
      "Epoch: 14000 | Loss: 1.7254972936460795e-06\n",
      "Epoch: 14100 | Loss: 1.7163513989945409e-06\n",
      "Epoch: 14200 | Loss: 1.7325856170323983e-06\n",
      "Epoch: 14300 | Loss: 1.7197503409702954e-06\n",
      "Epoch: 14400 | Loss: 1.7178429912901705e-06\n",
      "Epoch: 14500 | Loss: 1.8466789117447754e-06\n",
      "Epoch: 14600 | Loss: 1.7168841684031947e-06\n",
      "Epoch: 14700 | Loss: 2.251964357615882e-06\n",
      "Epoch: 14800 | Loss: 1.7390357954180012e-06\n",
      "Epoch: 14900 | Loss: 1.8273453162779637e-06\n",
      "time=182.48776721954346\n",
      "error=1.796935547795217e-06\n",
      "best loss=1.7163478296231775e-06\n",
      "best epoch=13443\n",
      "8\n",
      "Epoch: 0 | Loss: 0.43865335081685786\n",
      "Epoch: 100 | Loss: 0.01669752747689897\n",
      "Epoch: 200 | Loss: 0.008777570097084974\n",
      "Epoch: 300 | Loss: 0.001267021009506908\n",
      "Epoch: 400 | Loss: 0.0008947464180267702\n",
      "Epoch: 500 | Loss: 0.0007949005753070694\n",
      "Epoch: 600 | Loss: 0.0007584321536196785\n",
      "Epoch: 700 | Loss: 0.0005583563465902689\n",
      "Epoch: 800 | Loss: 0.0007765696593098725\n",
      "Epoch: 900 | Loss: 0.000451475595593085\n",
      "Epoch: 1000 | Loss: 0.0004502061467851397\n",
      "Epoch: 1100 | Loss: 0.000818347495654277\n",
      "Epoch: 1200 | Loss: 0.00045379599998227505\n",
      "Epoch: 1300 | Loss: 0.00033642838828412897\n",
      "Epoch: 1400 | Loss: 0.00020696629190412023\n",
      "Epoch: 1500 | Loss: 0.00015720171112529406\n",
      "Epoch: 1600 | Loss: 9.756975326786184e-05\n",
      "Epoch: 1700 | Loss: 0.00021311067284644216\n",
      "Epoch: 1800 | Loss: 0.0003068226717766518\n",
      "Epoch: 1900 | Loss: 0.000377453526065681\n",
      "Epoch: 2000 | Loss: 0.00025134228186150386\n",
      "Epoch: 2100 | Loss: 5.28428179295185e-05\n",
      "Epoch: 2200 | Loss: 0.00025029266339927697\n",
      "Epoch: 2300 | Loss: 4.447119683875461e-05\n",
      "Epoch: 2400 | Loss: 9.395894074317456e-06\n",
      "Epoch: 2500 | Loss: 0.0004157494564318643\n",
      "Epoch: 2600 | Loss: 0.0010669831120009947\n",
      "Epoch: 2700 | Loss: 0.00016627535645790978\n",
      "Epoch: 2800 | Loss: 4.901252363068769e-05\n",
      "Epoch: 2900 | Loss: 1.0919021180747028e-05\n",
      "Epoch: 3000 | Loss: 2.229734489556189e-06\n",
      "Epoch: 3100 | Loss: 2.0512354576667248e-06\n",
      "Epoch: 3200 | Loss: 1.9012066179766737e-06\n",
      "Epoch: 3300 | Loss: 1.7681279442389427e-06\n",
      "Epoch: 3400 | Loss: 1.6499956621291344e-06\n",
      "Epoch: 3500 | Loss: 1.5451419882254172e-06\n",
      "Epoch: 3600 | Loss: 1.4521173983705543e-06\n",
      "Epoch: 3700 | Loss: 1.3696494104678725e-06\n",
      "Epoch: 3800 | Loss: 1.2966137756081185e-06\n",
      "Epoch: 3900 | Loss: 1.2320122366110315e-06\n",
      "Epoch: 4000 | Loss: 1.1749547920992181e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4100 | Loss: 1.1246452892903503e-06\n",
      "Epoch: 4200 | Loss: 1.080369564380166e-06\n",
      "Epoch: 4300 | Loss: 1.0414855757303848e-06\n",
      "Epoch: 4400 | Loss: 1.0074151292435978e-06\n",
      "Epoch: 4500 | Loss: 2.2762087062004787e-06\n",
      "Epoch: 4600 | Loss: 5.206354923720462e-05\n",
      "Epoch: 4700 | Loss: 1.0277604748825856e-06\n",
      "Epoch: 4800 | Loss: 1.1276999373365447e-06\n",
      "Epoch: 4900 | Loss: 1.8187955869605944e-06\n",
      "Epoch: 5000 | Loss: 7.990523015997264e-05\n",
      "Epoch: 5100 | Loss: 2.7745843711899725e-05\n",
      "Epoch: 5200 | Loss: 1.8472551615811963e-05\n",
      "Epoch: 5300 | Loss: 1.988411183071097e-06\n",
      "Epoch: 5400 | Loss: 4.612557310684391e-06\n",
      "Epoch: 5500 | Loss: 4.297034467997454e-05\n",
      "Epoch: 5600 | Loss: 1.1246658248471332e-05\n",
      "Epoch: 5700 | Loss: 5.548055910662767e-06\n",
      "Epoch: 5800 | Loss: 2.3332989902154e-05\n",
      "Epoch: 5900 | Loss: 1.403696987333614e-05\n",
      "Epoch: 6000 | Loss: 1.0559299826429576e-06\n",
      "Epoch: 6100 | Loss: 8.826679249851817e-07\n",
      "Epoch: 6200 | Loss: 8.706130095135318e-07\n",
      "Epoch: 6300 | Loss: 8.604383453975868e-07\n",
      "Epoch: 6400 | Loss: 8.516574496326063e-07\n",
      "Epoch: 6500 | Loss: 8.440907411658682e-07\n",
      "Epoch: 6600 | Loss: 8.375825593367828e-07\n",
      "Epoch: 6700 | Loss: 8.319972656072981e-07\n",
      "Epoch: 6800 | Loss: 8.272161995223035e-07\n",
      "Epoch: 6900 | Loss: 8.2313523554546e-07\n",
      "Epoch: 7000 | Loss: 8.196628004349649e-07\n",
      "Epoch: 7100 | Loss: 8.167182474819755e-07\n",
      "Epoch: 7200 | Loss: 8.142305088903715e-07\n",
      "Epoch: 7300 | Loss: 8.121369660367749e-07\n",
      "Epoch: 7400 | Loss: 8.103824911741466e-07\n",
      "Epoch: 7500 | Loss: 2.177595238840981e-06\n",
      "Epoch: 7600 | Loss: 2.575711926127677e-05\n",
      "Epoch: 7700 | Loss: 8.154144300420733e-07\n",
      "Epoch: 7800 | Loss: 1.4011903939953082e-06\n",
      "Epoch: 7900 | Loss: 1.6573267425202685e-05\n",
      "Epoch: 8000 | Loss: 1.1916260053927623e-05\n",
      "Epoch: 8100 | Loss: 3.149004309852146e-06\n",
      "Epoch: 8200 | Loss: 8.245812370643137e-07\n",
      "Epoch: 8300 | Loss: 1.4044032569820115e-06\n",
      "Epoch: 8400 | Loss: 2.0573675878946614e-06\n",
      "Epoch: 8500 | Loss: 9.035554351438382e-06\n",
      "Epoch: 8600 | Loss: 1.3256861162565431e-05\n",
      "Epoch: 8700 | Loss: 3.499995029849858e-05\n",
      "Epoch: 8800 | Loss: 1.8777991151244617e-06\n",
      "Epoch: 8900 | Loss: 1.600240019245096e-05\n",
      "Epoch: 9000 | Loss: 1.0102749913491973e-06\n",
      "Epoch: 9100 | Loss: 8.105833304360733e-07\n",
      "Epoch: 9200 | Loss: 8.089360515734562e-07\n",
      "Epoch: 9300 | Loss: 8.076780647787939e-07\n",
      "Epoch: 9400 | Loss: 8.066496666601253e-07\n",
      "Epoch: 9500 | Loss: 8.05812322946233e-07\n",
      "Epoch: 9600 | Loss: 8.051335168682708e-07\n",
      "Epoch: 9700 | Loss: 8.045858197559788e-07\n",
      "Epoch: 9800 | Loss: 8.041461279048675e-07\n",
      "Epoch: 9900 | Loss: 8.037950211426605e-07\n",
      "Epoch: 10000 | Loss: 8.035162188594702e-07\n",
      "Epoch: 10100 | Loss: 8.032961157921342e-07\n",
      "Epoch: 10200 | Loss: 8.03123384228375e-07\n",
      "Epoch: 10300 | Loss: 8.029886323571885e-07\n",
      "Epoch: 10400 | Loss: 8.028841106045316e-07\n",
      "Epoch: 10500 | Loss: 8.028152030189434e-07\n",
      "Epoch: 10600 | Loss: 8.202638476322017e-07\n",
      "Epoch: 10700 | Loss: 8.349623076094449e-07\n",
      "Epoch: 10800 | Loss: 8.164267016295616e-07\n",
      "Epoch: 10900 | Loss: 8.45500085918327e-07\n",
      "Epoch: 11000 | Loss: 2.043124823234091e-06\n",
      "Epoch: 11100 | Loss: 1.2724860338933645e-06\n",
      "Epoch: 11200 | Loss: 9.78519079814599e-07\n",
      "Epoch: 11300 | Loss: 2.254270445822148e-06\n",
      "Epoch: 11400 | Loss: 2.7687071485485296e-06\n",
      "Epoch: 11500 | Loss: 6.891858793955974e-06\n",
      "Epoch: 11600 | Loss: 3.656065219655677e-06\n",
      "Epoch: 11700 | Loss: 1.1254071655970628e-06\n",
      "Epoch: 11800 | Loss: 2.5007533465803067e-06\n",
      "Epoch: 11900 | Loss: 2.4273177760398103e-06\n",
      "Epoch: 12000 | Loss: 8.158227262977546e-07\n",
      "Epoch: 12100 | Loss: 8.031303602535082e-07\n",
      "Epoch: 12200 | Loss: 8.029636904916258e-07\n",
      "Epoch: 12300 | Loss: 8.028476032810091e-07\n",
      "Epoch: 12400 | Loss: 8.027598219280415e-07\n",
      "Epoch: 12500 | Loss: 8.02693673782166e-07\n",
      "Epoch: 12600 | Loss: 8.026440016290633e-07\n",
      "Epoch: 12700 | Loss: 8.026068131177332e-07\n",
      "Epoch: 12800 | Loss: 8.025790186146774e-07\n",
      "Epoch: 12900 | Loss: 8.025582331415173e-07\n",
      "Epoch: 13000 | Loss: 8.025426249722973e-07\n",
      "Epoch: 13100 | Loss: 8.025307981234534e-07\n",
      "Epoch: 13200 | Loss: 8.025216996597884e-07\n",
      "Epoch: 13300 | Loss: 8.025145455475732e-07\n",
      "Epoch: 13400 | Loss: 8.025087608059955e-07\n",
      "Epoch: 13500 | Loss: 3.0932340546023247e-06\n",
      "Epoch: 13600 | Loss: 8.036485093838924e-07\n",
      "Epoch: 13700 | Loss: 8.025316071075405e-07\n",
      "Epoch: 13800 | Loss: 8.088889956855741e-07\n",
      "Epoch: 13900 | Loss: 8.068175305726584e-07\n",
      "Epoch: 14000 | Loss: 9.508894874479908e-07\n",
      "Epoch: 14100 | Loss: 8.927918818371447e-07\n",
      "Epoch: 14200 | Loss: 8.856215507377728e-07\n",
      "Epoch: 14300 | Loss: 9.094895510125702e-07\n",
      "Epoch: 14400 | Loss: 8.374985792844513e-07\n",
      "Epoch: 14500 | Loss: 1.0847108166672873e-06\n",
      "Epoch: 14600 | Loss: 8.032514033441889e-07\n",
      "Epoch: 14700 | Loss: 8.550065231739847e-07\n",
      "Epoch: 14800 | Loss: 8.863595356518413e-07\n",
      "Epoch: 14900 | Loss: 9.218696212128977e-07\n",
      "time=184.5773787498474\n",
      "error=8.095904516666078e-07\n",
      "best loss=8.025052333048629e-07\n",
      "best epoch=13473\n",
      "12\n",
      "Epoch: 0 | Loss: 0.457438571621816\n",
      "Epoch: 100 | Loss: 0.025516667163505695\n",
      "Epoch: 200 | Loss: 0.016135087193490612\n",
      "Epoch: 300 | Loss: 0.01330544175154559\n",
      "Epoch: 400 | Loss: 0.011694609703998887\n",
      "Epoch: 500 | Loss: 0.01119315263893828\n",
      "Epoch: 600 | Loss: 0.011010911019288461\n",
      "Epoch: 700 | Loss: 0.010849208222670907\n",
      "Epoch: 800 | Loss: 0.010831234143276018\n",
      "Epoch: 900 | Loss: 0.010682677724344985\n",
      "Epoch: 1000 | Loss: 0.01051634427917612\n",
      "Epoch: 1100 | Loss: 0.010509768301551897\n",
      "Epoch: 1200 | Loss: 0.01045552536832467\n",
      "Epoch: 1300 | Loss: 0.010292121093701541\n",
      "Epoch: 1400 | Loss: 0.010184584432429054\n",
      "Epoch: 1500 | Loss: 0.010152930614713673\n",
      "Epoch: 1600 | Loss: 0.009939244726126473\n",
      "Epoch: 1700 | Loss: 0.009693278254290292\n",
      "Epoch: 1800 | Loss: 0.009587370581555229\n",
      "Epoch: 1900 | Loss: 0.009463071163045199\n",
      "Epoch: 2000 | Loss: 0.009299785330847733\n",
      "Epoch: 2100 | Loss: 0.009182576370310516\n",
      "Epoch: 2200 | Loss: 0.009099761731471962\n",
      "Epoch: 2300 | Loss: 0.009087441783294216\n",
      "Epoch: 2400 | Loss: 0.00891408202499128\n",
      "Epoch: 2500 | Loss: 0.008829807715275135\n",
      "Epoch: 2600 | Loss: 0.008744022602661885\n",
      "Epoch: 2700 | Loss: 0.008634205530050967\n",
      "Epoch: 2800 | Loss: 0.008650227753362445\n",
      "Epoch: 2900 | Loss: 0.008526435073002825\n",
      "Epoch: 3000 | Loss: 0.008370048449931982\n",
      "Epoch: 3100 | Loss: 0.00832800753959076\n",
      "Epoch: 3200 | Loss: 0.008285491452923874\n",
      "Epoch: 3300 | Loss: 0.008243036579383037\n",
      "Epoch: 3400 | Loss: 0.008201282859045199\n",
      "Epoch: 3500 | Loss: 0.008159888726999845\n",
      "Epoch: 3600 | Loss: 0.008117211406868085\n",
      "Epoch: 3700 | Loss: 0.008070341884718656\n",
      "Epoch: 3800 | Loss: 0.008015491289748756\n",
      "Epoch: 3900 | Loss: 0.007951529333940546\n",
      "Epoch: 4000 | Loss: 0.007885856081187227\n",
      "Epoch: 4100 | Loss: 0.007826134833232338\n",
      "Epoch: 4200 | Loss: 0.007770026865146322\n",
      "Epoch: 4300 | Loss: 0.007735846269733743\n",
      "Epoch: 4400 | Loss: 0.007634505889276371\n",
      "Epoch: 4500 | Loss: 0.007564283604166287\n",
      "Epoch: 4600 | Loss: 0.0074948030433499\n",
      "Epoch: 4700 | Loss: 0.007397520006774846\n",
      "Epoch: 4800 | Loss: 0.007252965743528191\n",
      "Epoch: 4900 | Loss: 0.007268565292743593\n",
      "Epoch: 5000 | Loss: 0.006162883273802279\n",
      "Epoch: 5100 | Loss: 0.0056383265771754254\n",
      "Epoch: 5200 | Loss: 0.0047460695532873945\n",
      "Epoch: 5300 | Loss: 0.004621342756897253\n",
      "Epoch: 5400 | Loss: 0.004597987905832584\n",
      "Epoch: 5500 | Loss: 0.004584856711769127\n",
      "Epoch: 5600 | Loss: 0.004582733106113633\n",
      "Epoch: 5700 | Loss: 0.004639429367811031\n",
      "Epoch: 5800 | Loss: 0.004589665346225593\n",
      "Epoch: 5900 | Loss: 0.004585328591662705\n",
      "Epoch: 6000 | Loss: 0.0045709421496098876\n",
      "Epoch: 6100 | Loss: 0.004570336597021614\n",
      "Epoch: 6200 | Loss: 0.004569807282935488\n",
      "Epoch: 6300 | Loss: 0.004569291340995747\n",
      "Epoch: 6400 | Loss: 0.004568781513507661\n",
      "Epoch: 6500 | Loss: 0.004568270598906055\n",
      "Epoch: 6600 | Loss: 0.004567751140015165\n",
      "Epoch: 6700 | Loss: 0.004567215168133338\n",
      "Epoch: 6800 | Loss: 0.004566653938053103\n",
      "Epoch: 6900 | Loss: 0.004566057636546629\n",
      "Epoch: 7000 | Loss: 0.0045654150497940115\n",
      "Epoch: 7100 | Loss: 0.004564713178585278\n",
      "Epoch: 7200 | Loss: 0.004563936794622435\n",
      "Epoch: 7300 | Loss: 0.004563067937009663\n",
      "Epoch: 7400 | Loss: 0.004562085402588637\n",
      "Epoch: 7500 | Loss: 0.0045643945194109685\n",
      "Epoch: 7600 | Loss: 0.004577639280994011\n",
      "Epoch: 7700 | Loss: 0.004558715489576878\n",
      "Epoch: 7800 | Loss: 0.0045573244704944825\n",
      "Epoch: 7900 | Loss: 0.004559131412347985\n",
      "Epoch: 8000 | Loss: 0.00455645074114515\n",
      "Epoch: 8100 | Loss: 0.0045524110602935194\n",
      "Epoch: 8200 | Loss: 0.004550319647800907\n",
      "Epoch: 8300 | Loss: 0.004553586643390341\n",
      "Epoch: 8400 | Loss: 0.004569186290052514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8500 | Loss: 0.004544393597823447\n",
      "Epoch: 8600 | Loss: 0.00455490496635961\n",
      "Epoch: 8700 | Loss: 0.004546084840582734\n",
      "Epoch: 8800 | Loss: 0.004533521315213182\n",
      "Epoch: 8900 | Loss: 0.0045302656972912095\n",
      "Epoch: 9000 | Loss: 0.004526113717165981\n",
      "Epoch: 9100 | Loss: 0.0045240550602493376\n",
      "Epoch: 9200 | Loss: 0.004521860721909638\n",
      "Epoch: 9300 | Loss: 0.004519495100172227\n",
      "Epoch: 9400 | Loss: 0.004516942835310446\n",
      "Epoch: 9500 | Loss: 0.00451418593776038\n",
      "Epoch: 9600 | Loss: 0.004511202658285607\n",
      "Epoch: 9700 | Loss: 0.004507965826701792\n",
      "Epoch: 9800 | Loss: 0.0045044402778792415\n",
      "Epoch: 9900 | Loss: 0.004500578729093808\n",
      "Epoch: 10000 | Loss: 0.004496314971678976\n",
      "Epoch: 10100 | Loss: 0.004491552307175562\n",
      "Epoch: 10200 | Loss: 0.0044861435130275325\n",
      "Epoch: 10300 | Loss: 0.004479857034201806\n",
      "Epoch: 10400 | Loss: 0.00447233345106211\n",
      "Epoch: 10500 | Loss: 0.00446543147726402\n",
      "Epoch: 10600 | Loss: 0.00445362127735415\n",
      "Epoch: 10700 | Loss: 0.004441252067952492\n",
      "Epoch: 10800 | Loss: 0.00442927007300292\n",
      "Epoch: 10900 | Loss: 0.004417409569590432\n",
      "Epoch: 11000 | Loss: 0.004405280582782826\n",
      "Epoch: 11100 | Loss: 0.0043929790470377\n",
      "Epoch: 11200 | Loss: 0.004380367596845788\n",
      "Epoch: 11300 | Loss: 0.004368457017669793\n",
      "Epoch: 11400 | Loss: 0.004356434445884906\n",
      "Epoch: 11500 | Loss: 0.004345674092558934\n",
      "Epoch: 11600 | Loss: 0.004334741747793743\n",
      "Epoch: 11700 | Loss: 0.004326001515532569\n",
      "Epoch: 11800 | Loss: 0.004316519630435536\n",
      "Epoch: 11900 | Loss: 0.004308768907209809\n",
      "Epoch: 12000 | Loss: 0.004301887126667679\n",
      "Epoch: 12100 | Loss: 0.004298698500256634\n",
      "Epoch: 12200 | Loss: 0.004295588718485032\n",
      "Epoch: 12300 | Loss: 0.004292560124218933\n",
      "Epoch: 12400 | Loss: 0.004289620670088982\n",
      "Epoch: 12500 | Loss: 0.004286777798369383\n",
      "Epoch: 12600 | Loss: 0.004284038335246454\n",
      "Epoch: 12700 | Loss: 0.004281408397326008\n",
      "Epoch: 12800 | Loss: 0.004278893306326116\n",
      "Epoch: 12900 | Loss: 0.004276497514331416\n",
      "Epoch: 13000 | Loss: 0.0042742245440868555\n",
      "Epoch: 13100 | Loss: 0.0042720769493683666\n",
      "Epoch: 13200 | Loss: 0.004270056299938134\n",
      "Epoch: 13300 | Loss: 0.004268163194088315\n",
      "Epoch: 13400 | Loss: 0.0042663972994252605\n",
      "Epoch: 13500 | Loss: 0.00426511594927971\n",
      "Epoch: 13600 | Loss: 0.004263311198331427\n",
      "Epoch: 13700 | Loss: 0.004262008211845091\n",
      "Epoch: 13800 | Loss: 0.004260835039056611\n",
      "Epoch: 13900 | Loss: 0.004259760518969899\n",
      "Epoch: 14000 | Loss: 0.004258800711149244\n",
      "Epoch: 14100 | Loss: 0.004257932583486666\n",
      "Epoch: 14200 | Loss: 0.004257225492867981\n",
      "Epoch: 14300 | Loss: 0.0042564747435497995\n",
      "Epoch: 14400 | Loss: 0.004255941621549009\n",
      "Epoch: 14500 | Loss: 0.004255464977791891\n",
      "Epoch: 14600 | Loss: 0.004254880685784704\n",
      "Epoch: 14700 | Loss: 0.004254563295863009\n",
      "Epoch: 14800 | Loss: 0.004255212312693945\n",
      "Epoch: 14900 | Loss: 0.004253923481048298\n",
      "time=188.72899293899536\n",
      "error=0.0042538418729167506\n",
      "best loss=0.004253558045795551\n",
      "best epoch=14973\n",
      "16\n",
      "Epoch: 0 | Loss: 0.47327156186163877\n",
      "Epoch: 100 | Loss: 0.03657242409970622\n",
      "Epoch: 200 | Loss: 0.024039928747690627\n",
      "Epoch: 300 | Loss: 0.02091232570137623\n",
      "Epoch: 400 | Loss: 0.01942666635628434\n",
      "Epoch: 500 | Loss: 0.018433984368738922\n",
      "Epoch: 600 | Loss: 0.017471625135222196\n",
      "Epoch: 700 | Loss: 0.016552025153508232\n",
      "Epoch: 800 | Loss: 0.01544000156138422\n",
      "Epoch: 900 | Loss: 0.01456657365812042\n",
      "Epoch: 1000 | Loss: 0.01386423417060045\n",
      "Epoch: 1100 | Loss: 0.013030926704629298\n",
      "Epoch: 1200 | Loss: 0.01121268740784749\n",
      "Epoch: 1300 | Loss: 0.009482858723861318\n",
      "Epoch: 1400 | Loss: 0.008901534086760083\n",
      "Epoch: 1500 | Loss: 0.008658842305976057\n",
      "Epoch: 1600 | Loss: 0.008500498868688557\n",
      "Epoch: 1700 | Loss: 0.008389199213444266\n",
      "Epoch: 1800 | Loss: 0.008303782564545024\n",
      "Epoch: 1900 | Loss: 0.008159487165927981\n",
      "Epoch: 2000 | Loss: 0.008002913875999784\n",
      "Epoch: 2100 | Loss: 0.007942667646484389\n",
      "Epoch: 2200 | Loss: 0.007627601903438511\n",
      "Epoch: 2300 | Loss: 0.007478594678797323\n",
      "Epoch: 2400 | Loss: 0.007348346584326145\n",
      "Epoch: 2500 | Loss: 0.00719300385089474\n",
      "Epoch: 2600 | Loss: 0.0070701930479585146\n",
      "Epoch: 2700 | Loss: 0.0070733715998084635\n",
      "Epoch: 2800 | Loss: 0.006891861821477685\n",
      "Epoch: 2900 | Loss: 0.006778381694950743\n",
      "Epoch: 3000 | Loss: 0.006652308578388682\n",
      "Epoch: 3100 | Loss: 0.0065653594887889605\n",
      "Epoch: 3200 | Loss: 0.00644479058838871\n",
      "Epoch: 3300 | Loss: 0.0062616276377367615\n",
      "Epoch: 3400 | Loss: 0.00607308266244111\n",
      "Epoch: 3500 | Loss: 0.005957595216611213\n",
      "Epoch: 3600 | Loss: 0.005850024292991\n",
      "Epoch: 3700 | Loss: 0.005744925381740768\n",
      "Epoch: 3800 | Loss: 0.005639928948006248\n",
      "Epoch: 3900 | Loss: 0.005544242187657776\n",
      "Epoch: 4000 | Loss: 0.005460162102010677\n",
      "Epoch: 4100 | Loss: 0.005368726090741096\n",
      "Epoch: 4200 | Loss: 0.005253306690931967\n",
      "Epoch: 4300 | Loss: 0.005050929074559421\n",
      "Epoch: 4400 | Loss: 0.004987150016401121\n",
      "Epoch: 4500 | Loss: 0.0049744350093768475\n",
      "Epoch: 4600 | Loss: 0.0049646384924701414\n",
      "Epoch: 4700 | Loss: 0.004960442803082481\n",
      "Epoch: 4800 | Loss: 0.004951809523969631\n",
      "Epoch: 4900 | Loss: 0.0049481336343570985\n",
      "Epoch: 5000 | Loss: 0.004940314329531739\n",
      "Epoch: 5100 | Loss: 0.004967593043300191\n",
      "Epoch: 5200 | Loss: 0.00493739509257068\n",
      "Epoch: 5300 | Loss: 0.004949219834110442\n",
      "Epoch: 5400 | Loss: 0.004938671384750447\n",
      "Epoch: 5500 | Loss: 0.004928051142941713\n",
      "Epoch: 5600 | Loss: 0.0049268245998141495\n",
      "Epoch: 5700 | Loss: 0.0049242197505281445\n",
      "Epoch: 5800 | Loss: 0.004927581406601128\n",
      "Epoch: 5900 | Loss: 0.00492396520534826\n",
      "Epoch: 6000 | Loss: 0.004921349432420071\n",
      "Epoch: 6100 | Loss: 0.004920987004389752\n",
      "Epoch: 6200 | Loss: 0.004920682830543008\n",
      "Epoch: 6300 | Loss: 0.004920392589282221\n",
      "Epoch: 6400 | Loss: 0.004920115976852183\n",
      "Epoch: 6500 | Loss: 0.004919852914673259\n",
      "Epoch: 6600 | Loss: 0.004919603320050385\n",
      "Epoch: 6700 | Loss: 0.004919367099948773\n",
      "Epoch: 6800 | Loss: 0.004919144146998905\n",
      "Epoch: 6900 | Loss: 0.004918934335461429\n",
      "Epoch: 7000 | Loss: 0.004918737516674531\n",
      "Epoch: 7100 | Loss: 0.004918553513914691\n",
      "Epoch: 7200 | Loss: 0.004918382116807946\n",
      "Epoch: 7300 | Loss: 0.004918223075593675\n",
      "Epoch: 7400 | Loss: 0.00491807609568574\n",
      "Epoch: 7500 | Loss: 0.004934948442966036\n",
      "Epoch: 7600 | Loss: 0.004917860427267563\n",
      "Epoch: 7700 | Loss: 0.004921384154615542\n",
      "Epoch: 7800 | Loss: 0.004918179407472338\n",
      "Epoch: 7900 | Loss: 0.004918771436297568\n",
      "Epoch: 8000 | Loss: 0.004917602316026818\n",
      "Epoch: 8100 | Loss: 0.0049174586051020965\n",
      "Epoch: 8200 | Loss: 0.0049178851660019954\n",
      "Epoch: 8300 | Loss: 0.004918031066736586\n",
      "Epoch: 8400 | Loss: 0.004918211418061926\n",
      "Epoch: 8500 | Loss: 0.004918571542931835\n",
      "Epoch: 8600 | Loss: 0.004919102520119568\n",
      "Epoch: 8700 | Loss: 0.0049172210160533265\n",
      "Epoch: 8800 | Loss: 0.004920380803187832\n",
      "Epoch: 8900 | Loss: 0.0049280508148076005\n",
      "Epoch: 9000 | Loss: 0.00491699893001092\n",
      "Epoch: 9100 | Loss: 0.0049169700869384735\n",
      "Epoch: 9200 | Loss: 0.004916957471171494\n",
      "Epoch: 9300 | Loss: 0.004916945222729127\n",
      "Epoch: 9400 | Loss: 0.004916933340433311\n",
      "Epoch: 9500 | Loss: 0.004916921834826091\n",
      "Epoch: 9600 | Loss: 0.004916910714083452\n",
      "Epoch: 9700 | Loss: 0.004916899983751258\n",
      "Epoch: 9800 | Loss: 0.00491688964670259\n",
      "Epoch: 9900 | Loss: 0.004916879703206058\n",
      "Epoch: 10000 | Loss: 0.0049168701510773\n",
      "Epoch: 10100 | Loss: 0.0049168609859046135\n",
      "Epoch: 10200 | Loss: 0.00491685220134562\n",
      "Epoch: 10300 | Loss: 0.004916843789490441\n",
      "Epoch: 10400 | Loss: 0.004916835741286041\n",
      "Epoch: 10500 | Loss: 0.004918509663236571\n",
      "Epoch: 10600 | Loss: 0.004916821866873665\n",
      "Epoch: 10700 | Loss: 0.004916846066748626\n",
      "Epoch: 10800 | Loss: 0.004916829172867096\n",
      "Epoch: 10900 | Loss: 0.004918883934468904\n",
      "Epoch: 11000 | Loss: 0.004916841565627396\n",
      "Epoch: 11100 | Loss: 0.0049171541312656025\n",
      "Epoch: 11200 | Loss: 0.004916909103339597\n",
      "Epoch: 11300 | Loss: 0.004918392187885844\n",
      "Epoch: 11400 | Loss: 0.0049171453425027566\n",
      "Epoch: 11500 | Loss: 0.004916957522899333\n",
      "Epoch: 11600 | Loss: 0.004917304888678501\n",
      "Epoch: 11700 | Loss: 0.004916896045828558\n",
      "Epoch: 11800 | Loss: 0.004916785241483278\n",
      "Epoch: 11900 | Loss: 0.0049169481700756775\n",
      "Epoch: 12000 | Loss: 0.004916759618566999\n",
      "Epoch: 12100 | Loss: 0.004916753361164832\n",
      "Epoch: 12200 | Loss: 0.0049167520292898535\n",
      "Epoch: 12300 | Loss: 0.00491675074070116\n",
      "Epoch: 12400 | Loss: 0.0049167494969351465\n",
      "Epoch: 12500 | Loss: 0.0049167483021921084\n",
      "Epoch: 12600 | Loss: 0.004916747160337745\n",
      "Epoch: 12700 | Loss: 0.004916746074784218\n",
      "Epoch: 12800 | Loss: 0.0049167450483940615\n",
      "Epoch: 12900 | Loss: 0.004916744083398125\n",
      "Epoch: 13000 | Loss: 0.004916743181328922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13100 | Loss: 0.004916742342972841\n",
      "Epoch: 13200 | Loss: 0.004916741568344853\n",
      "Epoch: 13300 | Loss: 0.004916740856688764\n",
      "Epoch: 13400 | Loss: 0.0049167402065042835\n",
      "Epoch: 13500 | Loss: 0.004918876795724981\n",
      "Epoch: 13600 | Loss: 0.004916739213239273\n",
      "Epoch: 13700 | Loss: 0.0049168002149500905\n",
      "Epoch: 13800 | Loss: 0.004916738398456257\n",
      "Epoch: 13900 | Loss: 0.004916751907752988\n",
      "Epoch: 14000 | Loss: 0.004916816361765522\n",
      "Epoch: 14100 | Loss: 0.0049171214812998324\n",
      "Epoch: 14200 | Loss: 0.004916881551809407\n",
      "Epoch: 14300 | Loss: 0.004916870525177868\n",
      "Epoch: 14400 | Loss: 0.0049169171274370815\n",
      "Epoch: 14500 | Loss: 0.00491682968959224\n",
      "Epoch: 14600 | Loss: 0.004916859282320894\n",
      "Epoch: 14700 | Loss: 0.004916822564708152\n",
      "Epoch: 14800 | Loss: 0.004916756437155501\n",
      "Epoch: 14900 | Loss: 0.004916803130713435\n",
      "time=198.85853791236877\n",
      "error=0.004916765394573847\n",
      "best loss=0.004916738398456257\n",
      "best epoch=13800\n",
      "20\n",
      "Epoch: 0 | Loss: 0.48829022275567513\n",
      "Epoch: 100 | Loss: 0.05606952247092731\n",
      "Epoch: 200 | Loss: 0.027834700544619666\n",
      "Epoch: 300 | Loss: 0.02134743314310322\n",
      "Epoch: 400 | Loss: 0.017836462882604324\n",
      "Epoch: 500 | Loss: 0.01479251275710638\n",
      "Epoch: 600 | Loss: 0.012680980737799483\n",
      "Epoch: 700 | Loss: 0.01132841485899877\n",
      "Epoch: 800 | Loss: 0.010357223756391875\n",
      "Epoch: 900 | Loss: 0.009584602225260575\n",
      "Epoch: 1000 | Loss: 0.00891752951097911\n",
      "Epoch: 1100 | Loss: 0.008367593605999908\n",
      "Epoch: 1200 | Loss: 0.007806691718916378\n",
      "Epoch: 1300 | Loss: 0.00740430891757843\n",
      "Epoch: 1400 | Loss: 0.0071078202210040015\n",
      "Epoch: 1500 | Loss: 0.006893984568850589\n",
      "Epoch: 1600 | Loss: 0.006709001237371854\n",
      "Epoch: 1700 | Loss: 0.006566451997648949\n",
      "Epoch: 1800 | Loss: 0.006417596905475389\n",
      "Epoch: 1900 | Loss: 0.006260090697636343\n",
      "Epoch: 2000 | Loss: 0.0061772602327262085\n",
      "Epoch: 2100 | Loss: 0.006006214923187019\n",
      "Epoch: 2200 | Loss: 0.005903617075599624\n",
      "Epoch: 2300 | Loss: 0.005803301066113451\n",
      "Epoch: 2400 | Loss: 0.005752586644233129\n",
      "Epoch: 2500 | Loss: 0.005648538124366924\n",
      "Epoch: 2600 | Loss: 0.00557782725196262\n",
      "Epoch: 2700 | Loss: 0.005495299830071207\n",
      "Epoch: 2800 | Loss: 0.00543693017156136\n",
      "Epoch: 2900 | Loss: 0.005384906244466967\n",
      "Epoch: 3000 | Loss: 0.005145305946920411\n",
      "Epoch: 3100 | Loss: 0.005061437019032997\n",
      "Epoch: 3200 | Loss: 0.004987592896170947\n",
      "Epoch: 3300 | Loss: 0.004903802876342305\n",
      "Epoch: 3400 | Loss: 0.004786462364627651\n",
      "Epoch: 3500 | Loss: 0.004555820069600064\n",
      "Epoch: 3600 | Loss: 0.004272680982303953\n",
      "Epoch: 3700 | Loss: 0.004201740595078566\n",
      "Epoch: 3800 | Loss: 0.004161946057111198\n",
      "Epoch: 3900 | Loss: 0.004130301932078584\n",
      "Epoch: 4000 | Loss: 0.004103099572065866\n",
      "Epoch: 4100 | Loss: 0.004078453762066024\n",
      "Epoch: 4200 | Loss: 0.004055287962980275\n",
      "Epoch: 4300 | Loss: 0.004033113561394298\n",
      "Epoch: 4400 | Loss: 0.004012523874868566\n",
      "Epoch: 4500 | Loss: 0.00400639124287744\n",
      "Epoch: 4600 | Loss: 0.003977772672907429\n",
      "Epoch: 4700 | Loss: 0.0039758252743795665\n",
      "Epoch: 4800 | Loss: 0.003927385410298982\n",
      "Epoch: 4900 | Loss: 0.003918755900283579\n",
      "Epoch: 5000 | Loss: 0.0038808902290897373\n",
      "Epoch: 5100 | Loss: 0.0038510520369937577\n",
      "Epoch: 5200 | Loss: 0.003825712511506727\n",
      "Epoch: 5300 | Loss: 0.0037825004601217504\n",
      "Epoch: 5400 | Loss: 0.003727793617679873\n",
      "Epoch: 5500 | Loss: 0.0036553101476302476\n",
      "Epoch: 5600 | Loss: 0.0035766792344856585\n",
      "Epoch: 5700 | Loss: 0.003527447315231341\n",
      "Epoch: 5800 | Loss: 0.003470704302635725\n",
      "Epoch: 5900 | Loss: 0.003423280981870008\n",
      "Epoch: 6000 | Loss: 0.0033657663269628657\n",
      "Epoch: 6100 | Loss: 0.0033294766139943003\n",
      "Epoch: 6200 | Loss: 0.0032855466411483396\n",
      "Epoch: 6300 | Loss: 0.0032501965335129095\n",
      "Epoch: 6400 | Loss: 0.003232274347540796\n",
      "Epoch: 6500 | Loss: 0.003221798975787154\n",
      "Epoch: 6600 | Loss: 0.0032135438152328653\n",
      "Epoch: 6700 | Loss: 0.003205838855716709\n",
      "Epoch: 6800 | Loss: 0.003197657389290712\n",
      "Epoch: 6900 | Loss: 0.003188042384480727\n",
      "Epoch: 7000 | Loss: 0.0031777572004439807\n",
      "Epoch: 7100 | Loss: 0.003169826975680557\n",
      "Epoch: 7200 | Loss: 0.003163021053336044\n",
      "Epoch: 7300 | Loss: 0.003157812561067188\n",
      "Epoch: 7400 | Loss: 0.003153876284353085\n",
      "Epoch: 7500 | Loss: 0.003148850808440385\n",
      "Epoch: 7600 | Loss: 0.0031446853549061413\n",
      "Epoch: 7700 | Loss: 0.003139964171650288\n",
      "Epoch: 7800 | Loss: 0.0031355626976767035\n",
      "Epoch: 7900 | Loss: 0.0031306452709818086\n",
      "Epoch: 8000 | Loss: 0.003125607422959072\n",
      "Epoch: 8100 | Loss: 0.003118741691476354\n",
      "Epoch: 8200 | Loss: 0.003112587338851178\n",
      "Epoch: 8300 | Loss: 0.003106927647266524\n",
      "Epoch: 8400 | Loss: 0.003101752016570698\n",
      "Epoch: 8500 | Loss: 0.0030916514634196863\n",
      "Epoch: 8600 | Loss: 0.003086186298104963\n",
      "Epoch: 8700 | Loss: 0.0030845508525339086\n",
      "Epoch: 8800 | Loss: 0.003081307286361595\n",
      "Epoch: 8900 | Loss: 0.0030785207526792552\n",
      "Epoch: 9000 | Loss: 0.003074916933752339\n",
      "Epoch: 9100 | Loss: 0.0030735975473838257\n",
      "Epoch: 9200 | Loss: 0.0030722630555688937\n",
      "Epoch: 9300 | Loss: 0.003070874342088246\n",
      "Epoch: 9400 | Loss: 0.003069438454313074\n",
      "Epoch: 9500 | Loss: 0.0030679708859103977\n",
      "Epoch: 9600 | Loss: 0.0030664858845270934\n",
      "Epoch: 9700 | Loss: 0.0030649880128688287\n",
      "Epoch: 9800 | Loss: 0.00306347293378013\n",
      "Epoch: 9900 | Loss: 0.003061932613896758\n",
      "Epoch: 10000 | Loss: 0.0030603586526254203\n",
      "Epoch: 10100 | Loss: 0.0030587432340935497\n",
      "Epoch: 10200 | Loss: 0.0030570792141182204\n",
      "Epoch: 10300 | Loss: 0.0030553602072804133\n",
      "Epoch: 10400 | Loss: 0.003054059061012541\n",
      "Epoch: 10500 | Loss: 0.0030519537409306394\n",
      "Epoch: 10600 | Loss: 0.003050103574166268\n",
      "Epoch: 10700 | Loss: 0.003048090311962707\n",
      "Epoch: 10800 | Loss: 0.003046095384687181\n",
      "Epoch: 10900 | Loss: 0.0030440872134230752\n",
      "Epoch: 11000 | Loss: 0.0030421454078771388\n",
      "Epoch: 11100 | Loss: 0.0030396551129939398\n",
      "Epoch: 11200 | Loss: 0.0030375417198171754\n",
      "Epoch: 11300 | Loss: 0.0030341987871754005\n",
      "Epoch: 11400 | Loss: 0.003030829916269551\n",
      "Epoch: 11500 | Loss: 0.0030259484451648903\n",
      "Epoch: 11600 | Loss: 0.003019927534607416\n",
      "Epoch: 11700 | Loss: 0.0030135860488740953\n",
      "Epoch: 11800 | Loss: 0.0030065891257212165\n",
      "Epoch: 11900 | Loss: 0.0029977732074068313\n",
      "Epoch: 12000 | Loss: 0.0029881684195746244\n",
      "Epoch: 12100 | Loss: 0.002982943383063693\n",
      "Epoch: 12200 | Loss: 0.0029782359477660645\n",
      "Epoch: 12300 | Loss: 0.0029741371080065867\n",
      "Epoch: 12400 | Loss: 0.002970576742277348\n",
      "Epoch: 12500 | Loss: 0.0029674974861095852\n",
      "Epoch: 12600 | Loss: 0.0029648281514972332\n",
      "Epoch: 12700 | Loss: 0.002962483420039962\n",
      "Epoch: 12800 | Loss: 0.0029603866186970543\n",
      "Epoch: 12900 | Loss: 0.0029584772935797076\n",
      "Epoch: 13000 | Loss: 0.0029567061447124234\n",
      "Epoch: 13100 | Loss: 0.0029550317331470052\n",
      "Epoch: 13200 | Loss: 0.002953437389627738\n",
      "Epoch: 13300 | Loss: 0.002951966844714095\n",
      "Epoch: 13400 | Loss: 0.002950684941735002\n",
      "Epoch: 13500 | Loss: 0.002949544520951016\n",
      "Epoch: 13600 | Loss: 0.00294835231116881\n",
      "Epoch: 13700 | Loss: 0.0029475548122890566\n",
      "Epoch: 13800 | Loss: 0.0029466629810485255\n",
      "Epoch: 13900 | Loss: 0.0029461527214905006\n",
      "Epoch: 14000 | Loss: 0.0029455714392372934\n",
      "Epoch: 14100 | Loss: 0.0029453015462459183\n",
      "Epoch: 14200 | Loss: 0.0029448924556193593\n",
      "Epoch: 14300 | Loss: 0.0029446883624333004\n",
      "Epoch: 14400 | Loss: 0.0029444714178082435\n",
      "Epoch: 14500 | Loss: 0.002944258979480517\n",
      "Epoch: 14600 | Loss: 0.0029441190523175438\n",
      "Epoch: 14700 | Loss: 0.0029441009609069866\n",
      "Epoch: 14800 | Loss: 0.002944094362025874\n",
      "Epoch: 14900 | Loss: 0.002943895345611661\n",
      "time=226.03418016433716\n",
      "error=0.0029439552363273993\n",
      "best loss=0.0029438154550646464\n",
      "best epoch=14977\n",
      "24\n",
      "Epoch: 0 | Loss: 0.5032552410247972\n",
      "Epoch: 100 | Loss: 0.07761600297656468\n",
      "Epoch: 200 | Loss: 0.027618274652417254\n",
      "Epoch: 300 | Loss: 0.020516985606909573\n",
      "Epoch: 400 | Loss: 0.017067042101711026\n",
      "Epoch: 500 | Loss: 0.014612496803212877\n",
      "Epoch: 600 | Loss: 0.012665522550561921\n",
      "Epoch: 700 | Loss: 0.011228069071421494\n",
      "Epoch: 800 | Loss: 0.01026436374694846\n",
      "Epoch: 900 | Loss: 0.0096596163461041\n",
      "Epoch: 1000 | Loss: 0.009295509470934719\n",
      "Epoch: 1100 | Loss: 0.009064318885343049\n",
      "Epoch: 1200 | Loss: 0.00885113150074332\n",
      "Epoch: 1300 | Loss: 0.008705672359142654\n",
      "Epoch: 1400 | Loss: 0.008528660131892631\n",
      "Epoch: 1500 | Loss: 0.008396996184676493\n",
      "Epoch: 1600 | Loss: 0.008238012079863851\n",
      "Epoch: 1700 | Loss: 0.008102931575161412\n",
      "Epoch: 1800 | Loss: 0.007943136374522949\n",
      "Epoch: 1900 | Loss: 0.00788068433159805\n",
      "Epoch: 2000 | Loss: 0.00764893709100966\n",
      "Epoch: 2100 | Loss: 0.007487004643591947\n",
      "Epoch: 2200 | Loss: 0.007322090853037325\n",
      "Epoch: 2300 | Loss: 0.007123467821056708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2400 | Loss: 0.00692252895762714\n",
      "Epoch: 2500 | Loss: 0.006617407083633628\n",
      "Epoch: 2600 | Loss: 0.006354972547408341\n",
      "Epoch: 2700 | Loss: 0.0061398308226555615\n",
      "Epoch: 2800 | Loss: 0.00596627817778252\n",
      "Epoch: 2900 | Loss: 0.005832563425312949\n",
      "Epoch: 3000 | Loss: 0.0056500641577839576\n",
      "Epoch: 3100 | Loss: 0.005591296353943532\n",
      "Epoch: 3200 | Loss: 0.005537683596346498\n",
      "Epoch: 3300 | Loss: 0.005487669993349377\n",
      "Epoch: 3400 | Loss: 0.005439668129228812\n",
      "Epoch: 3500 | Loss: 0.005392411610140724\n",
      "Epoch: 3600 | Loss: 0.005344852756457287\n",
      "Epoch: 3700 | Loss: 0.005296021725053987\n",
      "Epoch: 3800 | Loss: 0.005245022829029951\n",
      "Epoch: 3900 | Loss: 0.005191033440760572\n",
      "Epoch: 4000 | Loss: 0.005133400501642024\n",
      "Epoch: 4100 | Loss: 0.005072698667932325\n",
      "Epoch: 4200 | Loss: 0.005010230689467347\n",
      "Epoch: 4300 | Loss: 0.004951916276958044\n",
      "Epoch: 4400 | Loss: 0.004896065481391811\n",
      "Epoch: 4500 | Loss: 0.004844077863302564\n",
      "Epoch: 4600 | Loss: 0.004784082496173219\n",
      "Epoch: 4700 | Loss: 0.004724540363235048\n",
      "Epoch: 4800 | Loss: 0.004670614938024823\n",
      "Epoch: 4900 | Loss: 0.004621667998075674\n",
      "Epoch: 5000 | Loss: 0.004568126198360648\n",
      "Epoch: 5100 | Loss: 0.0045259116600640525\n",
      "Epoch: 5200 | Loss: 0.004481986455314895\n",
      "Epoch: 5300 | Loss: 0.004442113169414735\n",
      "Epoch: 5400 | Loss: 0.00440791749315969\n",
      "Epoch: 5500 | Loss: 0.004417583166631227\n",
      "Epoch: 5600 | Loss: 0.004353604291467714\n",
      "Epoch: 5700 | Loss: 0.004330133045701414\n",
      "Epoch: 5800 | Loss: 0.004315757942490737\n",
      "Epoch: 5900 | Loss: 0.004307721721910328\n",
      "Epoch: 6000 | Loss: 0.004285685641480141\n",
      "Epoch: 6100 | Loss: 0.004280322680813045\n",
      "Epoch: 6200 | Loss: 0.004275228463565449\n",
      "Epoch: 6300 | Loss: 0.004270365970056523\n",
      "Epoch: 6400 | Loss: 0.004265711971938993\n",
      "Epoch: 6500 | Loss: 0.004261245621146052\n",
      "Epoch: 6600 | Loss: 0.004256951250361753\n",
      "Epoch: 6700 | Loss: 0.004252820056801075\n",
      "Epoch: 6800 | Loss: 0.004248849726272637\n",
      "Epoch: 6900 | Loss: 0.0042450394102468\n",
      "Epoch: 7000 | Loss: 0.0042413813784973635\n",
      "Epoch: 7100 | Loss: 0.004237854256475869\n",
      "Epoch: 7200 | Loss: 0.004234419553316332\n",
      "Epoch: 7300 | Loss: 0.004231025948557404\n",
      "Epoch: 7400 | Loss: 0.004228563133479463\n",
      "Epoch: 7500 | Loss: 0.004226144908560572\n",
      "Epoch: 7600 | Loss: 0.004220057343641209\n",
      "Epoch: 7700 | Loss: 0.0042158574287649245\n",
      "Epoch: 7800 | Loss: 0.004212223352560932\n",
      "Epoch: 7900 | Loss: 0.004202123309154367\n",
      "Epoch: 8000 | Loss: 0.004187699186983284\n",
      "Epoch: 8100 | Loss: 0.004167114332722753\n",
      "Epoch: 8200 | Loss: 0.004144163986069862\n",
      "Epoch: 8300 | Loss: 0.004129733670901758\n",
      "Epoch: 8400 | Loss: 0.004125970535953679\n",
      "Epoch: 8500 | Loss: 0.004120117980018373\n",
      "Epoch: 8600 | Loss: 0.004116130178957192\n",
      "Epoch: 8700 | Loss: 0.004114320283777154\n",
      "Epoch: 8800 | Loss: 0.004110924525368945\n",
      "Epoch: 8900 | Loss: 0.004107976730137961\n",
      "Epoch: 9000 | Loss: 0.004103125900672439\n",
      "Epoch: 9100 | Loss: 0.0041010990964588274\n",
      "Epoch: 9200 | Loss: 0.004098782097003794\n",
      "Epoch: 9300 | Loss: 0.00409606395119478\n",
      "Epoch: 9400 | Loss: 0.004092779910486689\n",
      "Epoch: 9500 | Loss: 0.004088698550720332\n",
      "Epoch: 9600 | Loss: 0.004083548980703618\n",
      "Epoch: 9700 | Loss: 0.004077109825435809\n",
      "Epoch: 9800 | Loss: 0.004069250831067534\n",
      "Epoch: 9900 | Loss: 0.004059860403766535\n",
      "Epoch: 10000 | Loss: 0.004048957292470807\n",
      "Epoch: 10100 | Loss: 0.004037099059192327\n",
      "Epoch: 10200 | Loss: 0.004025563743563236\n",
      "Epoch: 10300 | Loss: 0.004015720790564661\n",
      "Epoch: 10400 | Loss: 0.004008075591952766\n",
      "Epoch: 10500 | Loss: 0.004002406674949395\n",
      "Epoch: 10600 | Loss: 0.003998355059236658\n",
      "Epoch: 10700 | Loss: 0.003993715301820991\n",
      "Epoch: 10800 | Loss: 0.0039899331232417165\n",
      "Epoch: 10900 | Loss: 0.003986117831441461\n",
      "Epoch: 11000 | Loss: 0.003981670241450311\n",
      "Epoch: 11100 | Loss: 0.003977337061870093\n",
      "Epoch: 11200 | Loss: 0.003972737151983529\n",
      "Epoch: 11300 | Loss: 0.0039666528989529245\n",
      "Epoch: 11400 | Loss: 0.003961298380878133\n",
      "Epoch: 11500 | Loss: 0.003955872358289486\n",
      "Epoch: 11600 | Loss: 0.0039504095095027735\n",
      "Epoch: 11700 | Loss: 0.0039442401446835385\n",
      "Epoch: 11800 | Loss: 0.003937871127762721\n",
      "Epoch: 11900 | Loss: 0.0039306057533077515\n",
      "Epoch: 12000 | Loss: 0.003920458878213954\n",
      "Epoch: 12100 | Loss: 0.003915176595482339\n",
      "Epoch: 12200 | Loss: 0.003910142323649456\n",
      "Epoch: 12300 | Loss: 0.003905515556185792\n",
      "Epoch: 12400 | Loss: 0.003901296195603679\n",
      "Epoch: 12500 | Loss: 0.003897415219327411\n",
      "Epoch: 12600 | Loss: 0.003893794089246979\n",
      "Epoch: 12700 | Loss: 0.003890364145446782\n",
      "Epoch: 12800 | Loss: 0.0038870677496493896\n",
      "Epoch: 12900 | Loss: 0.0038838558815720934\n",
      "Epoch: 13000 | Loss: 0.003880686052917224\n",
      "Epoch: 13100 | Loss: 0.0038775196937363933\n",
      "Epoch: 13200 | Loss: 0.0038743163406673815\n",
      "Epoch: 13300 | Loss: 0.003871018703730557\n",
      "Epoch: 13400 | Loss: 0.0038675293352152303\n",
      "Epoch: 13500 | Loss: 0.0038638868692400805\n",
      "Epoch: 13600 | Loss: 0.0038602471402749104\n",
      "Epoch: 13700 | Loss: 0.003856303424658771\n",
      "Epoch: 13800 | Loss: 0.003852518903384025\n",
      "Epoch: 13900 | Loss: 0.0038487879504702015\n",
      "Epoch: 14000 | Loss: 0.003845124373264984\n",
      "Epoch: 14100 | Loss: 0.003841689292563687\n",
      "Epoch: 14200 | Loss: 0.0038383951842888633\n",
      "Epoch: 14300 | Loss: 0.003835321967937735\n",
      "Epoch: 14400 | Loss: 0.003832345112859371\n",
      "Epoch: 14500 | Loss: 0.0038296106869363704\n",
      "Epoch: 14600 | Loss: 0.0038270507649796077\n",
      "Epoch: 14700 | Loss: 0.0038244426987070635\n",
      "Epoch: 14800 | Loss: 0.003822148511414996\n",
      "Epoch: 14900 | Loss: 0.0038198237425088647\n",
      "time=243.91982007026672\n",
      "error=0.0038176319609839813\n",
      "best loss=0.0038176319609839813\n",
      "best epoch=14999\n",
      "28\n",
      "Epoch: 0 | Loss: 0.5185373676227194\n",
      "Epoch: 100 | Loss: 0.10421114744907534\n",
      "Epoch: 200 | Loss: 0.031129142305410105\n",
      "Epoch: 300 | Loss: 0.021486413030970195\n",
      "Epoch: 400 | Loss: 0.01823990222010004\n",
      "Epoch: 500 | Loss: 0.016583398977165455\n",
      "Epoch: 600 | Loss: 0.015576964625468193\n",
      "Epoch: 700 | Loss: 0.01485637802724495\n",
      "Epoch: 800 | Loss: 0.014262372713340951\n",
      "Epoch: 900 | Loss: 0.013732568824014245\n",
      "Epoch: 1000 | Loss: 0.013219693647517274\n",
      "Epoch: 1100 | Loss: 0.012740723523366446\n",
      "Epoch: 1200 | Loss: 0.012291623415208841\n",
      "Epoch: 1300 | Loss: 0.011853569476189799\n",
      "Epoch: 1400 | Loss: 0.011417849701363728\n",
      "Epoch: 1500 | Loss: 0.010975853444781272\n",
      "Epoch: 1600 | Loss: 0.010577065770717008\n",
      "Epoch: 1700 | Loss: 0.010195502831522912\n",
      "Epoch: 1800 | Loss: 0.009817473511751218\n",
      "Epoch: 1900 | Loss: 0.009466777004060598\n",
      "Epoch: 2000 | Loss: 0.009148783117209326\n",
      "Epoch: 2100 | Loss: 0.008804366510560723\n",
      "Epoch: 2200 | Loss: 0.008475244608561475\n",
      "Epoch: 2300 | Loss: 0.008167747708976946\n",
      "Epoch: 2400 | Loss: 0.007839971039602198\n",
      "Epoch: 2500 | Loss: 0.007529003252069799\n",
      "Epoch: 2600 | Loss: 0.007213130989705994\n",
      "Epoch: 2700 | Loss: 0.006866427929566016\n",
      "Epoch: 2800 | Loss: 0.006508905207479099\n",
      "Epoch: 2900 | Loss: 0.006125455240361623\n",
      "Epoch: 3000 | Loss: 0.005769844182854875\n",
      "Epoch: 3100 | Loss: 0.005623519166423065\n",
      "Epoch: 3200 | Loss: 0.005500154443824253\n",
      "Epoch: 3300 | Loss: 0.005394088849205659\n",
      "Epoch: 3400 | Loss: 0.005293562043650197\n",
      "Epoch: 3500 | Loss: 0.00519501665933744\n",
      "Epoch: 3600 | Loss: 0.005098015796159726\n",
      "Epoch: 3700 | Loss: 0.005004957952894306\n",
      "Epoch: 3800 | Loss: 0.004914891012296594\n",
      "Epoch: 3900 | Loss: 0.004824192006234554\n",
      "Epoch: 4000 | Loss: 0.00473066040630141\n",
      "Epoch: 4100 | Loss: 0.004633050561964267\n",
      "Epoch: 4200 | Loss: 0.004525572717892733\n",
      "Epoch: 4300 | Loss: 0.0044091557940172214\n",
      "Epoch: 4400 | Loss: 0.004279762551215054\n",
      "Epoch: 4500 | Loss: 0.004127314723665813\n",
      "Epoch: 4600 | Loss: 0.003958005143064608\n",
      "Epoch: 4700 | Loss: 0.003774127778427624\n",
      "Epoch: 4800 | Loss: 0.0036091254553448963\n",
      "Epoch: 4900 | Loss: 0.0034849064959514135\n",
      "Epoch: 5000 | Loss: 0.003380862369753981\n",
      "Epoch: 5100 | Loss: 0.0032885377728032574\n",
      "Epoch: 5200 | Loss: 0.0032253016251872803\n",
      "Epoch: 5300 | Loss: 0.0031763283204017884\n",
      "Epoch: 5400 | Loss: 0.003144937722099129\n",
      "Epoch: 5500 | Loss: 0.003118018467643319\n",
      "Epoch: 5600 | Loss: 0.0030931214310115774\n",
      "Epoch: 5700 | Loss: 0.003072957068104714\n",
      "Epoch: 5800 | Loss: 0.003050201438749776\n",
      "Epoch: 5900 | Loss: 0.0030283945204000176\n",
      "Epoch: 6000 | Loss: 0.003003905747990137\n",
      "Epoch: 6100 | Loss: 0.002993307189201574\n",
      "Epoch: 6200 | Loss: 0.0029826028706308036\n",
      "Epoch: 6300 | Loss: 0.0029717813546184307\n",
      "Epoch: 6400 | Loss: 0.0029609741509135554\n",
      "Epoch: 6500 | Loss: 0.0029503428954343177\n",
      "Epoch: 6600 | Loss: 0.0029400016654526906\n",
      "Epoch: 6700 | Loss: 0.0029299944894834538\n",
      "Epoch: 6800 | Loss: 0.0029203448580143727\n",
      "Epoch: 6900 | Loss: 0.0029110742238645836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7000 | Loss: 0.0029021761716029656\n",
      "Epoch: 7100 | Loss: 0.0028936195945895066\n",
      "Epoch: 7200 | Loss: 0.0028853791534502824\n",
      "Epoch: 7300 | Loss: 0.0028774077376476784\n",
      "Epoch: 7400 | Loss: 0.0028710002220807976\n",
      "Epoch: 7500 | Loss: 0.0028630366008312287\n",
      "Epoch: 7600 | Loss: 0.002856451057644583\n",
      "Epoch: 7700 | Loss: 0.0028490269305777127\n",
      "Epoch: 7800 | Loss: 0.0028415172894394776\n",
      "Epoch: 7900 | Loss: 0.002834748862475106\n",
      "Epoch: 8000 | Loss: 0.002828352932367594\n",
      "Epoch: 8100 | Loss: 0.0028223253417788067\n",
      "Epoch: 8200 | Loss: 0.0028166566474115582\n",
      "Epoch: 8300 | Loss: 0.002811478600271811\n",
      "Epoch: 8400 | Loss: 0.002805575251793926\n",
      "Epoch: 8500 | Loss: 0.002799615489225007\n",
      "Epoch: 8600 | Loss: 0.002793663417719772\n",
      "Epoch: 8700 | Loss: 0.002788554444669367\n",
      "Epoch: 8800 | Loss: 0.0027855906892495733\n",
      "Epoch: 8900 | Loss: 0.002777813546825674\n",
      "Epoch: 9000 | Loss: 0.0027726317133223615\n",
      "Epoch: 9100 | Loss: 0.002770208507130255\n",
      "Epoch: 9200 | Loss: 0.002767749924201403\n",
      "Epoch: 9300 | Loss: 0.002765250375394286\n",
      "Epoch: 9400 | Loss: 0.0027627174797775463\n",
      "Epoch: 9500 | Loss: 0.00276016008490617\n",
      "Epoch: 9600 | Loss: 0.002757579134589891\n",
      "Epoch: 9700 | Loss: 0.0027549706377569894\n",
      "Epoch: 9800 | Loss: 0.002752330713551434\n",
      "Epoch: 9900 | Loss: 0.0027496526442377413\n",
      "Epoch: 10000 | Loss: 0.0027469186935953982\n",
      "Epoch: 10100 | Loss: 0.0027440905844191574\n",
      "Epoch: 10200 | Loss: 0.002741098309225522\n",
      "Epoch: 10300 | Loss: 0.002737822502914756\n",
      "Epoch: 10400 | Loss: 0.002734333894102569\n",
      "Epoch: 10500 | Loss: 0.0027299686766617953\n",
      "Epoch: 10600 | Loss: 0.0027240159693622708\n",
      "Epoch: 10700 | Loss: 0.0027168932906633456\n",
      "Epoch: 10800 | Loss: 0.002707794870886955\n",
      "Epoch: 10900 | Loss: 0.0026981135094952067\n",
      "Epoch: 11000 | Loss: 0.0026872848395665663\n",
      "Epoch: 11100 | Loss: 0.0026783276161465057\n",
      "Epoch: 11200 | Loss: 0.0026703758277058336\n",
      "Epoch: 11300 | Loss: 0.0026641348840116333\n",
      "Epoch: 11400 | Loss: 0.0026579164174562733\n",
      "Epoch: 11500 | Loss: 0.0026527649482215605\n",
      "Epoch: 11600 | Loss: 0.002648350658213317\n",
      "Epoch: 11700 | Loss: 0.002644431470457889\n",
      "Epoch: 11800 | Loss: 0.0026413960499346812\n",
      "Epoch: 11900 | Loss: 0.002638131511015406\n",
      "Epoch: 12000 | Loss: 0.0026352877922788997\n",
      "Epoch: 12100 | Loss: 0.002633997683044302\n",
      "Epoch: 12200 | Loss: 0.0026327298006111896\n",
      "Epoch: 12300 | Loss: 0.002631483475073482\n",
      "Epoch: 12400 | Loss: 0.0026302593656925928\n",
      "Epoch: 12500 | Loss: 0.002629058343297066\n",
      "Epoch: 12600 | Loss: 0.0026278814109069043\n",
      "Epoch: 12700 | Loss: 0.002626729613033392\n",
      "Epoch: 12800 | Loss: 0.0026256039269181913\n",
      "Epoch: 12900 | Loss: 0.00262450514322287\n",
      "Epoch: 13000 | Loss: 0.002623433752319517\n",
      "Epoch: 13100 | Loss: 0.0026223898562681793\n",
      "Epoch: 13200 | Loss: 0.0026213731241591766\n",
      "Epoch: 13300 | Loss: 0.002620382799875489\n",
      "Epoch: 13400 | Loss: 0.002619417758721111\n",
      "Epoch: 13500 | Loss: 0.002618548233646571\n",
      "Epoch: 13600 | Loss: 0.0026175979944610137\n",
      "Epoch: 13700 | Loss: 0.002616744104338183\n",
      "Epoch: 13800 | Loss: 0.0026159625816292963\n",
      "Epoch: 13900 | Loss: 0.0026151089165328857\n",
      "Epoch: 14000 | Loss: 0.0026142622831967057\n",
      "Epoch: 14100 | Loss: 0.0026135461335244654\n",
      "Epoch: 14200 | Loss: 0.0026127651598694037\n",
      "Epoch: 14300 | Loss: 0.002611961574823223\n",
      "Epoch: 14400 | Loss: 0.00261130044952706\n",
      "Epoch: 14500 | Loss: 0.0026105364024580986\n",
      "Epoch: 14600 | Loss: 0.0026098390278265775\n",
      "Epoch: 14700 | Loss: 0.002609036302257252\n",
      "Epoch: 14800 | Loss: 0.00260832412849347\n",
      "Epoch: 14900 | Loss: 0.002607598944012921\n",
      "time=257.4157717227936\n",
      "error=0.0026069208081678993\n",
      "best loss=0.0026069208081678993\n",
      "best epoch=14999\n",
      "32\n",
      "Epoch: 0 | Loss: 0.534365778418515\n",
      "Epoch: 100 | Loss: 0.1342808223965651\n",
      "Epoch: 200 | Loss: 0.04001326510561925\n",
      "Epoch: 300 | Loss: 0.027367278702413157\n",
      "Epoch: 400 | Loss: 0.02278941049557131\n",
      "Epoch: 500 | Loss: 0.02045080765015666\n",
      "Epoch: 600 | Loss: 0.019131004854092228\n",
      "Epoch: 700 | Loss: 0.018159193253911375\n",
      "Epoch: 800 | Loss: 0.017314986991353875\n",
      "Epoch: 900 | Loss: 0.016563806125146286\n",
      "Epoch: 1000 | Loss: 0.015922503149112022\n",
      "Epoch: 1100 | Loss: 0.015376135391084386\n",
      "Epoch: 1200 | Loss: 0.014909408314145403\n",
      "Epoch: 1300 | Loss: 0.01446688011953359\n",
      "Epoch: 1400 | Loss: 0.014047851679278108\n",
      "Epoch: 1500 | Loss: 0.01361578188699343\n",
      "Epoch: 1600 | Loss: 0.013190320490923578\n",
      "Epoch: 1700 | Loss: 0.012777050830572055\n",
      "Epoch: 1800 | Loss: 0.012340819057623086\n",
      "Epoch: 1900 | Loss: 0.011957267266625896\n",
      "Epoch: 2000 | Loss: 0.011611044222497681\n",
      "Epoch: 2100 | Loss: 0.011291466085051612\n",
      "Epoch: 2200 | Loss: 0.010975607141700715\n",
      "Epoch: 2300 | Loss: 0.010683065410310215\n",
      "Epoch: 2400 | Loss: 0.01035363087737481\n",
      "Epoch: 2500 | Loss: 0.01003381256795912\n",
      "Epoch: 2600 | Loss: 0.009919855807773915\n",
      "Epoch: 2700 | Loss: 0.009331359344209162\n",
      "Epoch: 2800 | Loss: 0.008961235307759996\n",
      "Epoch: 2900 | Loss: 0.008556732297933177\n",
      "Epoch: 3000 | Loss: 0.00816868531566067\n",
      "Epoch: 3100 | Loss: 0.008013534077517783\n",
      "Epoch: 3200 | Loss: 0.00786810232923799\n",
      "Epoch: 3300 | Loss: 0.0077260676926510265\n",
      "Epoch: 3400 | Loss: 0.007587514646913551\n",
      "Epoch: 3500 | Loss: 0.007452434671147789\n",
      "Epoch: 3600 | Loss: 0.007323322757209441\n",
      "Epoch: 3700 | Loss: 0.007191412858695142\n",
      "Epoch: 3800 | Loss: 0.007063377753964533\n",
      "Epoch: 3900 | Loss: 0.0069331859734798\n",
      "Epoch: 4000 | Loss: 0.006798643531236522\n",
      "Epoch: 4100 | Loss: 0.006659400182511146\n",
      "Epoch: 4200 | Loss: 0.006518561485851207\n",
      "Epoch: 4300 | Loss: 0.006372715036203986\n",
      "Epoch: 4400 | Loss: 0.006228427150090201\n",
      "Epoch: 4500 | Loss: 0.006086342302739077\n",
      "Epoch: 4600 | Loss: 0.005950850507571697\n",
      "Epoch: 4700 | Loss: 0.005817313474132101\n",
      "Epoch: 4800 | Loss: 0.005687847844443561\n",
      "Epoch: 4900 | Loss: 0.0055548671674035405\n",
      "Epoch: 5000 | Loss: 0.005405421436577271\n",
      "Epoch: 5100 | Loss: 0.005217747694000508\n",
      "Epoch: 5200 | Loss: 0.0049690138421751035\n",
      "Epoch: 5300 | Loss: 0.004671641502333575\n",
      "Epoch: 5400 | Loss: 0.0044205098219102954\n",
      "Epoch: 5500 | Loss: 0.004243328688809283\n",
      "Epoch: 5600 | Loss: 0.0041245627432216795\n",
      "Epoch: 5700 | Loss: 0.004059533406942585\n",
      "Epoch: 5800 | Loss: 0.004007870029366876\n",
      "Epoch: 5900 | Loss: 0.003960033923654109\n",
      "Epoch: 6000 | Loss: 0.003912027471268841\n",
      "Epoch: 6100 | Loss: 0.0038878455978284127\n",
      "Epoch: 6200 | Loss: 0.0038625474080884736\n",
      "Epoch: 6300 | Loss: 0.003836120849634406\n",
      "Epoch: 6400 | Loss: 0.0038089715805794833\n",
      "Epoch: 6500 | Loss: 0.0037818502692783056\n",
      "Epoch: 6600 | Loss: 0.0037554876564291394\n",
      "Epoch: 6700 | Loss: 0.0037302237254913926\n",
      "Epoch: 6800 | Loss: 0.0037060149394052555\n",
      "Epoch: 6900 | Loss: 0.0036826791704168\n",
      "Epoch: 7000 | Loss: 0.0036600751219426396\n",
      "Epoch: 7100 | Loss: 0.0036381440563900917\n",
      "Epoch: 7200 | Loss: 0.003616915566073286\n",
      "Epoch: 7300 | Loss: 0.0035963845533958025\n",
      "Epoch: 7400 | Loss: 0.003576857050013189\n",
      "Epoch: 7500 | Loss: 0.003557390046177918\n",
      "Epoch: 7600 | Loss: 0.003537415115748453\n",
      "Epoch: 7700 | Loss: 0.0035184133610180078\n",
      "Epoch: 7800 | Loss: 0.003498637365626233\n",
      "Epoch: 7900 | Loss: 0.0034786125516140156\n",
      "Epoch: 8000 | Loss: 0.003458247388425526\n",
      "Epoch: 8100 | Loss: 0.0034364832948616288\n",
      "Epoch: 8200 | Loss: 0.003415189985195303\n",
      "Epoch: 8300 | Loss: 0.003391476416463918\n",
      "Epoch: 8400 | Loss: 0.0033644159829664955\n",
      "Epoch: 8500 | Loss: 0.003341178712463507\n",
      "Epoch: 8600 | Loss: 0.003310064199519739\n",
      "Epoch: 8700 | Loss: 0.0032748936473448497\n",
      "Epoch: 8800 | Loss: 0.0032354481682454923\n",
      "Epoch: 8900 | Loss: 0.003190932895222436\n",
      "Epoch: 9000 | Loss: 0.0031418795436351614\n",
      "Epoch: 9100 | Loss: 0.0031172385438518312\n",
      "Epoch: 9200 | Loss: 0.0030926657943839116\n",
      "Epoch: 9300 | Loss: 0.0030680926169988987\n",
      "Epoch: 9400 | Loss: 0.003043324251301517\n",
      "Epoch: 9500 | Loss: 0.003018059418067459\n",
      "Epoch: 9600 | Loss: 0.002991841392511383\n",
      "Epoch: 9700 | Loss: 0.0029640985730033254\n",
      "Epoch: 9800 | Loss: 0.0029341671378571256\n",
      "Epoch: 9900 | Loss: 0.0029014621262401746\n",
      "Epoch: 10000 | Loss: 0.0028660010282163762\n",
      "Epoch: 10100 | Loss: 0.0028291874595220527\n",
      "Epoch: 10200 | Loss: 0.0027940359342126875\n",
      "Epoch: 10300 | Loss: 0.002763745718618791\n",
      "Epoch: 10400 | Loss: 0.0027395340991131388\n",
      "Epoch: 10500 | Loss: 0.002720385018732372\n",
      "Epoch: 10600 | Loss: 0.0027050835660469615\n",
      "Epoch: 10700 | Loss: 0.0026915593470493897\n",
      "Epoch: 10800 | Loss: 0.0026799049245388286\n",
      "Epoch: 10900 | Loss: 0.002669168021058082\n",
      "Epoch: 11000 | Loss: 0.002659306505963327\n",
      "Epoch: 11100 | Loss: 0.0026497311863656987\n",
      "Epoch: 11200 | Loss: 0.0026394691597701673\n",
      "Epoch: 11300 | Loss: 0.002629227665887869\n",
      "Epoch: 11400 | Loss: 0.002620328913672849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11500 | Loss: 0.002611761790167379\n",
      "Epoch: 11600 | Loss: 0.0026033078136169807\n",
      "Epoch: 11700 | Loss: 0.002595182849107038\n",
      "Epoch: 11800 | Loss: 0.0025878390726353576\n",
      "Epoch: 11900 | Loss: 0.00257981985222469\n",
      "Epoch: 12000 | Loss: 0.002572358592357535\n",
      "Epoch: 12100 | Loss: 0.002568731013896872\n",
      "Epoch: 12200 | Loss: 0.0025651127868237464\n",
      "Epoch: 12300 | Loss: 0.0025614977821649186\n",
      "Epoch: 12400 | Loss: 0.0025578817040058962\n",
      "Epoch: 12500 | Loss: 0.002554259579585971\n",
      "Epoch: 12600 | Loss: 0.0025506254413796507\n",
      "Epoch: 12700 | Loss: 0.0025469719698758453\n",
      "Epoch: 12800 | Loss: 0.002543290109944199\n",
      "Epoch: 12900 | Loss: 0.002539568801172571\n",
      "Epoch: 13000 | Loss: 0.002535795176583542\n",
      "Epoch: 13100 | Loss: 0.0025319556883087605\n",
      "Epoch: 13200 | Loss: 0.002528036765104289\n",
      "Epoch: 13300 | Loss: 0.0025239917878552174\n",
      "Epoch: 13400 | Loss: 0.0025196582920021552\n",
      "Epoch: 13500 | Loss: 0.0025154956311280213\n",
      "Epoch: 13600 | Loss: 0.002511364160558035\n",
      "Epoch: 13700 | Loss: 0.0025073108696261172\n",
      "Epoch: 13800 | Loss: 0.00250324390223232\n",
      "Epoch: 13900 | Loss: 0.002499123033876762\n",
      "Epoch: 14000 | Loss: 0.002495105272793021\n",
      "Epoch: 14100 | Loss: 0.0024909882730157966\n",
      "Epoch: 14200 | Loss: 0.0024869095076137417\n",
      "Epoch: 14300 | Loss: 0.002482789942996348\n",
      "Epoch: 14400 | Loss: 0.0024787154896840444\n",
      "Epoch: 14500 | Loss: 0.0024744231489676693\n",
      "Epoch: 14600 | Loss: 0.0024700989968063436\n",
      "Epoch: 14700 | Loss: 0.0024656815881562303\n",
      "Epoch: 14800 | Loss: 0.002461083799687013\n",
      "Epoch: 14900 | Loss: 0.002456191896187207\n",
      "time=277.20100569725037\n",
      "error=0.0024510094870243584\n",
      "best loss=0.0024510094870243584\n",
      "best epoch=14999\n",
      "36\n",
      "Epoch: 0 | Loss: 0.5509114014824964\n",
      "Epoch: 100 | Loss: 0.16808198672593172\n",
      "Epoch: 200 | Loss: 0.05770498090642044\n",
      "Epoch: 300 | Loss: 0.03892711300128549\n",
      "Epoch: 400 | Loss: 0.03175380350770641\n",
      "Epoch: 500 | Loss: 0.0277379647977272\n",
      "Epoch: 600 | Loss: 0.025145218720749662\n",
      "Epoch: 700 | Loss: 0.023231502608390398\n",
      "Epoch: 800 | Loss: 0.02171912329855688\n",
      "Epoch: 900 | Loss: 0.020431438874134965\n",
      "Epoch: 1000 | Loss: 0.019280098349510836\n",
      "Epoch: 1100 | Loss: 0.018211696646786887\n",
      "Epoch: 1200 | Loss: 0.01718960191307742\n",
      "Epoch: 1300 | Loss: 0.016143043604220357\n",
      "Epoch: 1400 | Loss: 0.015033261413977659\n",
      "Epoch: 1500 | Loss: 0.01399318943753599\n",
      "Epoch: 1600 | Loss: 0.012921153906432302\n",
      "Epoch: 1700 | Loss: 0.011870042210097548\n",
      "Epoch: 1800 | Loss: 0.010924110423096196\n",
      "Epoch: 1900 | Loss: 0.01003939084427688\n",
      "Epoch: 2000 | Loss: 0.009148467236998132\n",
      "Epoch: 2100 | Loss: 0.008311135430737191\n",
      "Epoch: 2200 | Loss: 0.0076125856714433626\n",
      "Epoch: 2300 | Loss: 0.0070470363630092386\n",
      "Epoch: 2400 | Loss: 0.006526559355471898\n",
      "Epoch: 2500 | Loss: 0.006069287728703545\n",
      "Epoch: 2600 | Loss: 0.005633802645616315\n",
      "Epoch: 2700 | Loss: 0.0052682762626209126\n",
      "Epoch: 2800 | Loss: 0.005039739841739352\n",
      "Epoch: 2900 | Loss: 0.004882259162023487\n",
      "Epoch: 3000 | Loss: 0.004748522897850872\n",
      "Epoch: 3100 | Loss: 0.004687532742607222\n",
      "Epoch: 3200 | Loss: 0.004626237075661834\n",
      "Epoch: 3300 | Loss: 0.004563827595367262\n",
      "Epoch: 3400 | Loss: 0.004499762153485385\n",
      "Epoch: 3500 | Loss: 0.0044341371698054625\n",
      "Epoch: 3600 | Loss: 0.004367651578108656\n",
      "Epoch: 3700 | Loss: 0.004300781807029106\n",
      "Epoch: 3800 | Loss: 0.0042334288346285685\n",
      "Epoch: 3900 | Loss: 0.004165668450725107\n",
      "Epoch: 4000 | Loss: 0.004097741236163991\n",
      "Epoch: 4100 | Loss: 0.004029699255642039\n",
      "Epoch: 4200 | Loss: 0.003962502016216951\n",
      "Epoch: 4300 | Loss: 0.003898139276538334\n",
      "Epoch: 4400 | Loss: 0.0038390395383497674\n",
      "Epoch: 4500 | Loss: 0.003783607402544533\n",
      "Epoch: 4600 | Loss: 0.0037336447856653006\n",
      "Epoch: 4700 | Loss: 0.003683135942707782\n",
      "Epoch: 4800 | Loss: 0.0036377137403894692\n",
      "Epoch: 4900 | Loss: 0.003594821575071518\n",
      "Epoch: 5000 | Loss: 0.003550784818696363\n",
      "Epoch: 5100 | Loss: 0.003507588547926578\n",
      "Epoch: 5200 | Loss: 0.003467064559745034\n",
      "Epoch: 5300 | Loss: 0.0034316685660659967\n",
      "Epoch: 5400 | Loss: 0.003396241282885762\n",
      "Epoch: 5500 | Loss: 0.003362990803674538\n",
      "Epoch: 5600 | Loss: 0.003332015202459423\n",
      "Epoch: 5700 | Loss: 0.00330287696020395\n",
      "Epoch: 5800 | Loss: 0.003272733744143586\n",
      "Epoch: 5900 | Loss: 0.0032398274757503547\n",
      "Epoch: 6000 | Loss: 0.003203577598339833\n",
      "Epoch: 6100 | Loss: 0.0031867574476078424\n",
      "Epoch: 6200 | Loss: 0.0031695360016545365\n",
      "Epoch: 6300 | Loss: 0.0031519731299397675\n",
      "Epoch: 6400 | Loss: 0.0031341442089522355\n",
      "Epoch: 6500 | Loss: 0.0031161269349619138\n",
      "Epoch: 6600 | Loss: 0.003097997924477177\n",
      "Epoch: 6700 | Loss: 0.0030798320898593502\n",
      "Epoch: 6800 | Loss: 0.0030617077807623978\n",
      "Epoch: 6900 | Loss: 0.003043716192845915\n",
      "Epoch: 7000 | Loss: 0.003025963295954992\n",
      "Epoch: 7100 | Loss: 0.003008549427291696\n",
      "Epoch: 7200 | Loss: 0.0029915341256312265\n",
      "Epoch: 7300 | Loss: 0.002974917657828327\n",
      "Epoch: 7400 | Loss: 0.0029587331975956064\n",
      "Epoch: 7500 | Loss: 0.0029434342427631156\n",
      "Epoch: 7600 | Loss: 0.0029272906488858044\n",
      "Epoch: 7700 | Loss: 0.0029117195557469572\n",
      "Epoch: 7800 | Loss: 0.0028968095500020424\n",
      "Epoch: 7900 | Loss: 0.0028808741642154735\n",
      "Epoch: 8000 | Loss: 0.002866132211415059\n",
      "Epoch: 8100 | Loss: 0.002850917710989061\n",
      "Epoch: 8200 | Loss: 0.0028357743839280075\n",
      "Epoch: 8300 | Loss: 0.0028209360383292304\n",
      "Epoch: 8400 | Loss: 0.002805880611321254\n",
      "Epoch: 8500 | Loss: 0.0027911547163618827\n",
      "Epoch: 8600 | Loss: 0.002776497016512282\n",
      "Epoch: 8700 | Loss: 0.002763151048199936\n",
      "Epoch: 8800 | Loss: 0.0027461886669794674\n",
      "Epoch: 8900 | Loss: 0.002731314190772695\n",
      "Epoch: 9000 | Loss: 0.0027165887245618903\n",
      "Epoch: 9100 | Loss: 0.0027092722063952928\n",
      "Epoch: 9200 | Loss: 0.0027019003875757164\n",
      "Epoch: 9300 | Loss: 0.0026944900867053832\n",
      "Epoch: 9400 | Loss: 0.002687064597579434\n",
      "Epoch: 9500 | Loss: 0.002679643054761693\n",
      "Epoch: 9600 | Loss: 0.002672236810862802\n",
      "Epoch: 9700 | Loss: 0.0026648469223633743\n",
      "Epoch: 9800 | Loss: 0.0026574637657796243\n",
      "Epoch: 9900 | Loss: 0.0026500686146728267\n",
      "Epoch: 10000 | Loss: 0.002642636045414389\n",
      "Epoch: 10100 | Loss: 0.002635136139560408\n",
      "Epoch: 10200 | Loss: 0.00262754333374008\n",
      "Epoch: 10300 | Loss: 0.0026198343014520007\n",
      "Epoch: 10400 | Loss: 0.0026120312570298873\n",
      "Epoch: 10500 | Loss: 0.002604166715417406\n",
      "Epoch: 10600 | Loss: 0.0025961881663909532\n",
      "Epoch: 10700 | Loss: 0.0025880586968044786\n",
      "Epoch: 10800 | Loss: 0.0025799055267990228\n",
      "Epoch: 10900 | Loss: 0.0025722140429005726\n",
      "Epoch: 11000 | Loss: 0.0025648565515090684\n",
      "Epoch: 11100 | Loss: 0.0025577099037493362\n",
      "Epoch: 11200 | Loss: 0.0025512042877652375\n",
      "Epoch: 11300 | Loss: 0.00254544732362347\n",
      "Epoch: 11400 | Loss: 0.002539935180548227\n",
      "Epoch: 11500 | Loss: 0.002534883298256827\n",
      "Epoch: 11600 | Loss: 0.002530361796987411\n",
      "Epoch: 11700 | Loss: 0.002526166553077006\n",
      "Epoch: 11800 | Loss: 0.0025226765467598975\n",
      "Epoch: 11900 | Loss: 0.0025184536064060915\n",
      "Epoch: 12000 | Loss: 0.0025145523632672537\n",
      "Epoch: 12100 | Loss: 0.002512708721392322\n",
      "Epoch: 12200 | Loss: 0.0025108456661872627\n",
      "Epoch: 12300 | Loss: 0.002508961372480929\n",
      "Epoch: 12400 | Loss: 0.0025070546233295164\n",
      "Epoch: 12500 | Loss: 0.0025051241505366553\n",
      "Epoch: 12600 | Loss: 0.0025031685445482685\n",
      "Epoch: 12700 | Loss: 0.0025011862012915536\n",
      "Epoch: 12800 | Loss: 0.0024991753002005365\n",
      "Epoch: 12900 | Loss: 0.002497133799419584\n",
      "Epoch: 13000 | Loss: 0.0024950594314027954\n",
      "Epoch: 13100 | Loss: 0.0024929496849445667\n",
      "Epoch: 13200 | Loss: 0.0024908017642849625\n",
      "Epoch: 13300 | Loss: 0.002488612519575978\n",
      "Epoch: 13400 | Loss: 0.002486378346108071\n",
      "Epoch: 13500 | Loss: 0.002484139747345319\n",
      "Epoch: 13600 | Loss: 0.00248188994269607\n",
      "Epoch: 13700 | Loss: 0.0024796199883496696\n",
      "Epoch: 13800 | Loss: 0.0024773916769502543\n",
      "Epoch: 13900 | Loss: 0.0024749219114270906\n",
      "Epoch: 14000 | Loss: 0.002472604478361214\n",
      "Epoch: 14100 | Loss: 0.0024701068226838387\n",
      "Epoch: 14200 | Loss: 0.0024676797661000007\n",
      "Epoch: 14300 | Loss: 0.002465066656386922\n",
      "Epoch: 14400 | Loss: 0.0024624707423793173\n",
      "Epoch: 14500 | Loss: 0.0024598607737630904\n",
      "Epoch: 14600 | Loss: 0.0024570458488440755\n",
      "Epoch: 14700 | Loss: 0.0024542551593866365\n",
      "Epoch: 14800 | Loss: 0.0024513243991547903\n",
      "Epoch: 14900 | Loss: 0.0024483665973278304\n",
      "time=289.8863079547882\n",
      "error=0.0024451943861966896\n",
      "best loss=0.0024451943861966896\n",
      "best epoch=14999\n",
      "40\n",
      "Epoch: 0 | Loss: 0.568320834262247\n",
      "Epoch: 100 | Loss: 0.20311760043977922\n",
      "Epoch: 200 | Loss: 0.06499176145996914\n",
      "Epoch: 300 | Loss: 0.03933676631033556\n",
      "Epoch: 400 | Loss: 0.030106499019609458\n",
      "Epoch: 500 | Loss: 0.024845840268390713\n",
      "Epoch: 600 | Loss: 0.02132179803995674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 700 | Loss: 0.018834110878256254\n",
      "Epoch: 800 | Loss: 0.016925698818904808\n",
      "Epoch: 900 | Loss: 0.015409576879661876\n",
      "Epoch: 1000 | Loss: 0.01421243644349049\n",
      "Epoch: 1100 | Loss: 0.013197885352044593\n",
      "Epoch: 1200 | Loss: 0.012330932415643661\n",
      "Epoch: 1300 | Loss: 0.011557989653247239\n",
      "Epoch: 1400 | Loss: 0.010874303952163145\n",
      "Epoch: 1500 | Loss: 0.010275069697039252\n",
      "Epoch: 1600 | Loss: 0.009742321247911044\n",
      "Epoch: 1700 | Loss: 0.009262307967165309\n",
      "Epoch: 1800 | Loss: 0.008806979652301804\n",
      "Epoch: 1900 | Loss: 0.008391198417024785\n",
      "Epoch: 2000 | Loss: 0.008003370599209013\n",
      "Epoch: 2100 | Loss: 0.007608044561046266\n",
      "Epoch: 2200 | Loss: 0.0072415088988910005\n",
      "Epoch: 2300 | Loss: 0.00695216761819339\n",
      "Epoch: 2400 | Loss: 0.006742809349135662\n",
      "Epoch: 2500 | Loss: 0.006574265182109129\n",
      "Epoch: 2600 | Loss: 0.006428409696966792\n",
      "Epoch: 2700 | Loss: 0.006300917372352872\n",
      "Epoch: 2800 | Loss: 0.006190742886810546\n",
      "Epoch: 2900 | Loss: 0.006094599899951501\n",
      "Epoch: 3000 | Loss: 0.005986076446482831\n",
      "Epoch: 3100 | Loss: 0.005939034578186412\n",
      "Epoch: 3200 | Loss: 0.005891266028662332\n",
      "Epoch: 3300 | Loss: 0.005842285410812759\n",
      "Epoch: 3400 | Loss: 0.00579154177838709\n",
      "Epoch: 3500 | Loss: 0.005738400519034242\n",
      "Epoch: 3600 | Loss: 0.0056822403987642975\n",
      "Epoch: 3700 | Loss: 0.005622464588576346\n",
      "Epoch: 3800 | Loss: 0.0055586449814875195\n",
      "Epoch: 3900 | Loss: 0.005490532379006145\n",
      "Epoch: 4000 | Loss: 0.005418357578885897\n",
      "Epoch: 4100 | Loss: 0.005343121125298962\n",
      "Epoch: 4200 | Loss: 0.005266517594492315\n",
      "Epoch: 4300 | Loss: 0.005190975935506416\n",
      "Epoch: 4400 | Loss: 0.005117304775600944\n",
      "Epoch: 4500 | Loss: 0.00504568833310083\n",
      "Epoch: 4600 | Loss: 0.0049748421401357296\n",
      "Epoch: 4700 | Loss: 0.0049045481552720815\n",
      "Epoch: 4800 | Loss: 0.0048283895947662395\n",
      "Epoch: 4900 | Loss: 0.004755545629719112\n",
      "Epoch: 5000 | Loss: 0.004681664740777719\n",
      "Epoch: 5100 | Loss: 0.004614276314763549\n",
      "Epoch: 5200 | Loss: 0.004550995518383902\n",
      "Epoch: 5300 | Loss: 0.004492727943298251\n",
      "Epoch: 5400 | Loss: 0.004440155126881746\n",
      "Epoch: 5500 | Loss: 0.0043884380523462035\n",
      "Epoch: 5600 | Loss: 0.004348426368989085\n",
      "Epoch: 5700 | Loss: 0.00430419328732886\n",
      "Epoch: 5800 | Loss: 0.004267626584245165\n",
      "Epoch: 5900 | Loss: 0.004229008601147007\n",
      "Epoch: 6000 | Loss: 0.004190967425808652\n",
      "Epoch: 6100 | Loss: 0.004171389011361203\n",
      "Epoch: 6200 | Loss: 0.00415094499167003\n",
      "Epoch: 6300 | Loss: 0.00412959685591876\n",
      "Epoch: 6400 | Loss: 0.004107307754780982\n",
      "Epoch: 6500 | Loss: 0.004084083733312684\n",
      "Epoch: 6600 | Loss: 0.00406000758544487\n",
      "Epoch: 6700 | Loss: 0.004035257459207186\n",
      "Epoch: 6800 | Loss: 0.004010093981373455\n",
      "Epoch: 6900 | Loss: 0.003984786697850291\n",
      "Epoch: 7000 | Loss: 0.0039593960391935456\n",
      "Epoch: 7100 | Loss: 0.0039343100604463145\n",
      "Epoch: 7200 | Loss: 0.003909493444813214\n",
      "Epoch: 7300 | Loss: 0.003884498374422657\n",
      "Epoch: 7400 | Loss: 0.0038597486717971865\n",
      "Epoch: 7500 | Loss: 0.003834476891346145\n",
      "Epoch: 7600 | Loss: 0.00380985198020313\n",
      "Epoch: 7700 | Loss: 0.003784960619274161\n",
      "Epoch: 7800 | Loss: 0.0037596112013397317\n",
      "Epoch: 7900 | Loss: 0.003734325952147946\n",
      "Epoch: 8000 | Loss: 0.003709676957080467\n",
      "Epoch: 8100 | Loss: 0.0036855778638008718\n",
      "Epoch: 8200 | Loss: 0.003661727691193982\n",
      "Epoch: 8300 | Loss: 0.0036378263299801433\n",
      "Epoch: 8400 | Loss: 0.003616165079436218\n",
      "Epoch: 8500 | Loss: 0.003592069623572371\n",
      "Epoch: 8600 | Loss: 0.0035700832145533143\n",
      "Epoch: 8700 | Loss: 0.003547834569835168\n",
      "Epoch: 8800 | Loss: 0.003526226533207908\n",
      "Epoch: 8900 | Loss: 0.003504650717816583\n",
      "Epoch: 9000 | Loss: 0.0034831111954389837\n",
      "Epoch: 9100 | Loss: 0.003472262256508251\n",
      "Epoch: 9200 | Loss: 0.0034611309121871867\n",
      "Epoch: 9300 | Loss: 0.0034496887763505193\n",
      "Epoch: 9400 | Loss: 0.0034379126368851032\n",
      "Epoch: 9500 | Loss: 0.0034257839798046822\n",
      "Epoch: 9600 | Loss: 0.0034132947153260426\n",
      "Epoch: 9700 | Loss: 0.0034004588206193553\n",
      "Epoch: 9800 | Loss: 0.0033873189877832383\n",
      "Epoch: 9900 | Loss: 0.0033739264572104563\n",
      "Epoch: 10000 | Loss: 0.00336030858581314\n",
      "Epoch: 10100 | Loss: 0.003346458155951665\n",
      "Epoch: 10200 | Loss: 0.0033323394024577947\n",
      "Epoch: 10300 | Loss: 0.0033179242250002345\n",
      "Epoch: 10400 | Loss: 0.003303245309438157\n",
      "Epoch: 10500 | Loss: 0.003288005122131131\n",
      "Epoch: 10600 | Loss: 0.0032724613716402897\n",
      "Epoch: 10700 | Loss: 0.0032569083555255927\n",
      "Epoch: 10800 | Loss: 0.0032413794249174163\n",
      "Epoch: 10900 | Loss: 0.0032258109714484043\n",
      "Epoch: 11000 | Loss: 0.003210349388842966\n",
      "Epoch: 11100 | Loss: 0.003194906435019258\n",
      "Epoch: 11200 | Loss: 0.0031794438957589667\n",
      "Epoch: 11300 | Loss: 0.0031641223365561615\n",
      "Epoch: 11400 | Loss: 0.0031490380624018613\n",
      "Epoch: 11500 | Loss: 0.0031341614956487225\n",
      "Epoch: 11600 | Loss: 0.003119364288415527\n",
      "Epoch: 11700 | Loss: 0.003105023641469764\n",
      "Epoch: 11800 | Loss: 0.003090102788422012\n",
      "Epoch: 11900 | Loss: 0.0030756950529758063\n",
      "Epoch: 12000 | Loss: 0.003061005797499858\n",
      "Epoch: 12100 | Loss: 0.003053581950392798\n",
      "Epoch: 12200 | Loss: 0.0030459702992511827\n",
      "Epoch: 12300 | Loss: 0.003038156967462234\n",
      "Epoch: 12400 | Loss: 0.0030301278148864652\n",
      "Epoch: 12500 | Loss: 0.0030218660159763077\n",
      "Epoch: 12600 | Loss: 0.003013351844722588\n",
      "Epoch: 12700 | Loss: 0.0030045647648821432\n",
      "Epoch: 12800 | Loss: 0.002995489574230452\n",
      "Epoch: 12900 | Loss: 0.0029861268496041266\n",
      "Epoch: 13000 | Loss: 0.002976504245303678\n",
      "Epoch: 13100 | Loss: 0.0029666820485380295\n",
      "Epoch: 13200 | Loss: 0.002956748182211203\n",
      "Epoch: 13300 | Loss: 0.002946819046810281\n",
      "Epoch: 13400 | Loss: 0.0029369879393629453\n",
      "Epoch: 13500 | Loss: 0.0029273325596650907\n",
      "Epoch: 13600 | Loss: 0.002917978634477657\n",
      "Epoch: 13700 | Loss: 0.002908909460441475\n",
      "Epoch: 13800 | Loss: 0.00290017865876688\n",
      "Epoch: 13900 | Loss: 0.0028917457543362725\n",
      "Epoch: 14000 | Loss: 0.0028836192106603247\n",
      "Epoch: 14100 | Loss: 0.0028757569988866745\n",
      "Epoch: 14200 | Loss: 0.0028681256686871794\n",
      "Epoch: 14300 | Loss: 0.0028607081175421106\n",
      "Epoch: 14400 | Loss: 0.002853475156339853\n",
      "Epoch: 14500 | Loss: 0.0028463660515282967\n",
      "Epoch: 14600 | Loss: 0.002839396289516527\n",
      "Epoch: 14700 | Loss: 0.0028324988154993956\n",
      "Epoch: 14800 | Loss: 0.0028256660358238666\n",
      "Epoch: 14900 | Loss: 0.002818953492643453\n",
      "time=314.0684962272644\n",
      "error=0.0028122299567967177\n",
      "best loss=0.0028122299567967177\n",
      "best epoch=14999\n",
      "44\n",
      "Epoch: 0 | Loss: 0.58673241172987\n",
      "Epoch: 100 | Loss: 0.2280107721287784\n",
      "Epoch: 200 | Loss: 0.0754848575943156\n",
      "Epoch: 300 | Loss: 0.04395426723467949\n",
      "Epoch: 400 | Loss: 0.033940098984429665\n",
      "Epoch: 500 | Loss: 0.028384327408310345\n",
      "Epoch: 600 | Loss: 0.024603793564549274\n",
      "Epoch: 700 | Loss: 0.02193551687482967\n",
      "Epoch: 800 | Loss: 0.019945033860595488\n",
      "Epoch: 900 | Loss: 0.01839004655904493\n",
      "Epoch: 1000 | Loss: 0.017048665947140596\n",
      "Epoch: 1100 | Loss: 0.015852703089513885\n",
      "Epoch: 1200 | Loss: 0.014751969806709262\n",
      "Epoch: 1300 | Loss: 0.013765872730724403\n",
      "Epoch: 1400 | Loss: 0.012904286184323512\n",
      "Epoch: 1500 | Loss: 0.012133701921265803\n",
      "Epoch: 1600 | Loss: 0.011483170025544262\n",
      "Epoch: 1700 | Loss: 0.010954441914754788\n",
      "Epoch: 1800 | Loss: 0.010524817128330298\n",
      "Epoch: 1900 | Loss: 0.010131544549497329\n",
      "Epoch: 2000 | Loss: 0.009781545113633924\n",
      "Epoch: 2100 | Loss: 0.009457840674287083\n",
      "Epoch: 2200 | Loss: 0.009170397059335263\n",
      "Epoch: 2300 | Loss: 0.00889767229849865\n",
      "Epoch: 2400 | Loss: 0.00864068359961637\n",
      "Epoch: 2500 | Loss: 0.008400421318050334\n",
      "Epoch: 2600 | Loss: 0.008142622636201313\n",
      "Epoch: 2700 | Loss: 0.007902136705513194\n",
      "Epoch: 2800 | Loss: 0.007672105887394018\n",
      "Epoch: 2900 | Loss: 0.007466976495992227\n",
      "Epoch: 3000 | Loss: 0.007279709267530751\n",
      "Epoch: 3100 | Loss: 0.007197515396515895\n",
      "Epoch: 3200 | Loss: 0.007118980646929023\n",
      "Epoch: 3300 | Loss: 0.007043573157641519\n",
      "Epoch: 3400 | Loss: 0.006971703730194239\n",
      "Epoch: 3500 | Loss: 0.0069042644102961966\n",
      "Epoch: 3600 | Loss: 0.006841554792107936\n",
      "Epoch: 3700 | Loss: 0.006783181902657606\n",
      "Epoch: 3800 | Loss: 0.006728076254376972\n",
      "Epoch: 3900 | Loss: 0.006675218186951579\n",
      "Epoch: 4000 | Loss: 0.006623996498085722\n",
      "Epoch: 4100 | Loss: 0.006573855510286593\n",
      "Epoch: 4200 | Loss: 0.006524352232150152\n",
      "Epoch: 4300 | Loss: 0.006474733329826609\n",
      "Epoch: 4400 | Loss: 0.006426495006258025\n",
      "Epoch: 4500 | Loss: 0.0063764460844031885\n",
      "Epoch: 4600 | Loss: 0.006325745279347073\n",
      "Epoch: 4700 | Loss: 0.006273298336789345\n",
      "Epoch: 4800 | Loss: 0.006217187967070315\n",
      "Epoch: 4900 | Loss: 0.0061562037085083066\n",
      "Epoch: 5000 | Loss: 0.006092900165152274\n",
      "Epoch: 5100 | Loss: 0.006027875291315477\n",
      "Epoch: 5200 | Loss: 0.0059635588155139176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5300 | Loss: 0.005902962103021632\n",
      "Epoch: 5400 | Loss: 0.00584705117246165\n",
      "Epoch: 5500 | Loss: 0.005797339472559482\n",
      "Epoch: 5600 | Loss: 0.005750329870518873\n",
      "Epoch: 5700 | Loss: 0.005709282262856977\n",
      "Epoch: 5800 | Loss: 0.005672642208881393\n",
      "Epoch: 5900 | Loss: 0.005627446687069498\n",
      "Epoch: 6000 | Loss: 0.005588013578370956\n",
      "Epoch: 6100 | Loss: 0.00556834909332901\n",
      "Epoch: 6200 | Loss: 0.0055481019897989335\n",
      "Epoch: 6300 | Loss: 0.005527200687505715\n",
      "Epoch: 6400 | Loss: 0.005505595090512143\n",
      "Epoch: 6500 | Loss: 0.005483261400514219\n",
      "Epoch: 6600 | Loss: 0.005460213096069583\n",
      "Epoch: 6700 | Loss: 0.005436501930092463\n",
      "Epoch: 6800 | Loss: 0.005412198658022489\n",
      "Epoch: 6900 | Loss: 0.005387350456295171\n",
      "Epoch: 7000 | Loss: 0.005361931588544167\n",
      "Epoch: 7100 | Loss: 0.005335826164139709\n",
      "Epoch: 7200 | Loss: 0.005308834662280134\n",
      "Epoch: 7300 | Loss: 0.005280986211124267\n",
      "Epoch: 7400 | Loss: 0.005251617787813655\n",
      "Epoch: 7500 | Loss: 0.0052213952809574615\n",
      "Epoch: 7600 | Loss: 0.005190138541636875\n",
      "Epoch: 7700 | Loss: 0.005158150613469186\n",
      "Epoch: 7800 | Loss: 0.005125658142760243\n",
      "Epoch: 7900 | Loss: 0.005093519475826274\n",
      "Epoch: 8000 | Loss: 0.005061460200970654\n",
      "Epoch: 8100 | Loss: 0.005029341608234105\n",
      "Epoch: 8200 | Loss: 0.004997306502453329\n",
      "Epoch: 8300 | Loss: 0.004965556627616972\n",
      "Epoch: 8400 | Loss: 0.004933532587337476\n",
      "Epoch: 8500 | Loss: 0.004901629738029897\n",
      "Epoch: 8600 | Loss: 0.004869962024947338\n",
      "Epoch: 8700 | Loss: 0.004838199874619182\n",
      "Epoch: 8800 | Loss: 0.004806472223077382\n",
      "Epoch: 8900 | Loss: 0.004774190210858873\n",
      "Epoch: 9000 | Loss: 0.004739775264442159\n",
      "Epoch: 9100 | Loss: 0.0047224348989492685\n",
      "Epoch: 9200 | Loss: 0.004704455261094845\n",
      "Epoch: 9300 | Loss: 0.004685814477147693\n",
      "Epoch: 9400 | Loss: 0.004666503042433325\n",
      "Epoch: 9500 | Loss: 0.0046465033962809\n",
      "Epoch: 9600 | Loss: 0.004625780028600483\n",
      "Epoch: 9700 | Loss: 0.004604296395162064\n",
      "Epoch: 9800 | Loss: 0.00458206472991533\n",
      "Epoch: 9900 | Loss: 0.004559197096536448\n",
      "Epoch: 10000 | Loss: 0.004535904757439499\n",
      "Epoch: 10100 | Loss: 0.0045124242871096764\n",
      "Epoch: 10200 | Loss: 0.004488953465945883\n",
      "Epoch: 10300 | Loss: 0.004465482660032439\n",
      "Epoch: 10400 | Loss: 0.004441918502677153\n",
      "Epoch: 10500 | Loss: 0.004418082128503812\n",
      "Epoch: 10600 | Loss: 0.004393779873332558\n",
      "Epoch: 10700 | Loss: 0.004369203636173149\n",
      "Epoch: 10800 | Loss: 0.004344209938695097\n",
      "Epoch: 10900 | Loss: 0.004319205073012844\n",
      "Epoch: 11000 | Loss: 0.004294290781312226\n",
      "Epoch: 11100 | Loss: 0.004269399148105873\n",
      "Epoch: 11200 | Loss: 0.0042446287162439785\n",
      "Epoch: 11300 | Loss: 0.0042197713627645786\n",
      "Epoch: 11400 | Loss: 0.004194661735036052\n",
      "Epoch: 11500 | Loss: 0.004169032813034119\n",
      "Epoch: 11600 | Loss: 0.0041425535831026445\n",
      "Epoch: 11700 | Loss: 0.004115458168288718\n",
      "Epoch: 11800 | Loss: 0.0040872420379943816\n",
      "Epoch: 11900 | Loss: 0.004058270985393293\n",
      "Epoch: 12000 | Loss: 0.0040285740922914905\n",
      "Epoch: 12100 | Loss: 0.004013410569906008\n",
      "Epoch: 12200 | Loss: 0.003998034918236774\n",
      "Epoch: 12300 | Loss: 0.0039824349860839055\n",
      "Epoch: 12400 | Loss: 0.003966597852393453\n",
      "Epoch: 12500 | Loss: 0.003950513437120979\n",
      "Epoch: 12600 | Loss: 0.003934177026555338\n",
      "Epoch: 12700 | Loss: 0.003917590807860983\n",
      "Epoch: 12800 | Loss: 0.0039007656577440143\n",
      "Epoch: 12900 | Loss: 0.0038837228277566788\n",
      "Epoch: 13000 | Loss: 0.003866494058411294\n",
      "Epoch: 13100 | Loss: 0.003849119477978105\n",
      "Epoch: 13200 | Loss: 0.003831643597922588\n",
      "Epoch: 13300 | Loss: 0.003814118096573769\n",
      "Epoch: 13400 | Loss: 0.003796584845795253\n",
      "Epoch: 13500 | Loss: 0.0037790728146965815\n",
      "Epoch: 13600 | Loss: 0.0037615801904259585\n",
      "Epoch: 13700 | Loss: 0.003744083753397738\n",
      "Epoch: 13800 | Loss: 0.003726612448378872\n",
      "Epoch: 13900 | Loss: 0.0037091890887105457\n",
      "Epoch: 14000 | Loss: 0.003691922258140746\n",
      "Epoch: 14100 | Loss: 0.003674852359426029\n",
      "Epoch: 14200 | Loss: 0.0036579539751515743\n",
      "Epoch: 14300 | Loss: 0.003641239665921766\n",
      "Epoch: 14400 | Loss: 0.003624758920099843\n",
      "Epoch: 14500 | Loss: 0.0036084357755647163\n",
      "Epoch: 14600 | Loss: 0.003592378191732082\n",
      "Epoch: 14700 | Loss: 0.0035764805377123702\n",
      "Epoch: 14800 | Loss: 0.0035604212984436945\n",
      "Epoch: 14900 | Loss: 0.0035442731928486764\n",
      "time=322.56118607521057\n",
      "error=0.0035291201870042985\n",
      "best loss=0.0035291201870042985\n",
      "best epoch=14999\n",
      "48\n",
      "Epoch: 0 | Loss: 0.6062849502489416\n",
      "Epoch: 100 | Loss: 0.2512792924497841\n",
      "Epoch: 200 | Loss: 0.08500593846834129\n",
      "Epoch: 300 | Loss: 0.04723067247174169\n",
      "Epoch: 400 | Loss: 0.03602171579208212\n",
      "Epoch: 500 | Loss: 0.030119756372307877\n",
      "Epoch: 600 | Loss: 0.02637737578960765\n",
      "Epoch: 700 | Loss: 0.02363514688557448\n",
      "Epoch: 800 | Loss: 0.02146759152305349\n",
      "Epoch: 900 | Loss: 0.0196949792418112\n",
      "Epoch: 1000 | Loss: 0.018231636367800723\n",
      "Epoch: 1100 | Loss: 0.01694980844249177\n",
      "Epoch: 1200 | Loss: 0.01585523540768762\n",
      "Epoch: 1300 | Loss: 0.014945512838142803\n",
      "Epoch: 1400 | Loss: 0.01415565813798651\n",
      "Epoch: 1500 | Loss: 0.013471327352739683\n",
      "Epoch: 1600 | Loss: 0.012925346292257068\n",
      "Epoch: 1700 | Loss: 0.012441673236677688\n",
      "Epoch: 1800 | Loss: 0.011984641871893166\n",
      "Epoch: 1900 | Loss: 0.011575028518293378\n",
      "Epoch: 2000 | Loss: 0.011175399714329015\n",
      "Epoch: 2100 | Loss: 0.010805795249785687\n",
      "Epoch: 2200 | Loss: 0.01045260800999719\n",
      "Epoch: 2300 | Loss: 0.010108232972911132\n",
      "Epoch: 2400 | Loss: 0.009783804705429215\n",
      "Epoch: 2500 | Loss: 0.009483941935472577\n",
      "Epoch: 2600 | Loss: 0.009209894686154293\n",
      "Epoch: 2700 | Loss: 0.008956025961542196\n",
      "Epoch: 2800 | Loss: 0.008713851020162814\n",
      "Epoch: 2900 | Loss: 0.00848470376105799\n",
      "Epoch: 3000 | Loss: 0.008246396586916845\n",
      "Epoch: 3100 | Loss: 0.008135967053555711\n",
      "Epoch: 3200 | Loss: 0.008026007964461365\n",
      "Epoch: 3300 | Loss: 0.007918310327281165\n",
      "Epoch: 3400 | Loss: 0.007813780414560587\n",
      "Epoch: 3500 | Loss: 0.007710160414261676\n",
      "Epoch: 3600 | Loss: 0.007606373181528538\n",
      "Epoch: 3700 | Loss: 0.007503054510226981\n",
      "Epoch: 3800 | Loss: 0.007400585611810855\n",
      "Epoch: 3900 | Loss: 0.007299067853162363\n",
      "Epoch: 4000 | Loss: 0.007197096012535667\n",
      "Epoch: 4100 | Loss: 0.007093252608519721\n",
      "Epoch: 4200 | Loss: 0.006987884601815515\n",
      "Epoch: 4300 | Loss: 0.006883256106717469\n",
      "Epoch: 4400 | Loss: 0.006780031437661106\n",
      "Epoch: 4500 | Loss: 0.006680299414242037\n",
      "Epoch: 4600 | Loss: 0.006581419513535257\n",
      "Epoch: 4700 | Loss: 0.006481786481291401\n",
      "Epoch: 4800 | Loss: 0.006376331683818282\n",
      "Epoch: 4900 | Loss: 0.0062711418517814674\n",
      "Epoch: 5000 | Loss: 0.006161622349259884\n",
      "Epoch: 5100 | Loss: 0.006049784652360599\n",
      "Epoch: 5200 | Loss: 0.005936924231278699\n",
      "Epoch: 5300 | Loss: 0.005821388887719953\n",
      "Epoch: 5400 | Loss: 0.00569357356647603\n",
      "Epoch: 5500 | Loss: 0.005557889915687054\n",
      "Epoch: 5600 | Loss: 0.0054159075844863016\n",
      "Epoch: 5700 | Loss: 0.005269541690361177\n",
      "Epoch: 5800 | Loss: 0.005122913628689175\n",
      "Epoch: 5900 | Loss: 0.004987998616117335\n",
      "Epoch: 6000 | Loss: 0.004874540346132499\n",
      "Epoch: 6100 | Loss: 0.00482461198910277\n",
      "Epoch: 6200 | Loss: 0.004778195060434219\n",
      "Epoch: 6300 | Loss: 0.004735544959570539\n",
      "Epoch: 6400 | Loss: 0.00469678325653624\n",
      "Epoch: 6500 | Loss: 0.004660084723504753\n",
      "Epoch: 6600 | Loss: 0.004623912000065354\n",
      "Epoch: 6700 | Loss: 0.004587595790866197\n",
      "Epoch: 6800 | Loss: 0.004550736063864212\n",
      "Epoch: 6900 | Loss: 0.004513027702473402\n",
      "Epoch: 7000 | Loss: 0.004474329749027795\n",
      "Epoch: 7100 | Loss: 0.004434407591241748\n",
      "Epoch: 7200 | Loss: 0.004392659247574239\n",
      "Epoch: 7300 | Loss: 0.004348645161978228\n",
      "Epoch: 7400 | Loss: 0.0043019541438649635\n",
      "Epoch: 7500 | Loss: 0.0042517205253515605\n",
      "Epoch: 7600 | Loss: 0.0041972238652894095\n",
      "Epoch: 7700 | Loss: 0.0041367495484419\n",
      "Epoch: 7800 | Loss: 0.004071604180538354\n",
      "Epoch: 7900 | Loss: 0.004003576110683274\n",
      "Epoch: 8000 | Loss: 0.003935974306857799\n",
      "Epoch: 8100 | Loss: 0.0038708341945202446\n",
      "Epoch: 8200 | Loss: 0.0038104263135338226\n",
      "Epoch: 8300 | Loss: 0.003754039638643755\n",
      "Epoch: 8400 | Loss: 0.00370001905758354\n",
      "Epoch: 8500 | Loss: 0.0036484065489494222\n",
      "Epoch: 8600 | Loss: 0.0036000796595722106\n",
      "Epoch: 8700 | Loss: 0.003555948002902441\n",
      "Epoch: 8800 | Loss: 0.003517557919483263\n",
      "Epoch: 8900 | Loss: 0.003481240320071864\n",
      "Epoch: 9000 | Loss: 0.0034486879933884468\n",
      "Epoch: 9100 | Loss: 0.0034332385771001883\n",
      "Epoch: 9200 | Loss: 0.003417950183866775\n",
      "Epoch: 9300 | Loss: 0.003402690820887307\n",
      "Epoch: 9400 | Loss: 0.003387387563028952\n",
      "Epoch: 9500 | Loss: 0.003371994580569244\n",
      "Epoch: 9600 | Loss: 0.0033564747450238823\n",
      "Epoch: 9700 | Loss: 0.0033407907476100483\n",
      "Epoch: 9800 | Loss: 0.003324904726915354\n",
      "Epoch: 9900 | Loss: 0.0033087844929906643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10000 | Loss: 0.0032924067479527712\n",
      "Epoch: 10100 | Loss: 0.0032757511681310424\n",
      "Epoch: 10200 | Loss: 0.0032588025432360077\n",
      "Epoch: 10300 | Loss: 0.0032415531864791753\n",
      "Epoch: 10400 | Loss: 0.0032240862388213864\n",
      "Epoch: 10500 | Loss: 0.00320625002225389\n",
      "Epoch: 10600 | Loss: 0.0031882965132769368\n",
      "Epoch: 10700 | Loss: 0.0031703568016397533\n",
      "Epoch: 10800 | Loss: 0.00315264728215314\n",
      "Epoch: 10900 | Loss: 0.003135050874511741\n",
      "Epoch: 11000 | Loss: 0.00311747996536409\n",
      "Epoch: 11100 | Loss: 0.0030999172703690646\n",
      "Epoch: 11200 | Loss: 0.0030823396635247108\n",
      "Epoch: 11300 | Loss: 0.0030646256191909794\n",
      "Epoch: 11400 | Loss: 0.003046972963395969\n",
      "Epoch: 11500 | Loss: 0.0030292333901184836\n",
      "Epoch: 11600 | Loss: 0.0030115881813107077\n",
      "Epoch: 11700 | Loss: 0.0029938055338878263\n",
      "Epoch: 11800 | Loss: 0.002975937583012978\n",
      "Epoch: 11900 | Loss: 0.0029578451051437363\n",
      "Epoch: 12000 | Loss: 0.002939550210033201\n",
      "Epoch: 12100 | Loss: 0.002930183229544972\n",
      "Epoch: 12200 | Loss: 0.0029206358014314876\n",
      "Epoch: 12300 | Loss: 0.0029109000468882165\n",
      "Epoch: 12400 | Loss: 0.002900973582851814\n",
      "Epoch: 12500 | Loss: 0.0028908603232135365\n",
      "Epoch: 12600 | Loss: 0.0028805717424207162\n",
      "Epoch: 12700 | Loss: 0.00287012834355536\n",
      "Epoch: 12800 | Loss: 0.002859561121312827\n",
      "Epoch: 12900 | Loss: 0.002848912078082794\n",
      "Epoch: 13000 | Loss: 0.0028382317712979162\n",
      "Epoch: 13100 | Loss: 0.00282757286812133\n",
      "Epoch: 13200 | Loss: 0.0028169843205937254\n",
      "Epoch: 13300 | Loss: 0.002806506157971878\n",
      "Epoch: 13400 | Loss: 0.002796172612609486\n",
      "Epoch: 13500 | Loss: 0.0027859932738210497\n",
      "Epoch: 13600 | Loss: 0.0027760016829275445\n",
      "Epoch: 13700 | Loss: 0.0027662159611814754\n",
      "Epoch: 13800 | Loss: 0.00275662531011309\n",
      "Epoch: 13900 | Loss: 0.002747233732772565\n",
      "Epoch: 14000 | Loss: 0.002737975819860132\n",
      "Epoch: 14100 | Loss: 0.002728932300846987\n",
      "Epoch: 14200 | Loss: 0.0027200065595411643\n",
      "Epoch: 14300 | Loss: 0.0027112194952145312\n",
      "Epoch: 14400 | Loss: 0.002702535218922609\n",
      "Epoch: 14500 | Loss: 0.002693955884236398\n",
      "Epoch: 14600 | Loss: 0.002685412510917983\n",
      "Epoch: 14700 | Loss: 0.002676883977849891\n",
      "Epoch: 14800 | Loss: 0.002668338065852239\n",
      "Epoch: 14900 | Loss: 0.002659802842496688\n",
      "time=356.17457008361816\n",
      "error=0.0026512953549376606\n",
      "best loss=0.0026512953549376606\n",
      "best epoch=14999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy import interpolate\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import LBFGS\n",
    "\n",
    "interp_mode = \"bilinear\"\n",
    "align_corners = True\n",
    "\n",
    "\n",
    "#n_grids = [12,16,20,24,28,32,36,40]\n",
    "#n_grids = [12,16]\n",
    "#n_grids = [6,10,14,20,30,40,50]\n",
    "#n_grids = [6,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,200,300,400,500,600,700,800,900,1000]\n",
    "#n_grids = [10,20,30,40,50,60,70,80,90,100]\n",
    "n_grids = [6,8,12,16,20,24,28,32,36,40,44,48]\n",
    "#n_grids = [20]\n",
    "#n_grids = [200]\n",
    "theta_start = 0\n",
    "theta_end = np.pi/2\n",
    "r_start = 3\n",
    "r_end = 4\n",
    "\n",
    "a = 0.0\n",
    "M = 1\n",
    "\n",
    "errors = []\n",
    "times = []\n",
    "ii = 0\n",
    "\n",
    "\n",
    "for n_grid in n_grids:\n",
    "    print(n_grid)\n",
    "\n",
    "    thetas = torch.linspace(theta_start,theta_end,steps=n_grid, dtype=torch.double)\n",
    "    rs = torch.linspace(r_start,r_end,steps=n_grid, dtype=torch.double)\n",
    "    theta_h = (theta_end - theta_start)/(n_grid-1)\n",
    "    r_h  = (r_end - r_start)/(n_grid-1)\n",
    "\n",
    "    RS, THETAS = torch.meshgrid(rs, thetas)\n",
    "    # Transpose here is very important! Becareful of meshgrid and reshape stuff!\n",
    "    #RS = torch.transpose(RS,0,1)\n",
    "    #THETAS = torch.transpose(THETAS,0,1)\n",
    "    z = torch.transpose(torch.stack([RS.reshape(-1,), THETAS.reshape(-1,)]),0,1)\n",
    "    \n",
    "    #print(z)\n",
    "\n",
    "    # t' = t + f1(r,theta)\n",
    "    def f1(f1_free, n_grid):\n",
    "        # f1_free has shape (n_grid, n_grid-1).\n",
    "        # Along theta, zero derivative at theta=pi/2\n",
    "        f1_ = torch.zeros(n_grid,n_grid, dtype=torch.double)\n",
    "        f1_[:,:-1] = f1_free\n",
    "        f1_[:,-1] = f1_free[:,-1]\n",
    "        return f1_\n",
    "\n",
    "    # r' = r + f2(r,theta)\n",
    "    def f2(f2_free, n_grid):\n",
    "        # f2_free has shape (n_grid, n_grid-1).\n",
    "        # Along theta, zero derivative at theta=pi/2\n",
    "        f2_ = torch.zeros(n_grid,n_grid, dtype=torch.double)\n",
    "        f2_[:,:-1] = f2_free\n",
    "        f2_[:,-1] = f2_free[:,-1]\n",
    "        return f2_\n",
    "\n",
    "    # theta' = theta + f3(r,theta)\n",
    "    def f3(f3_free, n_grid):\n",
    "        # f3_free has shape (n_grid, n_grid-2)\n",
    "        # Along theta, zero at theta=0 and theta=pi/2\n",
    "        f3_ = torch.zeros(n_grid,n_grid, dtype=torch.double)\n",
    "        f3_[:,1:-1] = f3_free\n",
    "        return f3_\n",
    "\n",
    "    # phi' = phi + f4(r,theta)\n",
    "    def f4(f4_free, n_grid):\n",
    "        # f4_free has shape (n_grid, n_grid-1).\n",
    "        # Along theta, zero derivative at theta=pi/2\n",
    "        f4_ = torch.zeros(n_grid,n_grid, dtype=torch.double)\n",
    "        f4_[:,:-1] = f4_free\n",
    "        f4_[:,-1] = f4_free[:,-1]\n",
    "        return f4_\n",
    "    \n",
    "    \n",
    "    def interp_free(f_free, n_grid, mode=\"0\"):\n",
    "        n_grid_old = f_free.shape[0]\n",
    "        if mode == \"0\":\n",
    "            f_ = f1(f_free, n_grid_old)\n",
    "        else:\n",
    "            f_ = f3(f_free, n_grid_old)\n",
    "        f_free_std = f_.unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "        if mode == \"0\":\n",
    "            f_free_new = F.interpolate(f_free_std, size=(n_grid,n_grid), mode=interp_mode, align_corners=align_corners)[0,0,:,:-1]\n",
    "        else:\n",
    "            f_free_new = F.interpolate(f_free_std, size=(n_grid,n_grid), mode=interp_mode, align_corners=align_corners)[0,0,:,1:-1]\n",
    "        return f_free_new#torch.transpose(f_free_new,0,1)#\n",
    "\n",
    "    def interp_free_test(f_free, n_grid, mode=\"0\"):\n",
    "        if mode == \"0\":\n",
    "            f_ = f1(f_free, n_grid)\n",
    "        else:\n",
    "            f_ = f3(f_free, n_grid)\n",
    "        f_free_std = f_.unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "        if mode == \"0\":\n",
    "            f_free_new = F.interpolate(f_free_std, size=(2*n_grid-1,2*n_grid-1), mode=interp_mode, align_corners=align_corners)[0,0,1:-1:2,1:-3:2]\n",
    "        else:\n",
    "            f_free_new = F.interpolate(f_free_std, size=(2*n_grid-1,2*n_grid-1), mode=interp_mode, align_corners=align_corners)[0,0,1:-1:2,3:-3:2]\n",
    "        return f_free_new#torch.transpose(f_free_new,0,1)#\n",
    "\n",
    "    def decompose_free(f_free, n_grid):\n",
    "        f1_free = f_free[:,:n_grid-1]\n",
    "        f2_free = f_free[:,n_grid-1:2*n_grid-2]\n",
    "        f3_free = f_free[:,2*n_grid-2:3*n_grid-4]\n",
    "        f4_free = f_free[:,3*n_grid-4:4*n_grid-5]\n",
    "        return (f1_free,f2_free,f3_free,f4_free)\n",
    "\n",
    "    def compose_free(f1_free,f2_free,f3_free,f4_free):\n",
    "        return torch.cat([f1_free, f2_free, f3_free, f4_free], dim=1)\n",
    "\n",
    "    def interp_f_free(f_free, n_grid, n_grid_old):\n",
    "        f1_free, f2_free, f3_free, f4_free = decompose_free(f_free, n_grid_old)\n",
    "        f1_free_new = interp_free(f1_free, n_grid, mode=\"0\")\n",
    "        f2_free_new = interp_free(f2_free, n_grid, mode=\"0\")\n",
    "        f3_free_new = interp_free(f3_free, n_grid, mode=\"1\")\n",
    "        f4_free_new = interp_free(f4_free, n_grid, mode=\"0\")\n",
    "        f_free_new = compose_free(f1_free_new, f2_free_new, f3_free_new, f4_free_new)\n",
    "        return f_free_new\n",
    "\n",
    "\n",
    "    def interp_f_free_test(f_free, n_grid):\n",
    "        f_free = f_free.reshape(n_grid, 4*n_grid-5)\n",
    "        f1_free, f2_free, f3_free, f4_free = decompose_free(f_free, n_grid)\n",
    "        f1_free_new = interp_free_test(f1_free, n_grid, mode=\"0\")\n",
    "        f2_free_new = interp_free_test(f2_free, n_grid, mode=\"0\")\n",
    "        f3_free_new = interp_free_test(f3_free, n_grid, mode=\"1\")\n",
    "        f4_free_new = interp_free_test(f4_free, n_grid, mode=\"0\")\n",
    "        #print(f1_free_new.shape, f2_free_new.shape, f3_free_new.shape, f4_free_new.shape)\n",
    "        f_free_new = compose_free(f1_free_new, f2_free_new, f3_free_new, f4_free_new)\n",
    "        return f_free_new\n",
    "\n",
    "\n",
    "    def r_derivative(f, n_grid):\n",
    "        f_aug = torch.zeros(n_grid+2,n_grid,dtype=torch.double)\n",
    "        f_aug[1:-1] = f\n",
    "        f_aug[0] = 2*f[0] - f[1]\n",
    "        f_aug[-1] = 2*f[-1] - f[-2]\n",
    "        f_r = (f_aug[2:] - f_aug[:-2])/(2*r_h)\n",
    "        return f_r\n",
    "\n",
    "    def theta_derivative(f, n_grid):\n",
    "        f_aug = torch.zeros(n_grid,n_grid+2,dtype=torch.double)\n",
    "        f_aug[:,1:-1] = f\n",
    "        f_aug[:,0] = 2*f[:,0] - f[:,1]\n",
    "        f_aug[:,-1] = 2*f[:,-1] - f[:,-2]\n",
    "        f_theta = (f_aug[:,2:] - f_aug[:,:-2])/(2*theta_h)\n",
    "        return f_theta\n",
    "\n",
    "    def w(f1,f2,f3,f4, n_grid):\n",
    "        f1_r = r_derivative(f1, n_grid).reshape(-1,)\n",
    "        f2_r = r_derivative(f2, n_grid).reshape(-1,)\n",
    "        f3_r = r_derivative(f3, n_grid).reshape(-1,)\n",
    "        f4_r = r_derivative(f4, n_grid).reshape(-1,)\n",
    "        f1_theta = theta_derivative(f1, n_grid).reshape(-1,)\n",
    "        f2_theta = theta_derivative(f2, n_grid).reshape(-1,)\n",
    "        f3_theta = theta_derivative(f3, n_grid).reshape(-1,)\n",
    "        f4_theta = theta_derivative(f4, n_grid).reshape(-1,)\n",
    "        ones = torch.ones(f1_r.shape[0], dtype=torch.double)\n",
    "\n",
    "        stack1 = torch.stack([ones, f1_r, f1_theta, 0*ones])\n",
    "        stack2 = torch.stack([0*ones, 1+f2_r, f2_theta, 0*ones])\n",
    "        stack3 = torch.stack([0*ones, f3_r, 1+f3_theta, 0*ones])\n",
    "        stack4 = torch.stack([0*ones, f4_r, f4_theta, ones])\n",
    "        w_ = torch.stack([stack1, stack2, stack3, stack4])\n",
    "        w_ = w_.permute(2,0,1)\n",
    "        return w_\n",
    "\n",
    "    def w_inv_invt(w):\n",
    "        w_inv = torch.linalg.inv(w)\n",
    "        w_invt = w_inv.permute(0,2,1)\n",
    "        return w_inv, w_invt\n",
    "\n",
    "    def gp(g, w):\n",
    "        w_inv, w_invt = w_inv_invt(w)\n",
    "        gp_ = torch.matmul(torch.matmul(w_invt, g), w_inv)\n",
    "        return gp_\n",
    "\n",
    "    def zp(z, f2, f3):\n",
    "        f2 = f2.reshape(-1,)\n",
    "        f3 = f3.reshape(-1,)\n",
    "        rp = z[:,0] + f2\n",
    "        thetap = z[:,1] + f3\n",
    "        zp_ = torch.transpose(torch.stack([rp, thetap]),0,1)\n",
    "        return zp_\n",
    "\n",
    "    def g(x_, a=0.0):\n",
    "        r = x_[:,0]\n",
    "        theta = x_[:,1]\n",
    "        bs = x_.shape[0]\n",
    "        Sigma = r**2 + a**2*np.cos(theta)**2\n",
    "        Delta = r**2 - 2*M*r + a**2\n",
    "        one = torch.ones(bs, dtype=torch.double)\n",
    "        g01 = g02 = g10 = g12 = g13 = g20 = g21 = g23 = g31 = g32 = 0*one\n",
    "        g00 = -(1-2*M*r/Sigma)\n",
    "        g03 = g30 = -2*M*a*r*torch.sin(theta)**2/Sigma\n",
    "        g11 = Sigma/Delta\n",
    "        g22 = Sigma\n",
    "        g33 = (r**2+a**2+2*M*a**2*r*torch.sin(theta)**2/Sigma)*torch.sin(theta)**2\n",
    "        #print(g00.shape, g01.shape, g02.shape, g03.shape)\n",
    "        stack1 = torch.stack([g00, g01, g02, g03])\n",
    "        stack2 = torch.stack([g10, g11, g12, g13])\n",
    "        stack3 = torch.stack([g20, g21, g22, g23])\n",
    "        stack4 = torch.stack([g30, g31, g32, g33])\n",
    "        gs = torch.stack([stack1, stack2, stack3, stack4]).permute(2,0,1)\n",
    "        return gs\n",
    "\n",
    "    def gp_space_target(zp):\n",
    "        bs = zp.shape[0]\n",
    "        one = torch.ones(bs, dtype=torch.double)\n",
    "        g11 = one\n",
    "        g12 = g13 = g21 = g23 = g31 = g32 = 0*one\n",
    "        g22 = zp[:,0]**2\n",
    "        g33 = zp[:,0]**2*torch.sin(zp[:,1])**2\n",
    "        stack1 = torch.stack([g11,g12,g13])\n",
    "        stack2 = torch.stack([g21,g22,g23])\n",
    "        stack3 = torch.stack([g31,g32,g33])\n",
    "        gs = torch.stack([stack1, stack2, stack3]).permute(2,0,1)\n",
    "        return gs\n",
    "\n",
    "    def error(f_free, mode=\"train\"):\n",
    "        f_free = f_free.reshape(n_grid, 4*n_grid-5)\n",
    "        f1_free, f2_free, f3_free, f4_free = decompose_free(f_free, n_grid)\n",
    "        f1_ = f1(f1_free, n_grid)\n",
    "        f2_ = f2(f2_free, n_grid)\n",
    "        f3_ = f3(f3_free, n_grid)\n",
    "        f4_ = f4(f4_free, n_grid)\n",
    "        g_ = g(z, a=a)\n",
    "        w_ = w(f1_,f2_,f3_,f4_, n_grid)\n",
    "        zp_ = zp(z,f2_,f3_)\n",
    "        gp_space = gp(g_, w_)[:,1:,1:].reshape(n_grid,n_grid,3,3)\n",
    "        #print(gp_space): inconsistent here\n",
    "        \n",
    "        gp_space_target_ = gp_space_target(zp_).reshape(n_grid,n_grid,3,3)\n",
    "        if mode == \"train\":\n",
    "            error_ = torch.mean((gp_space-gp_space_target_)[1:-1,:]**2)\n",
    "        else:\n",
    "            error_ = torch.mean((gp_space-gp_space_target_)[1:-1,1:-1]**2)\n",
    "        return error_\n",
    "    \n",
    "    # Initialize next grid with the former solution\n",
    "    #if ii == 0:\n",
    "    if True:\n",
    "        f_free = torch.zeros((n_grid,4*n_grid-5), dtype=torch.double)\n",
    "        #f_free[:,:n_grid-1] = 1*M*(2*np.sqrt(rs/(2*M)) + 0.5*np.log((np.sqrt(rs/(2*M))-1)/(np.sqrt(rs/(2*M))+1)))[:,np.newaxis]\n",
    "        #f_free[:,:n_grid-1] = 2*M*(2*np.sqrt(rs/(2*M)) + 1*np.log((np.sqrt(rs/(2*M))-1)/(np.sqrt(rs/(2*M))+1)))[:,np.newaxis]\n",
    "        #f_free[:,:n_grid-1] = - 2*M*(2*np.sqrt(rs/(2*M)) - 1*np.log((np.sqrt(rs/(2*M))-1)/(np.sqrt(rs/(2*M))+1)))[:,np.newaxis]\n",
    "        f_free[:,:n_grid-1] = 0.001*(rs[:,np.newaxis] + thetas[np.newaxis,:-1])\n",
    "        f_free[:,n_grid-1:2*(n_grid-1)] = 0.001*(rs[:,np.newaxis] + thetas[np.newaxis,:-1])\n",
    "        f_free[:,2*(n_grid-1):3*n_grid-4] = 0.001*(rs[:,np.newaxis] + thetas[np.newaxis,1:-1])\n",
    "        f_free[:,3*n_grid-4:4*n_grid-5] = 0.001*(rs[:,np.newaxis] + thetas[np.newaxis,:-1])\n",
    "        f_free = f_free.reshape(-1,)\n",
    "        f_free = torch.nn.Parameter(f_free, requires_grad=True)\n",
    "        #print(f_free.shape)\n",
    "    else:\n",
    "        #best_free = torch.load('./results_grid/a_%.3f_grid_%d'%(1.0,50))\n",
    "        #f_free_old = best_free.reshape(50, 4*50-5)\n",
    "        #f_free = interp_f_free(f_free_old, n_grid, 50).reshape(-1,)\n",
    "        f_free_old = best_free.reshape(n_grid_old, 4*n_grid_old-5)\n",
    "        #print(f_free_old.shape)\n",
    "        #print(n_grid, n_grid_old)\n",
    "        f_free = interp_f_free(f_free_old, n_grid, n_grid_old).reshape(-1,)\n",
    "        f_free = torch.nn.Parameter(f_free, requires_grad=True)\n",
    "        #print(interp_f_free(f_free_old, n_grid, n_grid_old).shape)\n",
    "    \n",
    "    # \"Testing\"\n",
    "    #rs_test, thetas_test = grid(n_grid)\n",
    "    rs_test = (rs[1:] + rs[:-1])/2\n",
    "    thetas_test = (thetas[1:] + thetas[:-1])/2\n",
    "    f_free_test = interp_f_free_test(f_free, n_grid).reshape(-1,)\n",
    "    #plt.matshow(f_free_test.reshape(n_grid-1,4*(n_grid-1)-5))\n",
    "    RS_test, THETAS_test = torch.meshgrid(rs_test, thetas_test)\n",
    "    # Transpose here is very important! Becareful of meshgrid and reshape stuff!\n",
    "    #RS_test = torch.transpose(RS_test,0,1)\n",
    "    #THETAS_test = torch.transpose(THETAS_test,0,1)\n",
    "    z_test = torch.transpose(torch.stack([RS_test.reshape(-1,), THETAS_test.reshape(-1,)]),0,1)\n",
    "\n",
    "    \n",
    "    def error_test(f_free, mode=\"train\"):\n",
    "        #print(f_free.shape)\n",
    "        f_free = f_free.reshape(n_grid-1, 4*(n_grid-1)-5)\n",
    "        f1_free, f2_free, f3_free, f4_free = decompose_free(f_free, n_grid-1)\n",
    "        f1_ = f1(f1_free, n_grid-1)\n",
    "        f2_ = f2(f2_free, n_grid-1)\n",
    "        f3_ = f3(f3_free, n_grid-1)\n",
    "        f4_ = f4(f4_free, n_grid-1)\n",
    "        g_ = g(z_test, a=a)\n",
    "        w_ = w(f1_,f2_,f3_,f4_, n_grid-1)\n",
    "        zp_test = zp(z_test,f2_,f3_)\n",
    "        gp_space = gp(g_, w_)[:,1:,1:].reshape(n_grid-1,n_grid-1,3,3)\n",
    "        \n",
    "        gp_space_target_ = gp_space_target(zp_test).reshape(n_grid-1,n_grid-1,3,3)\n",
    "        if mode == \"train\":\n",
    "            error_ = torch.mean((gp_space-gp_space_target_)[1:-1,:]**2)\n",
    "        else:\n",
    "            error_ = torch.mean((gp_space-gp_space_target_)[1:-1,1:-1]**2)\n",
    "        return error_\n",
    "    \n",
    "    def error_all(f_free, mode=\"train\"):\n",
    "        f_free_test = interp_f_free_test(f_free, n_grid).reshape(-1,)\n",
    "        #print(interp_f_free_test(f_free, n_grid).shape)\n",
    "        return error(f_free, mode=mode) + error_test(f_free_test, mode=mode)\n",
    "    \n",
    "    \n",
    "    # Test and Train at the same time\n",
    "    start = time.time()\n",
    "    #lr = 36/n_grid**2\n",
    "    #opt = LBFGS({f_free}, lr=lr, max_iter=1000, max_eval=None, tolerance_grad=1e-30, tolerance_change=1e-30, history_size=1000, line_search_fn='strong_wolfe')\n",
    "    #opt = LBFGS({f_free}, lr=lr, max_iter=100, max_eval=None, tolerance_grad=1e-30, tolerance_change=1e-30, history_size=1000)\n",
    "    lr = 1e-2*(6/n_grid)**2\n",
    "    opt = torch.optim.Adam({f_free}, lr=lr, eps=1e-8)\n",
    "    \n",
    "    epochs = 15000\n",
    "    switch_epoch = 3000\n",
    "    log = 100\n",
    "    best_loss = 1e20\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        if (i+1) % switch_epoch == 0:\n",
    "            for opt_param in opt.param_groups:\n",
    "                lr = lr * 0.5\n",
    "                opt_param['lr'] = lr\n",
    "        \n",
    "        def loss_closure():\n",
    "            opt.zero_grad()\n",
    "            loss = error_all(f_free, mode=\"train\")\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        \n",
    "        def loss_closure2():\n",
    "            opt.zero_grad()\n",
    "            loss = error_all(f_free, mode=\"evaluate\")\n",
    "            loss.backward()\n",
    "            return loss\n",
    "          # -------------------------------------------\n",
    "        loss = loss_closure()\n",
    "        #print(\"loss_0={}\".format(loss.detach().numpy()))\n",
    "        #loss2 = loss_closure2()\n",
    "        opt.step(loss_closure)  # get loss, use to update wts\n",
    "        #loss = loss_closure()\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            #best_loss2 = loss2\n",
    "            best_epoch = i\n",
    "            best_free = f_free.clone()\n",
    "        if i % log == 0:\n",
    "            print(\"Epoch: {}\".format(i) + \" | \" + \"Loss: {}\".format(loss.detach().numpy()))\n",
    "        losses.append(loss.detach().numpy())\n",
    "    end = time.time()\n",
    "    print(\"time={}\".format(end-start))\n",
    "    times.append(end-start)\n",
    "    errors.append(best_loss.detach().numpy())\n",
    "    print(\"error={}\".format(loss.detach().numpy()))\n",
    "    print(\"best loss={}\".format(best_loss.detach().numpy()))\n",
    "    #print(\"best loss2={}\".format(best_loss2.detach().numpy()))\n",
    "    print(\"best epoch={}\".format(best_epoch))\n",
    "    #errors.append(error_all(f_free))\n",
    "    ii = ii + 1\n",
    "    n_grid_old = n_grid\n",
    "    torch.save(best_free, './results_grid/params_grid_adam_lrdecay_randominit_noseq_a_%.3f_n_%d'%(a,n_grid))\n",
    "np.save('./results_grid/loss_grid_adam_lrdecay_randominit_noseq_a_%.3f'%a, np.array([n_grids, errors, times]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.00000000e+00, 8.00000000e+00, 1.20000000e+01, 1.60000000e+01,\n",
       "        2.00000000e+01, 2.40000000e+01, 2.80000000e+01, 3.20000000e+01,\n",
       "        3.60000000e+01, 4.00000000e+01, 4.40000000e+01, 4.80000000e+01],\n",
       "       [1.71634783e-06, 8.02505233e-07, 4.25355805e-03, 4.91673840e-03,\n",
       "        2.94381546e-03, 3.81763196e-03, 2.60692081e-03, 2.45100949e-03,\n",
       "        2.44519439e-03, 2.81222996e-03, 3.52912019e-03, 2.65129535e-03],\n",
       "       [1.82487767e+02, 1.84577379e+02, 1.88728993e+02, 1.98858538e+02,\n",
       "        2.26034180e+02, 2.43919820e+02, 2.57415772e+02, 2.77201006e+02,\n",
       "        2.89886308e+02, 3.14068496e+02, 3.22561186e+02, 3.56174570e+02]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('./results_grid/loss_grid_adam_lrdecay_randominit_noseq_a_%.3f.npy'%a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'error')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_grids = np.array(n_grids)\n",
    "#n_params = (4*n_grids-5)*n_grids\n",
    "n_params = n_grids**2\n",
    "#n_params = (n_grids-1)**2\n",
    "plt.plot(n_params, errors, marker=\"o\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "#plt.ylim(1e-8,10e-5)\n",
    "plt.xlabel(r\"$N$\",fontsize=20)\n",
    "plt.ylabel(\"error\",fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9201069227039909"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_id = -6\n",
    "end_id = -1\n",
    "\n",
    "(np.log(errors[start_id]) - np.log(errors[end_id]))/(np.log(n_params[start_id])-np.log(n_params[end_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcc3e5b6dc0>]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEFCAYAAADqujDUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjnUlEQVR4nO3deZRc5Xnn8e9TVV29S91St6RGrR2hBYEEarPaGIYAEhlH4AOJsA0MB4doBnnIxEnQcXISTzhnQryM7QwYgjE2mWA4xAZbdmTLDDaWbYxAgFYkWa29tbbW7larl+p65o+6EkXRUldLXUt3/T7n1Ln3vu97732qdVVPve9dytwdEREpPKFcByAiIrmhBCAiUqCUAERECpQSgIhIgVICEBEpUJFcB9AfNTU1PnHixFyHISIyqLz99tuH3L02tXxQJYCJEyeyatWqXIchIjKomNnO3so1BCQiUqCUAERECpQSgIhIgVICEBEpUEoAIiIFqs8EYGbPmNlBM1t/hnozs382s0YzW2tmlyfVzTOzzUHdkqTyEWb2ipltCabVA/N2REQkXen0AL4LzDtL/XxgavB6AHgCwMzCwONB/UzgLjObGayzBHjV3acCrwbLIiKSRX3eB+DuK8xs4lmaLAD+1RPPlX7DzKrMrA6YCDS6+zYAM3shaPteML0+WP9Z4DXg4XN7C317deMB1uw+BmYYYAaGYQYhAzODlPLUdsF7SCqHUCix3Nt2k5c51d6McMgIhYywGeHQh8sip+ZDdroubEYoRNL8qfXff/XWNhIKURS20+9PRCTZQNwINhbYnbTcFJT1Vn5lMD/a3fcBuPs+Mxt1po2b2QMkehaMHz/+nAJ8bXMz//eNXu+DKAiRkFEUDhEJG9FgGgmFiEZCp+uKwu+3SSwnyiLhUGKdkFEUCVEUCsoiIYojIYojYUqKEtPiSIjiojTLIiEiYZ2CEsmlgUgAvX299LOU94u7PwU8BdDQ0HBOv17zyG2zeOS2Wae2h3siEHcn7uAkyhL17y+fauNBOb3UnVqf02W9rB9sO+5OT9yDKUnzTo878bgTiyemPWdpe2qa3DYeP7UdTpfFeuJ09zjdPXFicacrFicWj9Mdc7rjibpEm2A+qGuLxRLr9DhdwbQ7eVs9cbqC5fMRCVmQIBJJoSwapiwaoTQapjxlvjQaCaaJ8vLiMKVFYcqLI0FZmPJohLJomMqSIqIRJReRvgxEAmgCxiUt1wN7gegZygEOmFld8O2/Djg4AHGkxez9IZ3ec5SkqydIKp2xHjpjcTq6E9PO7l7KYj10dicvJ5XFeujojnOyq4f2rhgnuno41NZFe1c77V09wSvWr4RTHAlRWVLEsJIIlSURKkuKgmnyfGI6LGm+qjRKVXkRlcURDZ3JkDcQCWApsDgY478SOB58sDcDU81sErAHWAh8Kmmde4FHg+mPBiAOybJwyCgNvpVnQ3dP/HQyaO/qob0zmO9OzJ/oitHeGaO1I0ZrZ4zWjm5aOoLljm72t3TQ2tFNa0di/bOJhIyqsijVZUVUl0WpOjUtT0yry4qC+igjyouoqShmeGmRkoYMKn0mADN7nsQJ2xozawL+HigCcPcngWXArUAj0A7cF9TFzGwxsBwIA8+4+4Zgs48CL5rZ/cAu4M4BfE8yRBWFQwwvDTG8tOi8txXridMWJIuWjm7aOmK0dMQ41t7FsfZujrZ3cbS9m6Mnujja3sXOw+2s3n2MY+3ddPXEzxCfUVNRTE1FMbWVxdRWFFNTGQ2mieXaysS8ehiSD2ww/Sh8Q0OD62mgkkvuTntXD0eTEsWRE10cauuiubWTQ22dH5gePtFFT/zD/8fKomHqhpdQN7w0mJZQV1X6fllViZKEDBgze9vdG1LLB9XjoEVyzcwoL45QXhyhPo3bF+Nx52h7F81tnRxq7aK5rYPm1k72H+9k3/GT7DvewYotzRxs7ST1u1h5NExdVSn11aWMH1HG+BFljBtRxoSRZYyrLqO8WP995fzoCBLJoFDIGFlRzMiKYhhz5nbdPXEOtnay//hJ9h7rOJ0c9h3roOlYO2/vPEprR+wD69RURBkXJIYJI8qYWFPOlNoKpoyqoELJQdKgo0QkDxSFQ4ytKmVsVSlzJ/Te5lh7F7uOtJ9+7T7Szs7DieTw4zV7SR5pGjOshCmjyrkwSAinpqMqizWsJKcpAYgMElVlUarKolxaX/Whuq5YnF1H2tna3EbjwTa2Nrex9WAbP3hnD22d7/cchpVEmFE3jJkXDEtM64YxdXQFxZHsXMkl+UUJQGQIiEZCXDiqggtHVXDLxe+XuzsHWjpPJ4ZN+1vZuK+FF97czcnuxKWwkZAxpbaCGXWVzLxgGLPrq7ikfjhlUX08DHX6FxYZwsyMMcNLGDO8hGsvrDld3hN3dh4+wcZ9rby37zgb97WycvsRfrg6ca9mOGRcNLqSOeOqmDNuOHPGVXPhqArCIQ0fDSW6DFRETjvU1snapmOs3nWMd3cfY83uY7QEJ5/Lo2EuG1/NlZNGcOXkkcweN1xDR4PEmS4DVQIQkTOKx53th0+wetcxVu8+xls7jrBpfyuQeNzGZeOruHLSSK6cPILLx1dTUqSEkI+UAERkQBw90cWbO46wctsRVm4/zHv7WnCHkqIQV08eyccvquXj00YxqaY816FKQAlARDLi+Mlu3tp+hN80HuK1zQfZcbgdgAkjy/j4RbVcP62Wa6bUqHeQQ0oAIpIVOw6dYMWWZn61uZnXtx7mZHcP5dEwN0wfxbxZY7h+2ijdqJZlehSEiGTFxJpyJtaUc8/VE+mM9fC7rYdZvuEAr7y3n5+s3Uc0EuK6qTXMm1XHvFljlAxySD0AEcmKnrjz9s6j/Gz9fpZv2M+eYycpKQpx88wx3H75WD52YY1+JS5DNAQkInnD3Xln11FeemcPP1m7j+Mnu6mpiPKJ2Rew8CPjmTamMtchDilKACKSlzpjPfxyUzM/fHcPv9h0kK6eOB+ZWM1nrprAvFljdK/BAFACEJG8d+REF99/ezfPrdzFzsPtjCiPcmdDPXdfNYH66rJchzdoKQGIyKARjzu/3XqI597YxSsbDwDwiUvreOC6Kcy8YFiOoxt8dBWQiAwaoZDxsam1fGxqLXuPneQ7v93O91bu4oer9/KxqTUs+vgUrpkyUo+2Pk/qAYjIoHD8ZDfPrdzJd367g+bWThomVPP5m6dx9ZSRuQ4t72kISESGhM5YDy+uauKxX2zhQEsn1144kr+4aRpzJ6TxG50FSglARIaUju4enlu5iydea+RQWxc3zRzN39w6g4l6BtGHnCkBpHXXhZnNM7PNZtZoZkt6qa82s5fNbK2ZvWlms4LyaWa2OunVYmZ/HtR90cz2JNXdep7vUUQKSElRmPs/OokVf30Df3XLNF5vPMRNX/sV/7hsI60d3bkOb1DoswdgZmHg98BNQBPwFnCXu7+X1ObLQJu7/08zmw487u439rKdPcCV7r7TzL4YrPOVdINVD0BEzuRgSwdfWr6Z77/dRE1FlIfnTeeOufU6Ucz59QCuABrdfZu7dwEvAAtS2swEXgVw903ARDMbndLmRmCru+/sd/QiIn0YNayEr9w5mx89eC3jR5TxV99fy2e+vZKdh0/kOrS8lU4CGAvsTlpuCsqSrQE+CWBmVwATgPqUNguB51PKFgfDRs+YWa9ncMzsATNbZWarmpub0whXRArZ7HFVfH/RNTxy2yzW7D7OLV9fwVMrthLriec6tLyTTgLorf+UOm70KFBtZquBzwHvArHTGzCLAn8E/HvSOk8AU4A5wD7gq73t3N2fcvcGd2+ora1NI1wRKXShkHH3VRN45S+u46MX1vK/lm3izn/5HbuC3yqQhHQSQBMwLmm5Htib3MDdW9z9PnefA9wD1ALbk5rMB95x9wNJ6xxw9x53jwPfIjHUJCIyYOqGl/Kte+byjYVzaDzYxq3//GteeqeJwXT1YyalkwDeAqaa2aTgm/xCYGlyAzOrCuoAPguscPeWpCZ3kTL8Y2Z1SYu3A+v7G7yISF/MjAVzxvLThz7GzLph/MWLa3johdW0dcb6XnmI6zMBuHsMWAwsBzYCL7r7BjNbZGaLgmYzgA1mtonEt/2HTq1vZmUkriB6KWXTXzKzdWa2FrgB+B/n/W5ERM6gvrqM5x+4is/fdBH/sW4ftz/+W7Y1t+U6rJzSjWAiUnBebzzE4uffpTsW5+sL53DjjNSLFoeW87oRTERkKLnmwhqWLr6WCTVl3P/sKh7/ZWNBnhdQAhCRglRfXcb3F13DgjkX8OXlm/mbH64vuEtF9ThoESlYJUVhvvbHc7igqpQnXtvKgeMd/J9PXUZZtDA+GtUDEJGCFgoZD8+bziMLLuaXmw/y6adX0lIgzxJSAhARAe6+eiLf/PTlrN9znM88vZLj7UM/CSgBiIgE5s2q44lPz2XTvlY+9fQbHD3RleuQMkoJQEQkyR/MHM2/3DOXLQfb+My3h/ZwkBKAiEiKG6aN4l/unsvm/a386bOr6OjuyXVIGaEEICLSixumjeKrfzyblduP8N+ff3dIXiKqBCAicgYL5ozli5+Yyc/fO8DfLd0w5G4WK4yLXUVEztF/uXYS+1s6efJXW5k2upJ7r5mY65AGjHoAIiJ9+OtbpvEHM0bxDz95j982Hsp1OANGCUBEpA+hkPH1hZcxpbac//bcO0PmZyaVAERE0lBRHOHpez4CwOLvvUtnbPBfGaQEICKSpvEjy/jyHZeybs9xHv3pplyHc96UAERE+uHmi8dw37UT+c5vd7B8w/5ch3NelABERPppyfzpXDJ2OA//YC0HWztyHc45UwIQEemn4kiYr/3JHNq7evjbl9cP2vsDlABERM7BhaMq+MubL+Ln7x1g6Zq9uQ7nnCgBiIico/s/OpnLxlfx90s30Nzametw+k0JQETkHIVDxpfvmE17Zw//uGxjrsPpt7QSgJnNM7PNZtZoZkt6qa82s5fNbK2ZvWlms5LqdpjZOjNbbWarkspHmNkrZrYlmFYPzFsSEcmeC0dV8KfXTeKld/fw1o4juQ6nX/pMAGYWBh4H5gMzgbvMbGZKsy8Aq939UuAe4Bsp9Te4+xx3b0gqWwK86u5TgVeDZRGRQefBGy7kguEl/N2PNgyqp4am0wO4Amh0923u3gW8ACxIaTOTxIc47r4JmGhmo/vY7gLg2WD+WeC2dIMWEcknZdEIf/ufZ7JxXwvfe3NXrsNJWzoJYCywO2m5KShLtgb4JICZXQFMAOqDOgd+bmZvm9kDSeuMdvd9AMF0VG87N7MHzGyVma1qbm5OI1wRkeybP2sMV08eydf/3xbaOmO5Dict6SQA66Us9aLXR4FqM1sNfA54Fzj1F7jW3S8nMYT0oJld158A3f0pd29w94ba2tr+rCoikjVmxpL50zlyoounf70t1+GkJZ0E0ASMS1quBz5w0au7t7j7fe4+h8Q5gFpge1C3N5geBF4mMaQEcMDM6gCC6cFzfxsiIrk3e1wV82eN4VsrtnG4Lf8vC00nAbwFTDWzSWYWBRYCS5MbmFlVUAfwWWCFu7eYWbmZVQZtyoGbgfVBu6XAvcH8vcCPzu+tiIjk3udvnsbJ7h4e+2VjrkPpU58JwN1jwGJgObAReNHdN5jZIjNbFDSbAWwws00khnoeCspHA78xszXAm8B/uPvPgrpHgZvMbAtwU7AsIjKoXTiqgjvm1vPcyl15/5wgG0zPsGhoaPBVq1b13VBEJIe2HzrBjV99jQeum8KS+dNzHQ5m9nbKZfiA7gQWERlwk2rKufWSOv7tjZ0cP9md63DOSAlARCQD/uv1U2jrjPFvb+zMdShnpAQgIpIBF18wnOun1fLMb7bn7c9HKgGIiGTI/R+dxOETXSxbty/XofRKCUBEJEOunVLD5Npynn09P4eBlABERDIkFDLuuWoCq3cfY23TsVyH8yFKACIiGfTJufWURcN52QtQAhARyaBhJUXcdtlY/mPdXlo78uuSUCUAEZEMu3NuPR3d8bw7GawEICKSYXPGVTG5tpzvv92U61A+QAlARCTDzIw75tbz1o6j7Dx8ItfhnKYEICKSBbdfNhYz+EEe9QKUAEREsqBueCnXTqlh6Zq95MtDOJUARESy5NZL6thxuJ1N+1tzHQqgBCAikjU3XzyakMFP8+RqICUAEZEsqako5spJI1m2fn+uQwGUAEREsurWS8bQeLCNLQdyPwykBCAikkW3XDwGgOUbct8LUAIQEcmiUcNKuGTscF7b3JzrUJQARESy7fpptbyz6yjH23P7bCAlABGRLLt+Wi1xhxVbctsLSCsBmNk8M9tsZo1mtqSX+moze9nM1prZm2Y2KygfZ2a/NLONZrbBzB5KWueLZrbHzFYHr1sH7m2JiOSvOeOqGV5alPNhoD4TgJmFgceB+cBM4C4zm5nS7AvAane/FLgH+EZQHgM+7+4zgKuAB1PW/Zq7zwley87zvYiIDArhkHHdRbX86vfNxOO5uys4nR7AFUCju29z9y7gBWBBSpuZwKsA7r4JmGhmo919n7u/E5S3AhuBsQMWvYjIIHXd1BoOtXWyOYeXg6aTAMYCu5OWm/jwh/ga4JMAZnYFMAGoT25gZhOBy4CVScWLg2GjZ8ysuredm9kDZrbKzFY1N+f+rLmIyEC4avJIAFZuO5yzGNJJANZLWWqf5VGg2sxWA58D3iUx/JPYgFkF8APgz929JSh+ApgCzAH2AV/tbefu/pS7N7h7Q21tbRrhiojkv3EjyhhbVcrK7UdyFkMkjTZNwLik5Xpgb3KD4EP9PgAzM2B78MLMikh8+D/n7i8lrXPg1LyZfQv4ybm9BRGRwemqySP55eaDxONOKNTbd+3MSqcH8BYw1cwmmVkUWAgsTW5gZlVBHcBngRXu3hIkg28DG939f6esU5e0eDuw/lzfhIjIYHTl5BEcOdHFloNtOdl/nz0Ad4+Z2WJgORAGnnH3DWa2KKh/EpgB/KuZ9QDvAfcHq18L3A2sC4aHAL4QXPHzJTObQ2I4aQfwZwP1pkREBoOrT50H2H6YaWMqs77/dIaACD6wl6WUPZk0/ztgai/r/YbezyHg7nf3K1IRkSGmvrqUuuElvLn9CPdcPTHr+9edwCIiOWJmXDa+itW7j+Vk/0oAIiI5NGdcFU1HT3KorTPr+1YCEBHJodn1VQCsyUEvQAlARCSHLqkfTsiUAERECk5ZNMJFoyt5VwlARKTwXDa+ijW7j2X9wXBKACIiOXbJ2CpaOmLsOXYyq/tVAhARybEZdYmbwDbua+mj5cBSAhARybGLRldiBhv3ZffR0EoAIiI5Vl4cYcKIMjbtVw9ARKTgzKgbxqb96gGIiBSc6WOGsePwCdq7Yn03HiBKACIieWB6XSXusDmLvQAlABGRPDA9eBz077P4G8FKACIieaC+uoxoOMS25hNZ26cSgIhIHgiHjAkjy9iqBCAiUngm15az7VD2fh5SCUBEJE9Mrq1g1+F2unviWdmfEoCISJ6YXFNOLO7sPtKelf0pAYiI5InJtRUAWTsRnFYCMLN5ZrbZzBrNbEkv9dVm9rKZrTWzN81sVl/rmtkIM3vFzLYE0+qBeUsiIoPTlNpygKydB+gzAZhZGHgcmA/MBO4ys5kpzb4ArHb3S4F7gG+kse4S4FV3nwq8GiyLiBSsqrIoVWVF7DycP0NAVwCN7r7N3buAF4AFKW1mkvgQx903ARPNbHQf6y4Ang3mnwVuO583IiIyFNRXl9J0NDu/C5BOAhgL7E5abgrKkq0BPglgZlcAE4D6PtYd7e77AILpqP4GLyIy1NRXldF0NH96ANZLWervlj0KVJvZauBzwLtALM11z75zswfMbJWZrWpubu7PqiIig86pHoB75n8eMp0E0ASMS1quB/YmN3D3Fne/z93nkDgHUAts72PdA2ZWBxBMD/a2c3d/yt0b3L2htrY2jXBFRAavcSPK6IzFOdTWlfF9pZMA3gKmmtkkM4sCC4GlyQ3MrCqoA/gssMLdW/pYdylwbzB/L/Cj83srIiKDX311KUBWhoH6TADuHgMWA8uBjcCL7r7BzBaZ2aKg2Qxgg5ltInHFz0NnWzdY51HgJjPbAtwULIuIFLT66jKArJwIjqTTyN2XActSyp5Mmv8dMDXddYPyw8CN/QlWRGSoG3u6B5D5BKA7gUVE8khFcYTqsqL8GAISEZHsGjO8lAMtnRnfjxKAiEieGVVZzMHWjozvRwlARCTPjB5WzIEWJQARkYIzelgJza2d9MQzezOYEoCISJ4ZNayEuMPhtsyeB1ACEBHJM6MriwEyfiJYCUBEJM+MHlYCkPETwUoAIiJ55lQCUA9ARKTA1FREMYP9Gb4SSAlARCTPRMIhRpRFdRJYRKQQVZdHOdqe2UdCKwGIiOShRA9ACUBEpOCMUA9ARKQwVZdHOXKiO6P7UAIQEclDI4MeQDyDj4NQAhARyUPV5VF64k5rRyxj+1ACEBHJQyPKiwA4fCJzl4IqAYiI5KER5YnnAWXyRLASgIhIHhpRFgXI6IlgJQARkTxUVZYYAjp6Isc9ADObZ2abzazRzJb0Uj/czH5sZmvMbIOZ3ReUTzOz1UmvFjP786Dui2a2J6nu1gF9ZyIig9iw0kQCaOnIXA8g0lcDMwsDjwM3AU3AW2a21N3fS2r2IPCeu3/CzGqBzWb2nLtvBuYkbWcP8HLSel9z968MzFsRERk6KooTH8+5vgroCqDR3be5exfwArAgpY0DlWZmQAVwBEiN+kZgq7vvPM+YRUSGvHDIqCiO5DwBjAV2Jy03BWXJHgNmAHuBdcBD7h5PabMQeD6lbLGZrTWzZ8ysuredm9kDZrbKzFY1NzenEa6IyNAwrCSS0SGgdBKA9VKWemvaLcBq4AISQz6Pmdmw0xswiwJ/BPx70jpPAFOC9vuAr/a2c3d/yt0b3L2htrY2jXBFRIaGypIiWnOcAJqAcUnL9SS+6Se7D3jJExqB7cD0pPr5wDvufuBUgbsfcPeeoKfwLRJDTSIiEqgsyf0Q0FvAVDObFHyTXwgsTWmzi8QYP2Y2GpgGbEuqv4uU4R8zq0tavB1Y37/QRUSGtmGlRbm9CsjdY2a2GFgOhIFn3H2DmS0K6p8EHgG+a2brSAwZPezuhwDMrIzEFUR/lrLpL5nZHBLDSTt6qRcRKWiVJRG2NmeuB9BnAgBw92XAspSyJ5Pm9wI3n2HddmBkL+V39ytSEZECkw9DQCIikgPDSopoOdmNe2YeCa0EICKSpypLiojFnY7u1KvqB4YSgIhInqooDgPQ1pmZYSAlABGRPFUaTZymPdnVk5HtKwGIiOSpsmiiB9DerR6AiEhBKQ0SgHoAIiIFpqxICUBEpCCVBecA2pUAREQKS+npcwBKACIiBeX9cwA6CSwiUlBOnQPQEJCISIE5PQSkBCAiUliKIyFCpquAREQKjplRFo2oByAiUohKo2FO6k5gEZHCUxYNawhIRKQQRcMhunr0OGgRkYITjYTo1O8BiIgUnmhEPQARkYIUDYfojCkBiIgUnOKiMF25TABmNs/MNptZo5kt6aV+uJn92MzWmNkGM7svqW6Hma0zs9VmtiqpfISZvWJmW4Jp9cC8JRGRoSMaDuUuAZhZGHgcmA/MBO4ys5kpzR4E3nP32cD1wFfNLJpUf4O7z3H3hqSyJcCr7j4VeDVYFhGRJMU5PgdwBdDo7tvcvQt4AViQ0saBSjMzoAI4AvR158IC4Nlg/lngtnSDFhEpFNFIiM5Y7u4DGAvsTlpuCsqSPQbMAPYC64CH3P1UynLg52b2tpk9kLTOaHffBxBMR/W2czN7wMxWmdmq5ubmNMIVERk6cjoEBFgvZZ6yfAuwGrgAmAM8ZmbDgrpr3f1yEkNID5rZdf0J0N2fcvcGd2+ora3tz6oiIoNeNJLbBNAEjEtarifxTT/ZfcBLntAIbAemA7j73mB6EHiZxJASwAEzqwMIpgfP9U2IiAxVuU4AbwFTzWxScGJ3IbA0pc0u4EYAMxsNTAO2mVm5mVUG5eXAzcD6YJ2lwL3B/L3Aj87njYiIDEWZvBEs0lcDd4+Z2WJgORAGnnH3DWa2KKh/EngE+K6ZrSMxZPSwux8ys8nAy4lzw0SA77n7z4JNPwq8aGb3k0ggdw7wexMRGfSi4RDdPU487oRCvY3In7s+EwCAuy8DlqWUPZk0v5fEt/vU9bYBs8+wzcMEvQYREeldcVFioKarJ05JKDyg29adwCIieSwaTnxMZ+JxEEoAIiJ5rChIALEMnAdQAhARyWORcGLcvyeeevX9+VMCEBHJY5HgxG+3EoCISGGJhBIf0z09SgAiIgXl1BBQd1znAERECsrpHoCGgERECks4OAcQ0xCQiEhhKQqGgGIaAhIRKSynewAaAhIRKSzv3wimBCAiUlDe7wFoCEhEpKCcPgegHoCISGEZUV7MrZeMYUR5dMC3ndbjoEVEJDcm1ZTzzU/Pzci21QMQESlQSgAiIgVKCUBEpEApAYiIFCglABGRAqUEICJSoJQAREQKlBKAiEiBMveBv704U8ysGdh5jqvXAIcGMJyBkI8xQX7GpZjSo5jSl49xZSqmCe5em1o4qBLA+TCzVe7ekOs4kuVjTJCfcSmm9Cim9OVjXNmOSUNAIiIFSglARKRAFVICeCrXAfQiH2OC/IxLMaVHMaUvH+PKakwFcw5AREQ+qJB6ACIikkQJQESkQA25BGBm88xss5k1mtmSXurNzP45qF9rZpfnQUyfDmJZa2avm9nsXMeU1O4jZtZjZnfkQ0xmdr2ZrTazDWb2q0zHlE5cZjbczH5sZmuCuO7LcDzPmNlBM1t/hvqsH+NpxpWL4/ysMSW1y+Zx3mdMWTvO3X3IvIAwsBWYDESBNcDMlDa3Aj8FDLgKWJkHMV0DVAfz8/MhpqR2vwCWAXfkOiagCngPGB8sj8qTY+oLwD8F87XAESCawZiuAy4H1p+hPqvHeD/iyupxnk5MSf/GWTnO0/w7Ze04H2o9gCuARnff5u5dwAvAgpQ2C4B/9YQ3gCozq8tlTO7+ursfDRbfAOozGE9aMQU+B/wAOJjheNKN6VPAS+6+C8Dd8yUuByrNzIAKEgkglqmA3H1FsI8zyfYxnlZcOTjO0/lbQXaP83RiytpxPtQSwFhgd9JyU1DW3zbZjinZ/SS+vWVSnzGZ2VjgduDJDMeSdkzARUC1mb1mZm+b2T15EtdjwAxgL7AOeMjd41mI7UyyfYyfi2wc533KwXGejqwd50PtR+Gtl7LU61zTaTOQ0t6fmd1A4j/GRzMYD6QX09eBh929J/HFNuPSiSkCzAVuBEqB35nZG+7++xzHdQuwGvhPwBTgFTP7tbu3ZDCus8n2Md4vWTzO0/F1snucpyNrx/lQSwBNwLik5XoS38r62ybbMWFmlwJPA/Pd/XAG40k3pgbgheA/RQ1wq5nF3P2HOYypCTjk7ieAE2a2ApgNZDIBpBPXfcCjnhiwbTSz7cB04M0MxnU22T7G05bl4zwd2T7O05G94zzTJzyy+SKR0LYBk3j/hN3FKW3+kA+eIHszD2IaDzQC1+TL3yml/XfJ/EngdP5OM4BXg7ZlwHpgVh7E9QTwxWB+NLAHqMlwXBM580nErB7j/Ygrq8d5OjGltMv4cZ7m3ylrx/mQ6gG4e8zMFgPLSZzZf8bdN5jZoqD+SRJn+m8lcSC2k/j2luuY/g4YCXwz+CYS8ww+ETDNmLIqnZjcfaOZ/QxYC8SBp939rJf3ZSMu4BHgu2a2jsSH7sPunrHHDJvZ88D1QI2ZNQF/DxQlxZPVY7wfcWX1OE8zpqzrK6ZsHud6FISISIEaalcBiYhImpQAREQKlBKAiEiBUgIQESlQSgAiIjmS7sPq+rG9fzKz9cHrT/pqrwQgIpI73wXmDcSGzOwPSTxkbg5wJfBXZjbsbOsoAYiI5Ij38mA4M5tiZj8LngP0azObnubmZgK/cveYJ+4iXkMfyUUJQEQkvzwFfM7d5wJ/CXwzzfXWAPPNrMzMaoAb+OAjQT5kSN0JLCIymJlZBYnfTfj3pIfTFQd1nwT+oZfV9rj7Le7+czP7CPA60Az8jj4eS647gUVEcsjMJgI/cfdZwZj9Znc/799vMLPvAf/m7svO1EZDQCIiecITjxDfbmZ3wumf90zrpzPNLGxmI4P5S4FLgZ+fdR31AEREciP5wXDAARIPhvsFiSfM1pF4SNwL7t7b0E/qtkqAd4LFFmCRu68+6zpKACIihUlDQCIiBUoJQESkQCkBiIgUKCUAEZECpQQgIlKglABERAqUEoCISIH6/yuyBlyU0ArCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import scipy\n",
    "\n",
    "start_id = -6\n",
    "end_id = -1\n",
    "loss_w = errors[start_id:end_id]\n",
    "log_ws = np.log(n_params[start_id:end_id])\n",
    "log_loss = np.log(errors[start_id:end_id])\n",
    "\n",
    "reg = LinearRegression().fit(log_ws[:,np.newaxis], log_loss)\n",
    "eps_max = np.min(loss_w)*0.999\n",
    "\n",
    "num_sweep = 10001\n",
    "eps0_sweep = np.linspace(0, eps_max, num=num_sweep)\n",
    "scores = []\n",
    "\n",
    "for i in range(num_sweep):\n",
    "    score = np.abs(scipy.stats.pearsonr(log_ws, np.log(loss_w-eps0_sweep[i]))[0])\n",
    "    scores.append(score)\n",
    "    \n",
    "plt.plot(eps0_sweep, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0\n",
      "-1.9184066707629372\n",
      "0.0033148606783546743\n"
     ]
    }
   ],
   "source": [
    "max_id = np.argmax(scores)\n",
    "eps0 = eps0_sweep[max_id]\n",
    "reg.fit(log_ws[:,np.newaxis], np.log(loss_w-eps0))\n",
    "alpha = reg.coef_[0]\n",
    "A = np.e**reg.intercept_\n",
    "print(max_id)\n",
    "print(eps0)\n",
    "print(alpha)\n",
    "print(A)\n",
    "#[eps0, A, alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 5.549117469329293e-23\n",
       " hess_inv: array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]])\n",
       "      jac: array([1.49027668e-08, 9.45245568e-18, 2.01377313e-19])\n",
       "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
       "     nfev: 236\n",
       "      nit: 0\n",
       "     njev: 56\n",
       "   status: 2\n",
       "  success: False\n",
       "        x: array([ 0.        ,  0.00331486, -1.91840667])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def f(x):\n",
    "    eps0 = x[0]\n",
    "    A = x[1]\n",
    "    alpha = x[2]\n",
    "    return np.mean((loss_w-(A*n_params[start_id:end_id]**alpha+eps0))**2)\n",
    "\n",
    "x0 = np.array([eps0, A, alpha])\n",
    "sol = minimize(f, x0, tol=1e-32, options={'gtol':1e-30})\n",
    "sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "\n",
    "# Doubt: NN may have bad landscapes. BFGS may perform worse than Adam at high loss.\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Define Transformer\n",
    "\n",
    "\n",
    "# (t,rho,z) -> (t',rho',z')\n",
    "class T(nn.Module):\n",
    "    def __init__(self,w=256,a=0.,M=1.):\n",
    "        super(T, self).__init__()\n",
    "        self.l1 = nn.Linear(2,w)\n",
    "        self.l2 = nn.Linear(w,w)\n",
    "        self.l3 = nn.Linear(w,4)\n",
    "        self.a = a\n",
    "        self.M = torch.nn.Parameter(torch.ones(1,)*1., requires_grad=False)\n",
    "        self.eps = torch.nn.Parameter(torch.ones(1,)*0.01, requires_grad=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        # These non-polynomial activation function may not lead to power laws\n",
    "        #f = nn.Tanh()\n",
    "        #f = nn.SiLU()\n",
    "        #f = Rational()\n",
    "        f = nn.ReLU()\n",
    "        self.t = x[:,[0]]\n",
    "        self.x = x[:,[1]]\n",
    "        self.y = x[:,[2]]\n",
    "        self.z = x[:,[3]]\n",
    "        self.r = torch.sqrt(self.x**2+self.y**2+self.z**2)\n",
    "        self.u = torch.sqrt(self.r/(2*self.M))\n",
    "\n",
    "        self.rho = torch.unsqueeze(torch.sqrt(x[:,1]**2+x[:,2]**2),dim=1)\n",
    "        self.rhoz = torch.transpose(torch.stack([self.rho,self.z]),0,1)[:,:,0]\n",
    "        \n",
    "        self.x1 = f(self.l1(self.rhoz))\n",
    "        self.x2 = f(self.l2(self.x1))**2\n",
    "        self.x3 = self.l3(self.x2)\n",
    "\n",
    "        self.dt = self.x3[:,[0]]\n",
    "        self.drho = self.x3[:,[1]]\n",
    "        self.dphi = self.x3[:,[2]]\n",
    "        self.dz = self.x3[:,[3]]\n",
    "        nn_out = torch.empty((x.shape[0], 4), requires_grad=False)\n",
    "        nn_out[:,[0]] = self.dt\n",
    "        nn_out[:,[1]] = (-self.y*self.dphi+self.x*self.drho)\n",
    "        nn_out[:,[2]] = (self.x*self.dphi+self.y*self.drho)\n",
    "        nn_out[:,[3]] = self.dz*self.z\n",
    "\n",
    "        return x + self.eps.unsqueeze(dim=0)*nn_out\n",
    "    \n",
    "    def set_a(self,a):\n",
    "        self.a = a\n",
    "        \n",
    "    def batch_jacobian(self, func, x, create_graph=False):\n",
    "        # x in shape (Batch, Length)\n",
    "        def _func_sum(x):\n",
    "            return func(x).sum(dim=0)\n",
    "        return torch.autograd.functional.jacobian(_func_sum, x, create_graph=create_graph).permute(1,0,2)\n",
    "    \n",
    "    def transform_g(self, x):\n",
    "        jac_ts = self.batch_jacobian(self.forward, x, create_graph=True)\n",
    "        jac_inv_ts = torch.inverse(jac_ts)\n",
    "        return torch.matmul(torch.matmul(jac_inv_ts.permute(0,2,1), g(x)),jac_inv_ts)\n",
    "        \n",
    "    def jac(self, x):\n",
    "        jac_ts = self.batch_jacobian(self.forward, x, create_graph=True)\n",
    "        return jac_ts\n",
    "    \n",
    "\n",
    "def grow(t1, t2, w_s, w_l):\n",
    "\n",
    "    w_mask = torch.zeros(w_l,w_l)\n",
    "    w_mask[:w_s,:w_s] = torch.ones(w_s,w_s)\n",
    "\n",
    "    b_mask = torch.zeros(w_l,)\n",
    "    b_mask[:w_s] = torch.ones(w_s,)\n",
    "\n",
    "    t2.l2.weight.data = t2.l2.weight.data*w_mask\n",
    "    t2.l2.weight.data[:w_s,:w_s] = t1.l2.weight.data\n",
    "    t2.l2.bias.data = t2.l2.bias.data*b_mask\n",
    "    t2.l2.bias.data[:w_s] = t1.l2.bias.data\n",
    "\n",
    "    t2.l1.weight.data[:w_s,:] = t1.l1.weight.data\n",
    "    t2.l1.bias.data[:w_s] = t1.l1.bias.data\n",
    "\n",
    "    t2.l3.weight.data[:,:w_s] = t1.l3.weight.data\n",
    "    t2.l3.bias.data = t1.l3.bias.data\n",
    "    return t2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "n_train = 1000\n",
    "\n",
    "W = torch.normal(0,1,size=(n_train,4),requires_grad=True)\n",
    "input_ = torch.empty(n_train,4, requires_grad=False)\n",
    "input_[:,0] = (torch.rand(n_train, requires_grad=True)-0.5)*3.0 + 1.5\n",
    "rs = torch.linspace(3,4,steps=n_train+1)[:n_train]\n",
    "input_[:,1:] = W[:,1:]/torch.norm(W[:,1:], dim=1, keepdim=True)*torch.unsqueeze(rs, dim=1)\n",
    "\n",
    "n_test = 1000\n",
    "\n",
    "W_test = torch.normal(0,1,size=(n_test,4),requires_grad=True)\n",
    "input_test_ = torch.empty(n_test,4, requires_grad=False)\n",
    "input_test_[:,0] = (torch.rand(n_test, requires_grad=True)-0.5)*3.0 + 1.5\n",
    "rs_test = torch.linspace(3,4,steps=n_test+1)[:n_test]\n",
    "input_test_[:,1:] = W_test[:,1:]/torch.norm(W_test[:,1:], dim=1, keepdim=True)*torch.unsqueeze(rs_test, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w=5\n",
      "a_kerr=0.0\n",
      "M=1.0\n",
      "r_c=2.0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# w/wo growing network\n",
    "# Tanh, Relu, Relu2 activation, last-layer act(other relu). Even: Should we use spline networks?\n",
    "# Hybrid: Adam + LBFGS\n",
    "# Possible overfitting: train & test gap\n",
    "# LBFGS usage\n",
    "\n",
    "#ws = [50,60,70,80,90,100,120,140,160,180,200,250,300,350,400]\n",
    "ws = [5,10,15,20,25,30,35,40,45,50]\n",
    "#ws = [14]\n",
    "#ws = [2,4]\n",
    "losses_w = []\n",
    "losses_w_test = []\n",
    "times = []\n",
    "\n",
    "a_s = [0.0]\n",
    "\n",
    "w_i = 0\n",
    "\n",
    "for w in ws:\n",
    "    \n",
    "    print(\"w={}\".format(w))\n",
    "    if w_i == 0:\n",
    "        t = T(w=w, a=0.0, M=1.0)\n",
    "    else:\n",
    "        sd = t.state_dict()\n",
    "        t_old = T(w=w_old, a=0.0, M=1.0)\n",
    "        t_old.load_state_dict(sd)\n",
    "        t = T(w=w, a=0.0, M=1.0)\n",
    "        t = grow(t_old, t, w_old, w)\n",
    "    w_i = w_i + 1\n",
    "\n",
    "\n",
    "    losses_a = []\n",
    "    losses_all = []\n",
    "    a_i = 0\n",
    "\n",
    "\n",
    "\n",
    "    for a_kerr in a_s:\n",
    "        M = 1.0\n",
    "        r_c = M + np.sqrt(M**2-a_kerr**2)\n",
    "        print(\"a_kerr={}\".format(a_kerr))\n",
    "        print(\"M={}\".format(M))\n",
    "        print(\"r_c={}\".format(r_c))\n",
    "        #t = T(w=400, a=a_kerr, M=M)\n",
    "        t.set_a(a_kerr)\n",
    "        a_i += 1\n",
    "\n",
    "\n",
    "        # Kerr Metric\n",
    "        def g(x_):\n",
    "            a = a_kerr\n",
    "            bs = x_.shape[0]\n",
    "            t = x_[:,0]\n",
    "            x = x_[:,1]\n",
    "            y = x_[:,2]\n",
    "            z = x_[:,3]\n",
    "            rho = torch.sqrt(x**2+y**2)\n",
    "            r = torch.sqrt(x**2+y**2+z**2)\n",
    "            costheta = z/r\n",
    "            sintheta = rho/r\n",
    "            sin2theta = 2*sintheta*costheta\n",
    "            cos2theta = 2*costheta**2 - 1\n",
    "            cosphi = x/rho\n",
    "            sinphi = y/rho\n",
    "            sigma = r**2 + a**2*costheta**2\n",
    "            zeta = torch.sqrt(r**2+a**2)\n",
    "            sq2 = torch.sqrt(torch.tensor(2., dtype=torch.float, requires_grad=False))\n",
    "\n",
    "            u1 = a**2 + 2*r**2 + a**2*cos2theta\n",
    "            zeta = torch.sqrt(a**2+r**2)\n",
    "            u2 = u1/(zeta*torch.sqrt(M*r))\n",
    "            u3 = 8*a*M/sigma\n",
    "            u4 = zeta**2 + 2*a**2*M*r*sintheta**2/sigma\n",
    "\n",
    "            g00 = -1 + 2*M*r/sigma\n",
    "            g01 = g10 = 1/4*sintheta*(sq2*u2*cosphi+u3*sinphi)\n",
    "            g02 = g20 = 1/4*sintheta*(-u3*cosphi+sq2*u2*sinphi)\n",
    "            g03 = g30 = costheta*u2/sq2**3\n",
    "            g11 = (8*costheta**2*cosphi**2*sigma-u2**2*r**2*cosphi**2*sintheta**2+8*u4*sinphi**2)/(8*r**2)\n",
    "            g12 = g21 = cosphi*(8*costheta**2*sigma-u2**2*r**2*sintheta**2-8*u4)*sinphi/(8*r**2)\n",
    "            g13 = g31 = costheta*(-8*sigma-u2**2*r**2)*cosphi*sintheta/(8*r**2)\n",
    "            g22 = (8*cosphi**2*u4+8*costheta**2*sigma*sinphi**2-u2**2*r**2*sintheta**2*sinphi**2)/(8*r**2)\n",
    "            g23 = g32 = costheta*(-8*sigma-u2**2*r**2)*sintheta*sinphi/(8*r**2)\n",
    "            g33 = sintheta**2 + costheta**2*(-u2**2*r**2+8*a**2*sintheta**2)/(8*r**2)\n",
    "\n",
    "            stack1 = torch.stack([g00, g01, g02, g03])\n",
    "            stack2 = torch.stack([g10, g11, g12, g13])\n",
    "            stack3 = torch.stack([g20, g21, g22, g23])\n",
    "            stack4 = torch.stack([g30, g31, g32, g33])\n",
    "\n",
    "            gs = - torch.stack([stack1, stack2, stack3, stack4]).permute(2,0,1)\n",
    "            return gs\n",
    "        \n",
    "        def euclidean_loss(t,inputs):\n",
    "            gp = t.transform_g(inputs)\n",
    "            bs = gp.shape[0]\n",
    "            minkowski_metric = torch.unsqueeze(torch.unsqueeze(torch.ones(bs,),dim=1),dim=2) * torch.unsqueeze(torch.diag(torch.tensor([-1.,-1.,-1.], dtype=torch.float, requires_grad=True)), dim=0)\n",
    "            return torch.mean((gp[:,1:,1:]-minkowski_metric)**2)\n",
    "\n",
    "\n",
    "\n",
    "        lr = 1\n",
    "        '''if w_i == 0:\n",
    "            epochs = 2500\n",
    "            BFGS_epoch = 2000\n",
    "        else:\n",
    "            epochs = 500\n",
    "            BFGS_epoch = 0'''\n",
    "        \n",
    "        # We need to understand LBFGS better\n",
    "        epochs = 300\n",
    "        switch_epoch = 100\n",
    "        BFGS_epoch = 1000000\n",
    "\n",
    "        #optimizer = optim.Adam(t.parameters(), lr=lr, eps=1e-8)\n",
    "        #optimizer = optim.SGD(t.parameters(),lr=lr)\n",
    "        optimizer = optim.LBFGS(t.parameters(), lr=lr, max_iter=1e10, max_eval=1e10, tolerance_grad=1e-30, tolerance_change=1e-30, history_size=100, line_search_fn='strong_wolfe')\n",
    "        #optimizer = optim.LBFGS(t.parameters(), lr=0.1, max_iter=100, max_eval=1e10, tolerance_grad=1e-30, tolerance_change=1e-30, history_size=100)\n",
    "\n",
    "\n",
    "\n",
    "        #epochs = 100\n",
    "        #optimizer = optim.LBFGS(t.parameters(), lr=1, max_iter=100, max_eval=None, tolerance_grad=1e-30, tolerance_change=1e-30, history_size=100, line_search_fn=\"strong_wolfe\")\n",
    "        #optimizer = optim.LBFGS(t.parameters(), lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn=None)\n",
    "\n",
    "        log_save = 100000\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        n_train = 1000\n",
    "        \n",
    "        best_loss = 10000\n",
    "        \n",
    "        start = time.time()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            if (epoch+1) % switch_epoch == 0:\n",
    "                for opt_param in optimizer.param_groups:\n",
    "                    lr = lr * 0.5\n",
    "                    opt_param['lr'] = lr\n",
    "\n",
    "            if epoch == BFGS_epoch:\n",
    "                # BFGS learning rate. How to set?\n",
    "                optimizer = optim.LBFGS(t.parameters(), lr=0.1, max_iter=1e10, max_eval=1e10, tolerance_grad=1e-30, tolerance_change=1e-30, history_size=100, line_search_fn='strong_wolfe')\n",
    "\n",
    "            if epoch < BFGS_epoch:\n",
    "                log = 1\n",
    "                batch_size = n_train\n",
    "            else:\n",
    "                log = 1\n",
    "                batch_size = n_train\n",
    "            t.train()\n",
    "\n",
    "\n",
    "            choices = np.random.choice(n_train, batch_size, replace=False)\n",
    "            inputs = input_[choices]\n",
    "\n",
    "            # -------------------------------------------\n",
    "            def loss_closure():\n",
    "                if torch.is_grad_enabled():\n",
    "                    optimizer.zero_grad()\n",
    "                loss_inner = euclidean_loss(t,inputs)\n",
    "                #if loss_inner.requires_grad:\n",
    "                    #loss_inner.backward(retain_graph=True)\n",
    "                    #loss_inner.backward()\n",
    "                loss_inner.backward(retain_graph=True)\n",
    "                return loss_inner\n",
    "            # -------------------------------------------\n",
    "            loss = loss_closure()\n",
    "            # best_loss is trick for better scaling laws\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_epoch = epoch\n",
    "                def loss_closure_test():\n",
    "                    if torch.is_grad_enabled():\n",
    "                        optimizer.zero_grad()\n",
    "                    loss_inner = euclidean_loss(t,input_test_)\n",
    "                    return loss_inner\n",
    "                loss_test = loss_closure_test()\n",
    "            optimizer.step(loss_closure)  # get loss, use to update wts\n",
    "\n",
    "            losses.append(loss.detach().numpy())\n",
    "            losses_all.append(loss.detach().numpy())\n",
    "            '''loss = euclidean_loss(g,t,inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()'''\n",
    "            \n",
    "\n",
    "            if epoch%log == 0:\n",
    "                print('Epoch:  %d | loss: %.12f ' %(epoch, loss))\n",
    "\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    print(\"time={}\".format(duration))\n",
    "    times.append(duration)\n",
    "\n",
    "\n",
    "                \n",
    "    w_old = w\n",
    "    losses_w.append(best_loss.detach().numpy())\n",
    "    losses_w_test.append(loss_test.detach().numpy())\n",
    "    print(\"best_train_loss={}\".format(best_loss.detach().numpy()))\n",
    "    print(\"test_loss={}\".format(loss_test.detach().numpy()))\n",
    "    print(\"best_epoch={}\".format(best_epoch))\n",
    "    \n",
    "np.save('./results_nn/grow_relu2last_bfgs_nolrdecay',np.array([ws, losses_w, times, losses_w_test]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.00000000e+00, 1.00000000e+01, 1.50000000e+01, 2.00000000e+01,\n",
       "        2.50000000e+01, 3.00000000e+01, 3.50000000e+01, 4.00000000e+01,\n",
       "        4.50000000e+01, 5.00000000e+01],\n",
       "       [8.40038896e-01, 8.39580059e-01, 8.38885963e-01, 8.37528229e-01,\n",
       "        8.34495544e-01, 8.26765597e-01, 8.11857939e-01, 7.84920931e-01,\n",
       "        7.46198416e-01, 6.89135432e-01],\n",
       "       [2.39160061e-02, 2.55928040e-02, 2.62503624e-02, 2.72879601e-02,\n",
       "        2.90760994e-02, 3.07948589e-02, 3.64809036e-02, 3.75072956e-02,\n",
       "        4.11810875e-02, 4.08461094e-02],\n",
       "       [8.40034604e-01, 8.39553475e-01, 8.38789821e-01, 8.37325931e-01,\n",
       "        8.34066689e-01, 8.26035142e-01, 8.10749650e-01, 7.83347070e-01,\n",
       "        7.44064569e-01, 6.86552286e-01]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('./results_nn/grow_relu2last_adam.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Next try: relu2 (not just last layer)\n",
    "\n",
    "plt.plot(ws, losses_w, marker=\"o\")\n",
    "#plt.plot(ws, losses_w_test, marker=\"o\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "\n",
    "# Doubt: NN may have bad landscapes. BFGS may perform worse than Adam at high loss.\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Define Transformer\n",
    "\n",
    "\n",
    "# (t,rho,z) -> (t',rho',z')\n",
    "class T(nn.Module):\n",
    "    def __init__(self,w=256,a=0.,M=1.):\n",
    "        super(T, self).__init__()\n",
    "        self.l1 = nn.Linear(2,w)\n",
    "        self.l2 = nn.Linear(w,w)\n",
    "        self.l3 = nn.Linear(w,4)\n",
    "        self.a = a\n",
    "        self.M = torch.nn.Parameter(torch.ones(1,)*1., requires_grad=False)\n",
    "        self.eps = torch.nn.Parameter(torch.ones(1,)*0.01, requires_grad=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        # These non-polynomial activation function may not lead to power laws\n",
    "        #f = nn.Tanh()\n",
    "        #f = nn.SiLU()\n",
    "        #f = Rational()\n",
    "        f = nn.ReLU()\n",
    "        self.t = x[:,[0]]\n",
    "        self.x = x[:,[1]]\n",
    "        self.y = x[:,[2]]\n",
    "        self.z = x[:,[3]]\n",
    "        self.r = torch.sqrt(self.x**2+self.y**2+self.z**2)\n",
    "        self.u = torch.sqrt(self.r/(2*self.M))\n",
    "\n",
    "        self.rho = torch.unsqueeze(torch.sqrt(x[:,1]**2+x[:,2]**2),dim=1)\n",
    "        self.rhoz = torch.transpose(torch.stack([self.rho,self.z]),0,1)[:,:,0]\n",
    "        \n",
    "        self.x1 = f(self.l1(self.rhoz))\n",
    "        self.x2 = f(self.l2(self.x1))**2\n",
    "        self.x3 = self.l3(self.x2)\n",
    "\n",
    "        self.dt = self.x3[:,[0]]\n",
    "        self.drho = self.x3[:,[1]]\n",
    "        self.dphi = self.x3[:,[2]]\n",
    "        self.dz = self.x3[:,[3]]\n",
    "        nn_out = torch.empty((x.shape[0], 4), requires_grad=False)\n",
    "        nn_out[:,[0]] = self.dt\n",
    "        nn_out[:,[1]] = (-self.y*self.dphi+self.x*self.drho)\n",
    "        nn_out[:,[2]] = (self.x*self.dphi+self.y*self.drho)\n",
    "        nn_out[:,[3]] = self.dz*self.z\n",
    "\n",
    "        return x + self.eps.unsqueeze(dim=0)*nn_out\n",
    "    \n",
    "    def set_a(self,a):\n",
    "        self.a = a\n",
    "        \n",
    "    def batch_jacobian(self, func, x, create_graph=False):\n",
    "        # x in shape (Batch, Length)\n",
    "        def _func_sum(x):\n",
    "            return func(x).sum(dim=0)\n",
    "        return torch.autograd.functional.jacobian(_func_sum, x, create_graph=create_graph).permute(1,0,2)\n",
    "    \n",
    "    def transform_g(self, x):\n",
    "        jac_ts = self.batch_jacobian(self.forward, x, create_graph=True)\n",
    "        jac_inv_ts = torch.inverse(jac_ts)\n",
    "        return torch.matmul(torch.matmul(jac_inv_ts.permute(0,2,1), g(x)),jac_inv_ts)\n",
    "        \n",
    "    def jac(self, x):\n",
    "        jac_ts = self.batch_jacobian(self.forward, x, create_graph=True)\n",
    "        return jac_ts\n",
    "    \n",
    "\n",
    "def grow(t1, t2, w_s, w_l):\n",
    "\n",
    "    w_mask = torch.zeros(w_l,w_l)\n",
    "    w_mask[:w_s,:w_s] = torch.ones(w_s,w_s)\n",
    "\n",
    "    b_mask = torch.zeros(w_l,)\n",
    "    b_mask[:w_s] = torch.ones(w_s,)\n",
    "\n",
    "    t2.l2.weight.data = t2.l2.weight.data*w_mask\n",
    "    t2.l2.weight.data[:w_s,:w_s] = t1.l2.weight.data\n",
    "    t2.l2.bias.data = t2.l2.bias.data*b_mask\n",
    "    t2.l2.bias.data[:w_s] = t1.l2.bias.data\n",
    "\n",
    "    t2.l1.weight.data[:w_s,:] = t1.l1.weight.data\n",
    "    t2.l1.bias.data[:w_s] = t1.l1.bias.data\n",
    "\n",
    "    t2.l3.weight.data[:,:w_s] = t1.l3.weight.data\n",
    "    t2.l3.bias.data = t1.l3.bias.data\n",
    "    return t2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "n_train = 1000\n",
    "\n",
    "W = torch.normal(0,1,size=(n_train,4),requires_grad=True)\n",
    "input_ = torch.empty(n_train,4, requires_grad=False)\n",
    "input_[:,0] = (torch.rand(n_train, requires_grad=True)-0.5)*3.0 + 1.5\n",
    "rs = torch.linspace(3,4,steps=n_train+1)[:n_train]\n",
    "input_[:,1:] = W[:,1:]/torch.norm(W[:,1:], dim=1, keepdim=True)*torch.unsqueeze(rs, dim=1)\n",
    "\n",
    "n_test = 1000\n",
    "\n",
    "W_test = torch.normal(0,1,size=(n_test,4),requires_grad=True)\n",
    "input_test_ = torch.empty(n_test,4, requires_grad=False)\n",
    "input_test_[:,0] = (torch.rand(n_test, requires_grad=True)-0.5)*3.0 + 1.5\n",
    "rs_test = torch.linspace(3,4,steps=n_test+1)[:n_test]\n",
    "input_test_[:,1:] = W_test[:,1:]/torch.norm(W_test[:,1:], dim=1, keepdim=True)*torch.unsqueeze(rs_test, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w=5\n",
      "a_kerr=0.0\n",
      "M=1.0\n",
      "r_c=2.0\n",
      "Epoch:  0 | loss: 0.840038895607 \n",
      "Epoch:  1 | loss: 0.030007967725 \n",
      "Epoch:  2 | loss: 0.027235690504 \n",
      "Epoch:  3 | loss: 0.027235690504 \n",
      "Epoch:  4 | loss: 0.027235692367 \n",
      "Epoch:  5 | loss: 0.027235694230 \n",
      "Epoch:  6 | loss: 0.027235692367 \n",
      "Epoch:  7 | loss: 0.027235692367 \n",
      "Epoch:  8 | loss: 0.027235688642 \n",
      "Epoch:  9 | loss: 0.027235690504 \n",
      "Epoch:  10 | loss: 0.027235690504 \n",
      "Epoch:  11 | loss: 0.027235692367 \n",
      "Epoch:  12 | loss: 0.027235692367 \n",
      "Epoch:  13 | loss: 0.027235690504 \n",
      "Epoch:  14 | loss: 0.027235694230 \n",
      "Epoch:  15 | loss: 0.027235694230 \n",
      "Epoch:  16 | loss: 0.027235692367 \n",
      "Epoch:  17 | loss: 0.027235692367 \n",
      "Epoch:  18 | loss: 0.027235694230 \n",
      "Epoch:  19 | loss: 0.027235694230 \n",
      "Epoch:  20 | loss: 0.027235692367 \n",
      "Epoch:  21 | loss: 0.027235692367 \n",
      "Epoch:  22 | loss: 0.027235690504 \n",
      "Epoch:  23 | loss: 0.027235694230 \n",
      "Epoch:  24 | loss: 0.027235690504 \n",
      "Epoch:  25 | loss: 0.027235694230 \n",
      "Epoch:  26 | loss: 0.027235692367 \n",
      "Epoch:  27 | loss: 0.027235690504 \n",
      "Epoch:  28 | loss: 0.027235692367 \n",
      "Epoch:  29 | loss: 0.027235694230 \n",
      "Epoch:  30 | loss: 0.027235694230 \n",
      "Epoch:  31 | loss: 0.027235692367 \n",
      "Epoch:  32 | loss: 0.027235690504 \n",
      "Epoch:  33 | loss: 0.027235692367 \n",
      "Epoch:  34 | loss: 0.027235692367 \n",
      "Epoch:  35 | loss: 0.027235692367 \n",
      "Epoch:  36 | loss: 0.027235692367 \n",
      "Epoch:  37 | loss: 0.027235690504 \n",
      "Epoch:  38 | loss: 0.027235692367 \n",
      "Epoch:  39 | loss: 0.027235694230 \n",
      "Epoch:  40 | loss: 0.027235690504 \n",
      "Epoch:  41 | loss: 0.027235692367 \n",
      "Epoch:  42 | loss: 0.027235690504 \n",
      "Epoch:  43 | loss: 0.027235694230 \n",
      "Epoch:  44 | loss: 0.027235692367 \n",
      "Epoch:  45 | loss: 0.027235690504 \n",
      "Epoch:  46 | loss: 0.027235694230 \n",
      "Epoch:  47 | loss: 0.027235690504 \n",
      "Epoch:  48 | loss: 0.027235690504 \n",
      "Epoch:  49 | loss: 0.027235690504 \n",
      "Epoch:  50 | loss: 0.027235694230 \n",
      "Epoch:  51 | loss: 0.027235694230 \n",
      "Epoch:  52 | loss: 0.027235692367 \n",
      "Epoch:  53 | loss: 0.027235692367 \n",
      "Epoch:  54 | loss: 0.027235692367 \n",
      "Epoch:  55 | loss: 0.027235692367 \n",
      "Epoch:  56 | loss: 0.027235690504 \n",
      "Epoch:  57 | loss: 0.027235692367 \n",
      "Epoch:  58 | loss: 0.027235692367 \n",
      "Epoch:  59 | loss: 0.027235696092 \n",
      "Epoch:  60 | loss: 0.027235692367 \n",
      "Epoch:  61 | loss: 0.027235692367 \n",
      "Epoch:  62 | loss: 0.027235690504 \n",
      "Epoch:  63 | loss: 0.027235692367 \n",
      "Epoch:  64 | loss: 0.027235692367 \n",
      "Epoch:  65 | loss: 0.027235694230 \n",
      "Epoch:  66 | loss: 0.027235692367 \n",
      "Epoch:  67 | loss: 0.027235690504 \n",
      "Epoch:  68 | loss: 0.027235692367 \n",
      "Epoch:  69 | loss: 0.027235694230 \n",
      "Epoch:  70 | loss: 0.027235694230 \n",
      "Epoch:  71 | loss: 0.027235692367 \n",
      "Epoch:  72 | loss: 0.027235692367 \n",
      "Epoch:  73 | loss: 0.027235692367 \n",
      "Epoch:  74 | loss: 0.027235692367 \n",
      "Epoch:  75 | loss: 0.027235692367 \n",
      "Epoch:  76 | loss: 0.027235692367 \n",
      "Epoch:  77 | loss: 0.027235692367 \n",
      "Epoch:  78 | loss: 0.027235690504 \n",
      "Epoch:  79 | loss: 0.027235692367 \n",
      "Epoch:  80 | loss: 0.027235692367 \n",
      "Epoch:  81 | loss: 0.027235694230 \n",
      "Epoch:  82 | loss: 0.027235690504 \n",
      "Epoch:  83 | loss: 0.027235690504 \n",
      "Epoch:  84 | loss: 0.027235690504 \n",
      "Epoch:  85 | loss: 0.027235690504 \n",
      "Epoch:  86 | loss: 0.027235692367 \n",
      "Epoch:  87 | loss: 0.027235692367 \n",
      "Epoch:  88 | loss: 0.027235690504 \n",
      "Epoch:  89 | loss: 0.027235692367 \n",
      "Epoch:  90 | loss: 0.027235694230 \n",
      "Epoch:  91 | loss: 0.027235694230 \n",
      "Epoch:  92 | loss: 0.027235692367 \n",
      "Epoch:  93 | loss: 0.027235692367 \n",
      "Epoch:  94 | loss: 0.027235692367 \n",
      "Epoch:  95 | loss: 0.027235692367 \n",
      "Epoch:  96 | loss: 0.027235694230 \n",
      "Epoch:  97 | loss: 0.027235692367 \n",
      "Epoch:  98 | loss: 0.027235692367 \n",
      "Epoch:  99 | loss: 0.027235690504 \n",
      "Epoch:  100 | loss: 0.027235694230 \n",
      "Epoch:  101 | loss: 0.027235692367 \n",
      "Epoch:  102 | loss: 0.027235692367 \n",
      "Epoch:  103 | loss: 0.027235692367 \n",
      "Epoch:  104 | loss: 0.027235694230 \n",
      "Epoch:  105 | loss: 0.027235692367 \n",
      "Epoch:  106 | loss: 0.027235692367 \n",
      "Epoch:  107 | loss: 0.027235694230 \n",
      "Epoch:  108 | loss: 0.027235694230 \n",
      "Epoch:  109 | loss: 0.027235694230 \n",
      "Epoch:  110 | loss: 0.027235690504 \n",
      "Epoch:  111 | loss: 0.027235692367 \n",
      "Epoch:  112 | loss: 0.027235692367 \n",
      "Epoch:  113 | loss: 0.027235690504 \n",
      "Epoch:  114 | loss: 0.027235692367 \n",
      "Epoch:  115 | loss: 0.027235692367 \n",
      "Epoch:  116 | loss: 0.027235690504 \n",
      "Epoch:  117 | loss: 0.027235692367 \n",
      "Epoch:  118 | loss: 0.027235694230 \n",
      "Epoch:  119 | loss: 0.027235690504 \n",
      "Epoch:  120 | loss: 0.027235690504 \n",
      "Epoch:  121 | loss: 0.027235690504 \n",
      "Epoch:  122 | loss: 0.027235690504 \n",
      "Epoch:  123 | loss: 0.027235692367 \n",
      "Epoch:  124 | loss: 0.027235690504 \n",
      "Epoch:  125 | loss: 0.027235692367 \n",
      "Epoch:  126 | loss: 0.027235690504 \n",
      "Epoch:  127 | loss: 0.027235692367 \n",
      "Epoch:  128 | loss: 0.027235690504 \n",
      "Epoch:  129 | loss: 0.027235690504 \n",
      "Epoch:  130 | loss: 0.027235692367 \n",
      "Epoch:  131 | loss: 0.027235692367 \n",
      "Epoch:  132 | loss: 0.027235692367 \n",
      "Epoch:  133 | loss: 0.027235690504 \n",
      "Epoch:  134 | loss: 0.027235692367 \n",
      "Epoch:  135 | loss: 0.027235690504 \n",
      "Epoch:  136 | loss: 0.027235690504 \n",
      "Epoch:  137 | loss: 0.027235690504 \n",
      "Epoch:  138 | loss: 0.027235692367 \n",
      "Epoch:  139 | loss: 0.027235690504 \n",
      "Epoch:  140 | loss: 0.027235692367 \n",
      "Epoch:  141 | loss: 0.027235692367 \n",
      "Epoch:  142 | loss: 0.027235690504 \n",
      "Epoch:  143 | loss: 0.027235690504 \n",
      "Epoch:  144 | loss: 0.027235694230 \n",
      "Epoch:  145 | loss: 0.027235692367 \n",
      "Epoch:  146 | loss: 0.027235692367 \n",
      "Epoch:  147 | loss: 0.027235694230 \n",
      "Epoch:  148 | loss: 0.027235694230 \n",
      "Epoch:  149 | loss: 0.027235692367 \n",
      "Epoch:  150 | loss: 0.027235694230 \n",
      "Epoch:  151 | loss: 0.027235690504 \n",
      "Epoch:  152 | loss: 0.027235692367 \n",
      "Epoch:  153 | loss: 0.027235690504 \n",
      "Epoch:  154 | loss: 0.027235694230 \n",
      "Epoch:  155 | loss: 0.027235690504 \n",
      "Epoch:  156 | loss: 0.027235694230 \n",
      "Epoch:  157 | loss: 0.027235690504 \n",
      "Epoch:  158 | loss: 0.027235690504 \n",
      "Epoch:  159 | loss: 0.027235694230 \n",
      "Epoch:  160 | loss: 0.027235692367 \n",
      "Epoch:  161 | loss: 0.027235694230 \n",
      "Epoch:  162 | loss: 0.027235690504 \n",
      "Epoch:  163 | loss: 0.027235692367 \n",
      "Epoch:  164 | loss: 0.027235690504 \n",
      "Epoch:  165 | loss: 0.027235692367 \n",
      "Epoch:  166 | loss: 0.027235692367 \n",
      "Epoch:  167 | loss: 0.027235694230 \n",
      "Epoch:  168 | loss: 0.027235690504 \n",
      "Epoch:  169 | loss: 0.027235692367 \n",
      "Epoch:  170 | loss: 0.027235694230 \n",
      "Epoch:  171 | loss: 0.027235690504 \n",
      "Epoch:  172 | loss: 0.027235690504 \n",
      "Epoch:  173 | loss: 0.027235690504 \n",
      "Epoch:  174 | loss: 0.027235692367 \n",
      "Epoch:  175 | loss: 0.027235690504 \n",
      "Epoch:  176 | loss: 0.027235690504 \n",
      "Epoch:  177 | loss: 0.027235690504 \n",
      "Epoch:  178 | loss: 0.027235692367 \n",
      "Epoch:  179 | loss: 0.027235690504 \n",
      "Epoch:  180 | loss: 0.027235690504 \n",
      "Epoch:  181 | loss: 0.027235692367 \n",
      "Epoch:  182 | loss: 0.027235690504 \n",
      "Epoch:  183 | loss: 0.027235690504 \n",
      "Epoch:  184 | loss: 0.027235690504 \n",
      "Epoch:  185 | loss: 0.027235692367 \n",
      "Epoch:  186 | loss: 0.027235692367 \n",
      "Epoch:  187 | loss: 0.027235690504 \n",
      "Epoch:  188 | loss: 0.027235692367 \n",
      "Epoch:  189 | loss: 0.027235690504 \n",
      "Epoch:  190 | loss: 0.027235690504 \n",
      "Epoch:  191 | loss: 0.027235690504 \n",
      "Epoch:  192 | loss: 0.027235694230 \n",
      "Epoch:  193 | loss: 0.027235690504 \n",
      "Epoch:  194 | loss: 0.027235694230 \n",
      "Epoch:  195 | loss: 0.027235690504 \n",
      "Epoch:  196 | loss: 0.027235692367 \n",
      "Epoch:  197 | loss: 0.027235692367 \n",
      "Epoch:  198 | loss: 0.027235690504 \n",
      "Epoch:  199 | loss: 0.027235692367 \n",
      "Epoch:  200 | loss: 0.027235694230 \n",
      "Epoch:  201 | loss: 0.027235690504 \n",
      "Epoch:  202 | loss: 0.027235690504 \n",
      "Epoch:  203 | loss: 0.027235690504 \n",
      "Epoch:  204 | loss: 0.027235690504 \n",
      "Epoch:  205 | loss: 0.027235694230 \n",
      "Epoch:  206 | loss: 0.027235690504 \n",
      "Epoch:  207 | loss: 0.027235690504 \n",
      "Epoch:  208 | loss: 0.027235692367 \n",
      "Epoch:  209 | loss: 0.027235692367 \n",
      "Epoch:  210 | loss: 0.027235690504 \n",
      "Epoch:  211 | loss: 0.027235692367 \n",
      "Epoch:  212 | loss: 0.027235690504 \n",
      "Epoch:  213 | loss: 0.027235690504 \n",
      "Epoch:  214 | loss: 0.027235692367 \n",
      "Epoch:  215 | loss: 0.027235692367 \n",
      "Epoch:  216 | loss: 0.027235690504 \n",
      "Epoch:  217 | loss: 0.027235690504 \n",
      "Epoch:  218 | loss: 0.027235690504 \n",
      "Epoch:  219 | loss: 0.027235690504 \n",
      "Epoch:  220 | loss: 0.027235692367 \n",
      "Epoch:  221 | loss: 0.027235690504 \n",
      "Epoch:  222 | loss: 0.027235690504 \n",
      "Epoch:  223 | loss: 0.027235690504 \n",
      "Epoch:  224 | loss: 0.027235690504 \n",
      "Epoch:  225 | loss: 0.027235690504 \n",
      "Epoch:  226 | loss: 0.027235690504 \n",
      "Epoch:  227 | loss: 0.027235690504 \n",
      "Epoch:  228 | loss: 0.027235692367 \n",
      "Epoch:  229 | loss: 0.027235692367 \n",
      "Epoch:  230 | loss: 0.027235690504 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  231 | loss: 0.027235692367 \n",
      "Epoch:  232 | loss: 0.027235692367 \n",
      "Epoch:  233 | loss: 0.027235690504 \n",
      "Epoch:  234 | loss: 0.027235692367 \n",
      "Epoch:  235 | loss: 0.027235690504 \n",
      "Epoch:  236 | loss: 0.027235690504 \n",
      "Epoch:  237 | loss: 0.027235690504 \n",
      "Epoch:  238 | loss: 0.027235690504 \n",
      "Epoch:  239 | loss: 0.027235694230 \n",
      "Epoch:  240 | loss: 0.027235692367 \n",
      "Epoch:  241 | loss: 0.027235690504 \n",
      "Epoch:  242 | loss: 0.027235692367 \n",
      "Epoch:  243 | loss: 0.027235694230 \n",
      "Epoch:  244 | loss: 0.027235692367 \n",
      "Epoch:  245 | loss: 0.027235690504 \n",
      "Epoch:  246 | loss: 0.027235692367 \n",
      "Epoch:  247 | loss: 0.027235690504 \n",
      "Epoch:  248 | loss: 0.027235692367 \n",
      "Epoch:  249 | loss: 0.027235692367 \n",
      "Epoch:  250 | loss: 0.027235690504 \n",
      "Epoch:  251 | loss: 0.027235692367 \n",
      "Epoch:  252 | loss: 0.027235690504 \n",
      "Epoch:  253 | loss: 0.027235692367 \n",
      "Epoch:  254 | loss: 0.027235692367 \n",
      "Epoch:  255 | loss: 0.027235688642 \n",
      "Epoch:  256 | loss: 0.027235690504 \n",
      "Epoch:  257 | loss: 0.027235692367 \n",
      "Epoch:  258 | loss: 0.027235694230 \n",
      "Epoch:  259 | loss: 0.027235690504 \n",
      "Epoch:  260 | loss: 0.027235690504 \n",
      "Epoch:  261 | loss: 0.027235688642 \n",
      "Epoch:  262 | loss: 0.027235692367 \n",
      "Epoch:  263 | loss: 0.027235690504 \n",
      "Epoch:  264 | loss: 0.027235690504 \n",
      "Epoch:  265 | loss: 0.027235690504 \n",
      "Epoch:  266 | loss: 0.027235690504 \n",
      "Epoch:  267 | loss: 0.027235690504 \n",
      "Epoch:  268 | loss: 0.027235692367 \n",
      "Epoch:  269 | loss: 0.027235690504 \n",
      "Epoch:  270 | loss: 0.027235692367 \n",
      "Epoch:  271 | loss: 0.027235690504 \n",
      "Epoch:  272 | loss: 0.027235692367 \n",
      "Epoch:  273 | loss: 0.027235692367 \n",
      "Epoch:  274 | loss: 0.027235692367 \n",
      "Epoch:  275 | loss: 0.027235690504 \n",
      "Epoch:  276 | loss: 0.027235690504 \n",
      "Epoch:  277 | loss: 0.027235692367 \n",
      "Epoch:  278 | loss: 0.027235692367 \n",
      "Epoch:  279 | loss: 0.027235692367 \n",
      "Epoch:  280 | loss: 0.027235692367 \n",
      "Epoch:  281 | loss: 0.027235692367 \n",
      "Epoch:  282 | loss: 0.027235692367 \n",
      "Epoch:  283 | loss: 0.027235692367 \n",
      "Epoch:  284 | loss: 0.027235694230 \n",
      "Epoch:  285 | loss: 0.027235690504 \n",
      "Epoch:  286 | loss: 0.027235690504 \n",
      "Epoch:  287 | loss: 0.027235692367 \n",
      "Epoch:  288 | loss: 0.027235690504 \n",
      "Epoch:  289 | loss: 0.027235694230 \n",
      "Epoch:  290 | loss: 0.027235690504 \n",
      "Epoch:  291 | loss: 0.027235692367 \n",
      "Epoch:  292 | loss: 0.027235690504 \n",
      "Epoch:  293 | loss: 0.027235692367 \n",
      "Epoch:  294 | loss: 0.027235690504 \n",
      "Epoch:  295 | loss: 0.027235688642 \n",
      "Epoch:  296 | loss: 0.027235692367 \n",
      "Epoch:  297 | loss: 0.027235692367 \n",
      "Epoch:  298 | loss: 0.027235690504 \n",
      "Epoch:  299 | loss: 0.027235692367 \n",
      "time=25.954263925552368\n",
      "best_train_loss=0.027235688641667366\n",
      "test_loss=0.02617640607059002\n",
      "best_epoch=8\n",
      "w=10\n",
      "a_kerr=0.0\n",
      "M=1.0\n",
      "r_c=2.0\n",
      "Epoch:  0 | loss: 0.027235692367 \n",
      "Epoch:  1 | loss: 0.027235440910 \n",
      "Epoch:  2 | loss: 0.027235437185 \n",
      "Epoch:  3 | loss: 0.027235435322 \n",
      "Epoch:  4 | loss: 0.027235435322 \n",
      "Epoch:  5 | loss: 0.027235431597 \n",
      "Epoch:  6 | loss: 0.027235427871 \n",
      "Epoch:  7 | loss: 0.027235418558 \n",
      "Epoch:  8 | loss: 0.027235422283 \n",
      "Epoch:  9 | loss: 0.027235424146 \n",
      "Epoch:  10 | loss: 0.027235420421 \n",
      "Epoch:  11 | loss: 0.027235422283 \n",
      "Epoch:  12 | loss: 0.027235418558 \n",
      "Epoch:  13 | loss: 0.027235420421 \n",
      "Epoch:  14 | loss: 0.027235418558 \n",
      "Epoch:  15 | loss: 0.027235418558 \n",
      "Epoch:  16 | loss: 0.027235412970 \n",
      "Epoch:  17 | loss: 0.027235416695 \n",
      "Epoch:  18 | loss: 0.027235412970 \n",
      "Epoch:  19 | loss: 0.027235414833 \n",
      "Epoch:  20 | loss: 0.027235412970 \n",
      "Epoch:  21 | loss: 0.027235407382 \n",
      "Epoch:  22 | loss: 0.027235401794 \n",
      "Epoch:  23 | loss: 0.027235403657 \n",
      "Epoch:  24 | loss: 0.027235399932 \n",
      "Epoch:  25 | loss: 0.027235399932 \n",
      "Epoch:  26 | loss: 0.027235401794 \n",
      "Epoch:  27 | loss: 0.027235399932 \n",
      "Epoch:  28 | loss: 0.027235399932 \n",
      "Epoch:  29 | loss: 0.027235399932 \n",
      "Epoch:  30 | loss: 0.027235394344 \n",
      "Epoch:  31 | loss: 0.027235396206 \n",
      "Epoch:  32 | loss: 0.027235396206 \n",
      "Epoch:  33 | loss: 0.027235398069 \n",
      "Epoch:  34 | loss: 0.027235396206 \n",
      "Epoch:  35 | loss: 0.027235394344 \n",
      "Epoch:  36 | loss: 0.027235388756 \n",
      "Epoch:  37 | loss: 0.027235390618 \n",
      "Epoch:  38 | loss: 0.027235388756 \n",
      "Epoch:  39 | loss: 0.027235385031 \n",
      "Epoch:  40 | loss: 0.027235388756 \n",
      "Epoch:  41 | loss: 0.027235385031 \n",
      "Epoch:  42 | loss: 0.027235381305 \n",
      "Epoch:  43 | loss: 0.027235381305 \n",
      "Epoch:  44 | loss: 0.027235383168 \n",
      "Epoch:  45 | loss: 0.027235381305 \n",
      "Epoch:  46 | loss: 0.027235379443 \n",
      "Epoch:  47 | loss: 0.027235379443 \n",
      "Epoch:  48 | loss: 0.027235381305 \n",
      "Epoch:  49 | loss: 0.027235373855 \n",
      "Epoch:  50 | loss: 0.027235375717 \n",
      "Epoch:  51 | loss: 0.027235373855 \n",
      "Epoch:  52 | loss: 0.027235375717 \n",
      "Epoch:  53 | loss: 0.027235375717 \n",
      "Epoch:  54 | loss: 0.027235373855 \n",
      "Epoch:  55 | loss: 0.027235375717 \n",
      "Epoch:  56 | loss: 0.027235375717 \n",
      "Epoch:  57 | loss: 0.027235373855 \n",
      "Epoch:  58 | loss: 0.027235371992 \n",
      "Epoch:  59 | loss: 0.027235360816 \n",
      "Epoch:  60 | loss: 0.027235360816 \n",
      "Epoch:  61 | loss: 0.027235362679 \n",
      "Epoch:  62 | loss: 0.027235360816 \n",
      "Epoch:  63 | loss: 0.027235360816 \n",
      "Epoch:  64 | loss: 0.027235360816 \n",
      "Epoch:  65 | loss: 0.027235360816 \n",
      "Epoch:  66 | loss: 0.027235360816 \n",
      "Epoch:  67 | loss: 0.027235360816 \n",
      "Epoch:  68 | loss: 0.027235360816 \n",
      "Epoch:  69 | loss: 0.027235360816 \n",
      "Epoch:  70 | loss: 0.027235360816 \n",
      "Epoch:  71 | loss: 0.027235358953 \n",
      "Epoch:  72 | loss: 0.027235357091 \n",
      "Epoch:  73 | loss: 0.027235355228 \n",
      "Epoch:  74 | loss: 0.027235347778 \n",
      "Epoch:  75 | loss: 0.027235351503 \n",
      "Epoch:  76 | loss: 0.027235347778 \n",
      "Epoch:  77 | loss: 0.027235351503 \n",
      "Epoch:  78 | loss: 0.027235347778 \n",
      "Epoch:  79 | loss: 0.027235342190 \n",
      "Epoch:  80 | loss: 0.027235342190 \n",
      "Epoch:  81 | loss: 0.027235334739 \n",
      "Epoch:  82 | loss: 0.027235334739 \n",
      "Epoch:  83 | loss: 0.027235332876 \n",
      "Epoch:  84 | loss: 0.027235327289 \n",
      "Epoch:  85 | loss: 0.027235329151 \n",
      "Epoch:  86 | loss: 0.027235325426 \n",
      "Epoch:  87 | loss: 0.027235323563 \n",
      "Epoch:  88 | loss: 0.027235323563 \n",
      "Epoch:  89 | loss: 0.027235323563 \n",
      "Epoch:  90 | loss: 0.027235323563 \n",
      "Epoch:  91 | loss: 0.027235321701 \n",
      "Epoch:  92 | loss: 0.027235319838 \n",
      "Epoch:  93 | loss: 0.027235317975 \n",
      "Epoch:  94 | loss: 0.027235317975 \n",
      "Epoch:  95 | loss: 0.027235319838 \n",
      "Epoch:  96 | loss: 0.027235317975 \n",
      "Epoch:  97 | loss: 0.027235314250 \n",
      "Epoch:  98 | loss: 0.027235314250 \n",
      "Epoch:  99 | loss: 0.027235316113 \n",
      "Epoch:  100 | loss: 0.027235304937 \n",
      "Epoch:  101 | loss: 0.027235306799 \n",
      "Epoch:  102 | loss: 0.027235297486 \n",
      "Epoch:  103 | loss: 0.027235282585 \n",
      "Epoch:  104 | loss: 0.027235209942 \n",
      "Epoch:  105 | loss: 0.027235174552 \n",
      "Epoch:  106 | loss: 0.027235152200 \n",
      "Epoch:  107 | loss: 0.027235148475 \n",
      "Epoch:  108 | loss: 0.027235150337 \n",
      "Epoch:  109 | loss: 0.027235135436 \n",
      "Epoch:  110 | loss: 0.027235120535 \n",
      "Epoch:  111 | loss: 0.027235059068 \n",
      "Epoch:  112 | loss: 0.027235059068 \n",
      "Epoch:  113 | loss: 0.027234988287 \n",
      "Epoch:  114 | loss: 0.027234926820 \n",
      "Epoch:  115 | loss: 0.027234889567 \n",
      "Epoch:  116 | loss: 0.027234883979 \n",
      "Epoch:  117 | loss: 0.027234837413 \n",
      "Epoch:  118 | loss: 0.027234813198 \n",
      "Epoch:  119 | loss: 0.027234815061 \n",
      "Epoch:  120 | loss: 0.027234798297 \n",
      "Epoch:  121 | loss: 0.027234794572 \n",
      "Epoch:  122 | loss: 0.027234755456 \n",
      "Epoch:  123 | loss: 0.027234727517 \n",
      "Epoch:  124 | loss: 0.027234707028 \n",
      "Epoch:  125 | loss: 0.027234699577 \n",
      "Epoch:  126 | loss: 0.027234680951 \n",
      "Epoch:  127 | loss: 0.027234610170 \n",
      "Epoch:  128 | loss: 0.027234598994 \n",
      "Epoch:  129 | loss: 0.027234544978 \n",
      "Epoch:  130 | loss: 0.027234515175 \n",
      "Epoch:  131 | loss: 0.027234494686 \n",
      "Epoch:  132 | loss: 0.027234451845 \n",
      "Epoch:  133 | loss: 0.027234431356 \n",
      "Epoch:  134 | loss: 0.027234379202 \n",
      "Epoch:  135 | loss: 0.027234379202 \n",
      "Epoch:  136 | loss: 0.027234364301 \n",
      "Epoch:  137 | loss: 0.027234347537 \n",
      "Epoch:  138 | loss: 0.027234314010 \n",
      "Epoch:  139 | loss: 0.027234302834 \n",
      "Epoch:  140 | loss: 0.027234273031 \n",
      "Epoch:  141 | loss: 0.027234265581 \n",
      "Epoch:  142 | loss: 0.027234209701 \n",
      "Epoch:  143 | loss: 0.027234196663 \n",
      "Epoch:  144 | loss: 0.027234192938 \n",
      "Epoch:  145 | loss: 0.027234179899 \n",
      "Epoch:  146 | loss: 0.027234176174 \n",
      "Epoch:  147 | loss: 0.027234157547 \n",
      "Epoch:  148 | loss: 0.027234133333 \n",
      "Epoch:  149 | loss: 0.027234107256 \n",
      "Epoch:  150 | loss: 0.027234110981 \n",
      "Epoch:  151 | loss: 0.027234097943 \n",
      "Epoch:  152 | loss: 0.027234017849 \n",
      "Epoch:  153 | loss: 0.027234012261 \n",
      "Epoch:  154 | loss: 0.027233999223 \n",
      "Epoch:  155 | loss: 0.027233999223 \n",
      "Epoch:  156 | loss: 0.027233995497 \n",
      "Epoch:  157 | loss: 0.027233999223 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  158 | loss: 0.027233995497 \n",
      "Epoch:  159 | loss: 0.027233924717 \n",
      "Epoch:  160 | loss: 0.027233920991 \n",
      "Epoch:  161 | loss: 0.027233907953 \n",
      "Epoch:  162 | loss: 0.027233893052 \n",
      "Epoch:  163 | loss: 0.027233870700 \n",
      "Epoch:  164 | loss: 0.027233863249 \n",
      "Epoch:  165 | loss: 0.027233863249 \n",
      "Epoch:  166 | loss: 0.027233865112 \n",
      "Epoch:  167 | loss: 0.027233814821 \n",
      "Epoch:  168 | loss: 0.027233783156 \n",
      "Epoch:  169 | loss: 0.027233758941 \n",
      "Epoch:  170 | loss: 0.027233729139 \n",
      "Epoch:  171 | loss: 0.027233695611 \n",
      "Epoch:  172 | loss: 0.027233686298 \n",
      "Epoch:  173 | loss: 0.027233673260 \n",
      "Epoch:  174 | loss: 0.027233665809 \n",
      "Epoch:  175 | loss: 0.027233645320 \n",
      "Epoch:  176 | loss: 0.027233626693 \n",
      "Epoch:  177 | loss: 0.027233611792 \n",
      "Epoch:  178 | loss: 0.027233613655 \n",
      "Epoch:  179 | loss: 0.027233608067 \n",
      "Epoch:  180 | loss: 0.027233606204 \n",
      "Epoch:  181 | loss: 0.027233585715 \n",
      "Epoch:  182 | loss: 0.027233585715 \n",
      "Epoch:  183 | loss: 0.027233550325 \n",
      "Epoch:  184 | loss: 0.027233524248 \n",
      "Epoch:  185 | loss: 0.027233457193 \n",
      "Epoch:  186 | loss: 0.027233408764 \n",
      "Epoch:  187 | loss: 0.027233360335 \n",
      "Epoch:  188 | loss: 0.027233354747 \n",
      "Epoch:  189 | loss: 0.027233352885 \n",
      "Epoch:  190 | loss: 0.027233326808 \n",
      "Epoch:  191 | loss: 0.027233302593 \n",
      "Epoch:  192 | loss: 0.027233278379 \n",
      "Epoch:  193 | loss: 0.027233248577 \n",
      "Epoch:  194 | loss: 0.027233226225 \n",
      "Epoch:  195 | loss: 0.027233213186 \n",
      "Epoch:  196 | loss: 0.027233198285 \n",
      "Epoch:  197 | loss: 0.027233166620 \n",
      "Epoch:  198 | loss: 0.027233136818 \n",
      "Epoch:  199 | loss: 0.027233060449 \n",
      "Epoch:  200 | loss: 0.027233060449 \n",
      "Epoch:  201 | loss: 0.027233060449 \n",
      "Epoch:  202 | loss: 0.027233062312 \n",
      "Epoch:  203 | loss: 0.027233060449 \n",
      "Epoch:  204 | loss: 0.027233062312 \n",
      "Epoch:  205 | loss: 0.027233060449 \n",
      "Epoch:  206 | loss: 0.027233058587 \n",
      "Epoch:  207 | loss: 0.027233062312 \n",
      "Epoch:  208 | loss: 0.027233060449 \n",
      "Epoch:  209 | loss: 0.027233058587 \n",
      "Epoch:  210 | loss: 0.027233058587 \n",
      "Epoch:  211 | loss: 0.027233058587 \n",
      "Epoch:  212 | loss: 0.027233056724 \n",
      "Epoch:  213 | loss: 0.027233058587 \n",
      "Epoch:  214 | loss: 0.027233056724 \n",
      "Epoch:  215 | loss: 0.027233056724 \n",
      "Epoch:  216 | loss: 0.027233058587 \n",
      "Epoch:  217 | loss: 0.027233058587 \n",
      "Epoch:  218 | loss: 0.027233058587 \n",
      "Epoch:  219 | loss: 0.027233056724 \n",
      "Epoch:  220 | loss: 0.027233054861 \n",
      "Epoch:  221 | loss: 0.027233054861 \n",
      "Epoch:  222 | loss: 0.027233056724 \n",
      "Epoch:  223 | loss: 0.027233054861 \n",
      "Epoch:  224 | loss: 0.027233056724 \n",
      "Epoch:  225 | loss: 0.027233052999 \n",
      "Epoch:  226 | loss: 0.027233054861 \n",
      "Epoch:  227 | loss: 0.027233052999 \n",
      "Epoch:  228 | loss: 0.027233054861 \n",
      "Epoch:  229 | loss: 0.027233049273 \n",
      "Epoch:  230 | loss: 0.027233051136 \n",
      "Epoch:  231 | loss: 0.027233052999 \n",
      "Epoch:  232 | loss: 0.027233049273 \n",
      "Epoch:  233 | loss: 0.027233049273 \n",
      "Epoch:  234 | loss: 0.027233052999 \n",
      "Epoch:  235 | loss: 0.027233052999 \n",
      "Epoch:  236 | loss: 0.027233049273 \n",
      "Epoch:  237 | loss: 0.027233049273 \n",
      "Epoch:  238 | loss: 0.027233049273 \n",
      "Epoch:  239 | loss: 0.027233047411 \n",
      "Epoch:  240 | loss: 0.027233047411 \n",
      "Epoch:  241 | loss: 0.027233043686 \n",
      "Epoch:  242 | loss: 0.027233047411 \n",
      "Epoch:  243 | loss: 0.027233043686 \n",
      "Epoch:  244 | loss: 0.027233043686 \n",
      "Epoch:  245 | loss: 0.027233041823 \n",
      "Epoch:  246 | loss: 0.027233045548 \n",
      "Epoch:  247 | loss: 0.027233043686 \n",
      "Epoch:  248 | loss: 0.027233039960 \n",
      "Epoch:  249 | loss: 0.027233039960 \n",
      "Epoch:  250 | loss: 0.027233038098 \n",
      "Epoch:  251 | loss: 0.027233038098 \n",
      "Epoch:  252 | loss: 0.027233039960 \n",
      "Epoch:  253 | loss: 0.027233038098 \n",
      "Epoch:  254 | loss: 0.027233038098 \n",
      "Epoch:  255 | loss: 0.027233038098 \n",
      "Epoch:  256 | loss: 0.027233038098 \n",
      "Epoch:  257 | loss: 0.027233038098 \n",
      "Epoch:  258 | loss: 0.027233038098 \n",
      "Epoch:  259 | loss: 0.027233039960 \n",
      "Epoch:  260 | loss: 0.027233038098 \n",
      "Epoch:  261 | loss: 0.027233038098 \n",
      "Epoch:  262 | loss: 0.027233036235 \n",
      "Epoch:  263 | loss: 0.027233036235 \n",
      "Epoch:  264 | loss: 0.027233038098 \n",
      "Epoch:  265 | loss: 0.027233038098 \n",
      "Epoch:  266 | loss: 0.027233036235 \n",
      "Epoch:  267 | loss: 0.027233036235 \n",
      "Epoch:  268 | loss: 0.027233038098 \n",
      "Epoch:  269 | loss: 0.027233036235 \n",
      "Epoch:  270 | loss: 0.027233036235 \n",
      "Epoch:  271 | loss: 0.027233038098 \n",
      "Epoch:  272 | loss: 0.027233034372 \n",
      "Epoch:  273 | loss: 0.027233032510 \n",
      "Epoch:  274 | loss: 0.027233032510 \n",
      "Epoch:  275 | loss: 0.027233034372 \n",
      "Epoch:  276 | loss: 0.027233032510 \n",
      "Epoch:  277 | loss: 0.027233036235 \n",
      "Epoch:  278 | loss: 0.027233036235 \n",
      "Epoch:  279 | loss: 0.027233036235 \n",
      "Epoch:  280 | loss: 0.027233032510 \n",
      "Epoch:  281 | loss: 0.027233030647 \n",
      "Epoch:  282 | loss: 0.027233034372 \n",
      "Epoch:  283 | loss: 0.027233030647 \n",
      "Epoch:  284 | loss: 0.027233028784 \n",
      "Epoch:  285 | loss: 0.027233026922 \n",
      "Epoch:  286 | loss: 0.027233028784 \n",
      "Epoch:  287 | loss: 0.027233028784 \n",
      "Epoch:  288 | loss: 0.027233032510 \n",
      "Epoch:  289 | loss: 0.027233028784 \n",
      "Epoch:  290 | loss: 0.027233026922 \n",
      "Epoch:  291 | loss: 0.027233026922 \n",
      "Epoch:  292 | loss: 0.027233028784 \n",
      "Epoch:  293 | loss: 0.027233028784 \n",
      "Epoch:  294 | loss: 0.027233025059 \n",
      "Epoch:  295 | loss: 0.027233026922 \n",
      "Epoch:  296 | loss: 0.027233025059 \n",
      "Epoch:  297 | loss: 0.027233025059 \n",
      "Epoch:  298 | loss: 0.027233026922 \n",
      "Epoch:  299 | loss: 0.027233023196 \n",
      "time=64.6682608127594\n",
      "best_train_loss=0.027233023196458817\n",
      "test_loss=0.026174375787377357\n",
      "best_epoch=299\n",
      "w=15\n",
      "a_kerr=0.0\n",
      "M=1.0\n",
      "r_c=2.0\n",
      "Epoch:  0 | loss: 0.027233025059 \n",
      "Epoch:  1 | loss: 0.027232326567 \n",
      "Epoch:  2 | loss: 0.027231713757 \n",
      "Epoch:  3 | loss: 0.027231093496 \n",
      "Epoch:  4 | loss: 0.027230489999 \n",
      "Epoch:  5 | loss: 0.027229918167 \n",
      "Epoch:  6 | loss: 0.027229379863 \n",
      "Epoch:  7 | loss: 0.027228837833 \n",
      "Epoch:  8 | loss: 0.027228305116 \n",
      "Epoch:  9 | loss: 0.027227774262 \n",
      "Epoch:  10 | loss: 0.027227288112 \n",
      "Epoch:  11 | loss: 0.027226794511 \n",
      "Epoch:  12 | loss: 0.027226340026 \n",
      "Epoch:  13 | loss: 0.027225853875 \n",
      "Epoch:  14 | loss: 0.027225390077 \n",
      "Epoch:  15 | loss: 0.027224937454 \n",
      "Epoch:  16 | loss: 0.027224525809 \n",
      "Epoch:  17 | loss: 0.027224091813 \n",
      "Epoch:  18 | loss: 0.027223680168 \n",
      "Epoch:  19 | loss: 0.027223253623 \n",
      "Epoch:  20 | loss: 0.027222841978 \n",
      "Epoch:  21 | loss: 0.027222458273 \n",
      "Epoch:  22 | loss: 0.027222050354 \n",
      "Epoch:  23 | loss: 0.027221666649 \n",
      "Epoch:  24 | loss: 0.027221295983 \n",
      "Epoch:  25 | loss: 0.027220936492 \n",
      "Epoch:  26 | loss: 0.027220580727 \n",
      "Epoch:  27 | loss: 0.027220234275 \n",
      "Epoch:  28 | loss: 0.027219872922 \n",
      "Epoch:  29 | loss: 0.027219524607 \n",
      "Epoch:  30 | loss: 0.027219196782 \n",
      "Epoch:  31 | loss: 0.027218881994 \n",
      "Epoch:  32 | loss: 0.027218572795 \n",
      "Epoch:  33 | loss: 0.027218267322 \n",
      "Epoch:  34 | loss: 0.027217967436 \n",
      "Epoch:  35 | loss: 0.027217671275 \n",
      "Epoch:  36 | loss: 0.027217390016 \n",
      "Epoch:  37 | loss: 0.027217114344 \n",
      "Epoch:  38 | loss: 0.027216807008 \n",
      "Epoch:  39 | loss: 0.027216475457 \n",
      "Epoch:  40 | loss: 0.027216145769 \n",
      "Epoch:  41 | loss: 0.027215821669 \n",
      "Epoch:  42 | loss: 0.027215503156 \n",
      "Epoch:  43 | loss: 0.027215190232 \n",
      "Epoch:  44 | loss: 0.027214871719 \n",
      "Epoch:  45 | loss: 0.027214568108 \n",
      "Epoch:  46 | loss: 0.027214260772 \n",
      "Epoch:  47 | loss: 0.027213964611 \n",
      "Epoch:  48 | loss: 0.027213675901 \n",
      "Epoch:  49 | loss: 0.027213379741 \n",
      "Epoch:  50 | loss: 0.027213098481 \n",
      "Epoch:  51 | loss: 0.027212813497 \n",
      "Epoch:  52 | loss: 0.027212530375 \n",
      "Epoch:  53 | loss: 0.027212260291 \n",
      "Epoch:  54 | loss: 0.027211995795 \n",
      "Epoch:  55 | loss: 0.027211725712 \n",
      "Epoch:  56 | loss: 0.027211466804 \n",
      "Epoch:  57 | loss: 0.027211204171 \n",
      "Epoch:  58 | loss: 0.027210952714 \n",
      "Epoch:  59 | loss: 0.027210703120 \n",
      "Epoch:  60 | loss: 0.027210451663 \n",
      "Epoch:  61 | loss: 0.027210207656 \n",
      "Epoch:  62 | loss: 0.027209971100 \n",
      "Epoch:  63 | loss: 0.027209734544 \n",
      "Epoch:  64 | loss: 0.027209492400 \n",
      "Epoch:  65 | loss: 0.027209263295 \n",
      "Epoch:  66 | loss: 0.027209026739 \n",
      "Epoch:  67 | loss: 0.027208803222 \n",
      "Epoch:  68 | loss: 0.027208574116 \n",
      "Epoch:  69 | loss: 0.027208354324 \n",
      "Epoch:  70 | loss: 0.027208136395 \n",
      "Epoch:  71 | loss: 0.027207916602 \n",
      "Epoch:  72 | loss: 0.027207702398 \n",
      "Epoch:  73 | loss: 0.027207491919 \n",
      "Epoch:  74 | loss: 0.027207275853 \n",
      "Epoch:  75 | loss: 0.027207067236 \n",
      "Epoch:  76 | loss: 0.027206864208 \n",
      "Epoch:  77 | loss: 0.027206668630 \n",
      "Epoch:  78 | loss: 0.027206469327 \n",
      "Epoch:  79 | loss: 0.027206266299 \n",
      "Epoch:  80 | loss: 0.027206070721 \n",
      "Epoch:  81 | loss: 0.027205871418 \n",
      "Epoch:  82 | loss: 0.027205673978 \n",
      "Epoch:  83 | loss: 0.027205478400 \n",
      "Epoch:  84 | loss: 0.027205284685 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  85 | loss: 0.027205090970 \n",
      "Epoch:  86 | loss: 0.027204900980 \n",
      "Epoch:  87 | loss: 0.027204714715 \n",
      "Epoch:  88 | loss: 0.027204528451 \n",
      "Epoch:  89 | loss: 0.027204345912 \n",
      "Epoch:  90 | loss: 0.027204180136 \n",
      "Epoch:  91 | loss: 0.027203999460 \n",
      "Epoch:  92 | loss: 0.027203921229 \n",
      "Epoch:  93 | loss: 0.027203744277 \n",
      "Epoch:  94 | loss: 0.027203565463 \n",
      "Epoch:  95 | loss: 0.027203390375 \n",
      "Epoch:  96 | loss: 0.027203226462 \n",
      "Epoch:  97 | loss: 0.027203163132 \n",
      "Epoch:  98 | loss: 0.027202993631 \n",
      "Epoch:  99 | loss: 0.027202824131 \n",
      "Epoch:  100 | loss: 0.027202822268 \n",
      "Epoch:  101 | loss: 0.027202822268 \n",
      "Epoch:  102 | loss: 0.027202824131 \n",
      "Epoch:  103 | loss: 0.027202812955 \n",
      "Epoch:  104 | loss: 0.027202812955 \n",
      "Epoch:  105 | loss: 0.027202814817 \n",
      "Epoch:  106 | loss: 0.027202805504 \n",
      "Epoch:  107 | loss: 0.027202809229 \n",
      "Epoch:  108 | loss: 0.027202805504 \n",
      "Epoch:  109 | loss: 0.027202803642 \n",
      "Epoch:  110 | loss: 0.027202803642 \n",
      "Epoch:  111 | loss: 0.027202801779 \n",
      "Epoch:  112 | loss: 0.027202801779 \n",
      "Epoch:  113 | loss: 0.027202803642 \n",
      "Epoch:  114 | loss: 0.027202803642 \n",
      "Epoch:  115 | loss: 0.027202796191 \n",
      "Epoch:  116 | loss: 0.027202798054 \n",
      "Epoch:  117 | loss: 0.027202799916 \n",
      "Epoch:  118 | loss: 0.027202788740 \n",
      "Epoch:  119 | loss: 0.027202792466 \n",
      "Epoch:  120 | loss: 0.027202788740 \n",
      "Epoch:  121 | loss: 0.027202788740 \n",
      "Epoch:  122 | loss: 0.027202785015 \n",
      "Epoch:  123 | loss: 0.027202785015 \n",
      "Epoch:  124 | loss: 0.027202785015 \n",
      "Epoch:  125 | loss: 0.027202786878 \n",
      "Epoch:  126 | loss: 0.027202783152 \n",
      "Epoch:  127 | loss: 0.027202783152 \n",
      "Epoch:  128 | loss: 0.027202783152 \n",
      "Epoch:  129 | loss: 0.027202785015 \n",
      "Epoch:  130 | loss: 0.027202779427 \n",
      "Epoch:  131 | loss: 0.027202775702 \n",
      "Epoch:  132 | loss: 0.027202779427 \n",
      "Epoch:  133 | loss: 0.027202779427 \n",
      "Epoch:  134 | loss: 0.027202779427 \n",
      "Epoch:  135 | loss: 0.027202775702 \n",
      "Epoch:  136 | loss: 0.027202773839 \n",
      "Epoch:  137 | loss: 0.027202773839 \n",
      "Epoch:  138 | loss: 0.027202773839 \n",
      "Epoch:  139 | loss: 0.027202773839 \n",
      "Epoch:  140 | loss: 0.027202771977 \n",
      "Epoch:  141 | loss: 0.027202773839 \n",
      "Epoch:  142 | loss: 0.027202775702 \n",
      "Epoch:  143 | loss: 0.027202771977 \n",
      "Epoch:  144 | loss: 0.027202768251 \n",
      "Epoch:  145 | loss: 0.027202766389 \n",
      "Epoch:  146 | loss: 0.027202766389 \n",
      "Epoch:  147 | loss: 0.027202758938 \n",
      "Epoch:  148 | loss: 0.027202760801 \n",
      "Epoch:  149 | loss: 0.027202758938 \n",
      "Epoch:  150 | loss: 0.027202762663 \n",
      "Epoch:  151 | loss: 0.027202758938 \n",
      "Epoch:  152 | loss: 0.027202758938 \n",
      "Epoch:  153 | loss: 0.027202753350 \n",
      "Epoch:  154 | loss: 0.027202751487 \n",
      "Epoch:  155 | loss: 0.027202751487 \n",
      "Epoch:  156 | loss: 0.027202747762 \n",
      "Epoch:  157 | loss: 0.027202747762 \n",
      "Epoch:  158 | loss: 0.027202747762 \n",
      "Epoch:  159 | loss: 0.027202747762 \n",
      "Epoch:  160 | loss: 0.027202749625 \n",
      "Epoch:  161 | loss: 0.027202747762 \n",
      "Epoch:  162 | loss: 0.027202745900 \n",
      "Epoch:  163 | loss: 0.027202744037 \n",
      "Epoch:  164 | loss: 0.027202745900 \n",
      "Epoch:  165 | loss: 0.027202740312 \n",
      "Epoch:  166 | loss: 0.027202734724 \n",
      "Epoch:  167 | loss: 0.027202734724 \n",
      "Epoch:  168 | loss: 0.027202730998 \n",
      "Epoch:  169 | loss: 0.027202734724 \n",
      "Epoch:  170 | loss: 0.027202732861 \n",
      "Epoch:  171 | loss: 0.027202734724 \n",
      "Epoch:  172 | loss: 0.027202729136 \n",
      "Epoch:  173 | loss: 0.027202729136 \n",
      "Epoch:  174 | loss: 0.027202729136 \n",
      "Epoch:  175 | loss: 0.027202727273 \n",
      "Epoch:  176 | loss: 0.027202729136 \n",
      "Epoch:  177 | loss: 0.027202725410 \n",
      "Epoch:  178 | loss: 0.027202725410 \n",
      "Epoch:  179 | loss: 0.027202725410 \n",
      "Epoch:  180 | loss: 0.027202723548 \n",
      "Epoch:  181 | loss: 0.027202723548 \n",
      "Epoch:  182 | loss: 0.027202725410 \n",
      "Epoch:  183 | loss: 0.027202723548 \n",
      "Epoch:  184 | loss: 0.027202721685 \n",
      "Epoch:  185 | loss: 0.027202721685 \n",
      "Epoch:  186 | loss: 0.027202723548 \n",
      "Epoch:  187 | loss: 0.027202723548 \n",
      "Epoch:  188 | loss: 0.027202717960 \n",
      "Epoch:  189 | loss: 0.027202719823 \n",
      "Epoch:  190 | loss: 0.027202717960 \n",
      "Epoch:  191 | loss: 0.027202719823 \n",
      "Epoch:  192 | loss: 0.027202717960 \n",
      "Epoch:  193 | loss: 0.027202717960 \n",
      "Epoch:  194 | loss: 0.027202717960 \n",
      "Epoch:  195 | loss: 0.027202714235 \n",
      "Epoch:  196 | loss: 0.027202717960 \n",
      "Epoch:  197 | loss: 0.027202710509 \n",
      "Epoch:  198 | loss: 0.027202712372 \n",
      "Epoch:  199 | loss: 0.027202710509 \n",
      "Epoch:  200 | loss: 0.027202690020 \n",
      "Epoch:  201 | loss: 0.027202660218 \n",
      "Epoch:  202 | loss: 0.027202649042 \n",
      "Epoch:  203 | loss: 0.027202630416 \n",
      "Epoch:  204 | loss: 0.027202615514 \n",
      "Epoch:  205 | loss: 0.027202578261 \n",
      "Epoch:  206 | loss: 0.027202581987 \n",
      "Epoch:  207 | loss: 0.027202561498 \n",
      "Epoch:  208 | loss: 0.027202498168 \n",
      "Epoch:  209 | loss: 0.027202440426 \n",
      "Epoch:  210 | loss: 0.027202412486 \n",
      "Epoch:  211 | loss: 0.027202408761 \n",
      "Epoch:  212 | loss: 0.027202408761 \n",
      "Epoch:  213 | loss: 0.027202388272 \n",
      "Epoch:  214 | loss: 0.027202375233 \n",
      "Epoch:  215 | loss: 0.027202373371 \n",
      "Epoch:  216 | loss: 0.027202377096 \n",
      "Epoch:  217 | loss: 0.027202351019 \n",
      "Epoch:  218 | loss: 0.027202332392 \n",
      "Epoch:  219 | loss: 0.027202334255 \n",
      "Epoch:  220 | loss: 0.027202324942 \n",
      "Epoch:  221 | loss: 0.027202315629 \n",
      "Epoch:  222 | loss: 0.027202295139 \n",
      "Epoch:  223 | loss: 0.027202295139 \n",
      "Epoch:  224 | loss: 0.027202280238 \n",
      "Epoch:  225 | loss: 0.027202256024 \n",
      "Epoch:  226 | loss: 0.027202239260 \n",
      "Epoch:  227 | loss: 0.027202218771 \n",
      "Epoch:  228 | loss: 0.027202183381 \n",
      "Epoch:  229 | loss: 0.027202151716 \n",
      "Epoch:  230 | loss: 0.027202144265 \n",
      "Epoch:  231 | loss: 0.027202147990 \n",
      "Epoch:  232 | loss: 0.027202138677 \n",
      "Epoch:  233 | loss: 0.027202140540 \n",
      "Epoch:  234 | loss: 0.027202101424 \n",
      "Epoch:  235 | loss: 0.027202086523 \n",
      "Epoch:  236 | loss: 0.027202088386 \n",
      "Epoch:  237 | loss: 0.027202013880 \n",
      "Epoch:  238 | loss: 0.027201993391 \n",
      "Epoch:  239 | loss: 0.027201933786 \n",
      "Epoch:  240 | loss: 0.027201920748 \n",
      "Epoch:  241 | loss: 0.027201911435 \n",
      "Epoch:  242 | loss: 0.027201907709 \n",
      "Epoch:  243 | loss: 0.027201911435 \n",
      "Epoch:  244 | loss: 0.027201898396 \n",
      "Epoch:  245 | loss: 0.027201898396 \n",
      "Epoch:  246 | loss: 0.027201881632 \n",
      "Epoch:  247 | loss: 0.027201879770 \n",
      "Epoch:  248 | loss: 0.027201864868 \n",
      "Epoch:  249 | loss: 0.027201831341 \n",
      "Epoch:  250 | loss: 0.027201818302 \n",
      "Epoch:  251 | loss: 0.027201805264 \n",
      "Epoch:  252 | loss: 0.027201803401 \n",
      "Epoch:  253 | loss: 0.027201766148 \n",
      "Epoch:  254 | loss: 0.027201758698 \n",
      "Epoch:  255 | loss: 0.027201745659 \n",
      "Epoch:  256 | loss: 0.027201727033 \n",
      "Epoch:  257 | loss: 0.027201725170 \n",
      "Epoch:  258 | loss: 0.027201708406 \n",
      "Epoch:  259 | loss: 0.027201708406 \n",
      "Epoch:  260 | loss: 0.027201710269 \n",
      "Epoch:  261 | loss: 0.027201706544 \n",
      "Epoch:  262 | loss: 0.027201669291 \n",
      "Epoch:  263 | loss: 0.027201626450 \n",
      "Epoch:  264 | loss: 0.027201605961 \n",
      "Epoch:  265 | loss: 0.027201592922 \n",
      "Epoch:  266 | loss: 0.027201553807 \n",
      "Epoch:  267 | loss: 0.027201535180 \n",
      "Epoch:  268 | loss: 0.027201501653 \n",
      "Epoch:  269 | loss: 0.027201477438 \n",
      "Epoch:  270 | loss: 0.027201469988 \n",
      "Epoch:  271 | loss: 0.027201466262 \n",
      "Epoch:  272 | loss: 0.027201443911 \n",
      "Epoch:  273 | loss: 0.027201430872 \n",
      "Epoch:  274 | loss: 0.027201427147 \n",
      "Epoch:  275 | loss: 0.027201388031 \n",
      "Epoch:  276 | loss: 0.027201386169 \n",
      "Epoch:  277 | loss: 0.027201348916 \n",
      "Epoch:  278 | loss: 0.027201304212 \n",
      "Epoch:  279 | loss: 0.027201283723 \n",
      "Epoch:  280 | loss: 0.027201276273 \n",
      "Epoch:  281 | loss: 0.027201246470 \n",
      "Epoch:  282 | loss: 0.027201207355 \n",
      "Epoch:  283 | loss: 0.027201199904 \n",
      "Epoch:  284 | loss: 0.027201198041 \n",
      "Epoch:  285 | loss: 0.027201194316 \n",
      "Epoch:  286 | loss: 0.027201183140 \n",
      "Epoch:  287 | loss: 0.027201144025 \n",
      "Epoch:  288 | loss: 0.027201142162 \n",
      "Epoch:  289 | loss: 0.027201123536 \n",
      "Epoch:  290 | loss: 0.027201125398 \n",
      "Epoch:  291 | loss: 0.027201106772 \n",
      "Epoch:  292 | loss: 0.027201067656 \n",
      "Epoch:  293 | loss: 0.027201062068 \n",
      "Epoch:  294 | loss: 0.027201054618 \n",
      "Epoch:  295 | loss: 0.027201039717 \n",
      "Epoch:  296 | loss: 0.027201028541 \n",
      "Epoch:  297 | loss: 0.027201000601 \n",
      "Epoch:  298 | loss: 0.027200991288 \n",
      "Epoch:  299 | loss: 0.027200939134 \n",
      "time=193.62942171096802\n",
      "best_train_loss=0.027200939133763313\n",
      "test_loss=0.026150710880756378\n",
      "best_epoch=299\n",
      "w=20\n",
      "a_kerr=0.0\n",
      "M=1.0\n",
      "r_c=2.0\n",
      "Epoch:  0 | loss: 0.027200935408 \n",
      "Epoch:  1 | loss: 0.027200939134 \n",
      "Epoch:  2 | loss: 0.027200935408 \n",
      "Epoch:  3 | loss: 0.027200926095 \n",
      "Epoch:  4 | loss: 0.027200927958 \n",
      "Epoch:  5 | loss: 0.027200916782 \n",
      "Epoch:  6 | loss: 0.027200916782 \n",
      "Epoch:  7 | loss: 0.027200914919 \n",
      "Epoch:  8 | loss: 0.027200916782 \n",
      "Epoch:  9 | loss: 0.027200881392 \n",
      "Epoch:  10 | loss: 0.027200855315 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11 | loss: 0.027200851589 \n",
      "Epoch:  12 | loss: 0.027200840414 \n",
      "Epoch:  13 | loss: 0.027200840414 \n",
      "Epoch:  14 | loss: 0.027200840414 \n",
      "Epoch:  15 | loss: 0.027200818062 \n",
      "Epoch:  16 | loss: 0.027200805023 \n",
      "Epoch:  17 | loss: 0.027200803161 \n",
      "Epoch:  18 | loss: 0.027200780809 \n",
      "Epoch:  19 | loss: 0.027200782672 \n",
      "Epoch:  20 | loss: 0.027200730518 \n",
      "Epoch:  21 | loss: 0.027200726792 \n",
      "Epoch:  22 | loss: 0.027200726792 \n",
      "Epoch:  23 | loss: 0.027200728655 \n",
      "Epoch:  24 | loss: 0.027200685814 \n",
      "Epoch:  25 | loss: 0.027200687677 \n",
      "Epoch:  26 | loss: 0.027200674638 \n",
      "Epoch:  27 | loss: 0.027200669050 \n",
      "Epoch:  28 | loss: 0.027200669050 \n",
      "Epoch:  29 | loss: 0.027200670913 \n",
      "Epoch:  30 | loss: 0.027200652286 \n",
      "Epoch:  31 | loss: 0.027200652286 \n",
      "Epoch:  32 | loss: 0.027200646698 \n",
      "Epoch:  33 | loss: 0.027200646698 \n",
      "Epoch:  34 | loss: 0.027200639248 \n",
      "Epoch:  35 | loss: 0.027200631797 \n",
      "Epoch:  36 | loss: 0.027200613171 \n",
      "Epoch:  37 | loss: 0.027200596407 \n",
      "Epoch:  38 | loss: 0.027200583369 \n",
      "Epoch:  39 | loss: 0.027200579643 \n",
      "Epoch:  40 | loss: 0.027200577781 \n",
      "Epoch:  41 | loss: 0.027200572193 \n",
      "Epoch:  42 | loss: 0.027200568467 \n",
      "Epoch:  43 | loss: 0.027200568467 \n",
      "Epoch:  44 | loss: 0.027200559154 \n",
      "Epoch:  45 | loss: 0.027200540528 \n",
      "Epoch:  46 | loss: 0.027200534940 \n",
      "Epoch:  47 | loss: 0.027200531214 \n",
      "Epoch:  48 | loss: 0.027200497687 \n",
      "Epoch:  49 | loss: 0.027200467885 \n",
      "Epoch:  50 | loss: 0.027200449258 \n",
      "Epoch:  51 | loss: 0.027200441808 \n",
      "Epoch:  52 | loss: 0.027200408280 \n",
      "Epoch:  53 | loss: 0.027200402692 \n",
      "Epoch:  54 | loss: 0.027200387791 \n",
      "Epoch:  55 | loss: 0.027200387791 \n",
      "Epoch:  56 | loss: 0.027200369164 \n",
      "Epoch:  57 | loss: 0.027200344950 \n",
      "Epoch:  58 | loss: 0.027200337499 \n",
      "Epoch:  59 | loss: 0.027200339362 \n",
      "Epoch:  60 | loss: 0.027200331911 \n",
      "Epoch:  61 | loss: 0.027200307697 \n",
      "Epoch:  62 | loss: 0.027200302109 \n",
      "Epoch:  63 | loss: 0.027200300246 \n",
      "Epoch:  64 | loss: 0.027200270444 \n",
      "Epoch:  65 | loss: 0.027200274169 \n",
      "Epoch:  66 | loss: 0.027200266719 \n",
      "Epoch:  67 | loss: 0.027200262994 \n",
      "Epoch:  68 | loss: 0.027200255543 \n",
      "Epoch:  69 | loss: 0.027200257406 \n",
      "Epoch:  70 | loss: 0.027200238779 \n",
      "Epoch:  71 | loss: 0.027200225741 \n",
      "Epoch:  72 | loss: 0.027200218290 \n",
      "Epoch:  73 | loss: 0.027200216427 \n",
      "Epoch:  74 | loss: 0.027200201526 \n",
      "Epoch:  75 | loss: 0.027200199664 \n",
      "Epoch:  76 | loss: 0.027200201526 \n",
      "Epoch:  77 | loss: 0.027200205252 \n",
      "Epoch:  78 | loss: 0.027200197801 \n",
      "Epoch:  79 | loss: 0.027200182900 \n",
      "Epoch:  80 | loss: 0.027200173587 \n",
      "Epoch:  81 | loss: 0.027200162411 \n",
      "Epoch:  82 | loss: 0.027200160548 \n",
      "Epoch:  83 | loss: 0.027200164273 \n",
      "Epoch:  84 | loss: 0.027200149372 \n",
      "Epoch:  85 | loss: 0.027200143784 \n",
      "Epoch:  86 | loss: 0.027200138196 \n",
      "Epoch:  87 | loss: 0.027200134471 \n",
      "Epoch:  88 | loss: 0.027200130746 \n",
      "Epoch:  89 | loss: 0.027200130746 \n",
      "Epoch:  90 | loss: 0.027200123295 \n",
      "Epoch:  91 | loss: 0.027200104669 \n",
      "Epoch:  92 | loss: 0.027200097218 \n",
      "Epoch:  93 | loss: 0.027200100943 \n",
      "Epoch:  94 | loss: 0.027200091630 \n",
      "Epoch:  95 | loss: 0.027200086042 \n",
      "Epoch:  96 | loss: 0.027200071141 \n",
      "Epoch:  97 | loss: 0.027200052515 \n",
      "Epoch:  98 | loss: 0.027200054377 \n",
      "Epoch:  99 | loss: 0.027200039476 \n",
      "Epoch:  100 | loss: 0.027199869975 \n",
      "Epoch:  101 | loss: 0.027199707925 \n",
      "Epoch:  102 | loss: 0.027199540287 \n",
      "Epoch:  103 | loss: 0.027199383825 \n",
      "Epoch:  104 | loss: 0.027199225500 \n",
      "Epoch:  105 | loss: 0.027199070901 \n",
      "Epoch:  106 | loss: 0.027198914438 \n",
      "Epoch:  107 | loss: 0.027198754251 \n",
      "Epoch:  108 | loss: 0.027198597789 \n",
      "Epoch:  109 | loss: 0.027198443189 \n",
      "Epoch:  110 | loss: 0.027198292315 \n",
      "Epoch:  111 | loss: 0.027198137715 \n",
      "Epoch:  112 | loss: 0.027197986841 \n",
      "Epoch:  113 | loss: 0.027197876945 \n",
      "Epoch:  114 | loss: 0.027197726071 \n",
      "Epoch:  115 | loss: 0.027197571471 \n",
      "Epoch:  116 | loss: 0.027197420597 \n",
      "Epoch:  117 | loss: 0.027197271585 \n",
      "Epoch:  118 | loss: 0.027197115123 \n",
      "Epoch:  119 | loss: 0.027196958661 \n",
      "Epoch:  120 | loss: 0.027196804062 \n",
      "Epoch:  121 | loss: 0.027196656913 \n",
      "Epoch:  122 | loss: 0.027196506038 \n",
      "Epoch:  123 | loss: 0.027196358889 \n",
      "Epoch:  124 | loss: 0.027196208015 \n",
      "Epoch:  125 | loss: 0.027196183801 \n",
      "Epoch:  126 | loss: 0.027196032926 \n",
      "Epoch:  127 | loss: 0.027195882052 \n",
      "Epoch:  128 | loss: 0.027195729315 \n",
      "Epoch:  129 | loss: 0.027195587754 \n",
      "Epoch:  130 | loss: 0.027195438743 \n",
      "Epoch:  131 | loss: 0.027195289731 \n",
      "Epoch:  132 | loss: 0.027195142582 \n",
      "Epoch:  133 | loss: 0.027195001021 \n",
      "Epoch:  134 | loss: 0.027194853872 \n",
      "Epoch:  135 | loss: 0.027194706723 \n",
      "Epoch:  136 | loss: 0.027194557711 \n",
      "Epoch:  137 | loss: 0.027194412425 \n",
      "Epoch:  138 | loss: 0.027194269001 \n",
      "Epoch:  139 | loss: 0.027194129303 \n",
      "Epoch:  140 | loss: 0.027193985879 \n",
      "Epoch:  141 | loss: 0.027193844318 \n",
      "Epoch:  142 | loss: 0.027193702757 \n",
      "Epoch:  143 | loss: 0.027193607762 \n",
      "Epoch:  144 | loss: 0.027193468064 \n",
      "Epoch:  145 | loss: 0.027193324640 \n",
      "Epoch:  146 | loss: 0.027193183079 \n",
      "Epoch:  147 | loss: 0.027193047106 \n",
      "Epoch:  148 | loss: 0.027192911133 \n",
      "Epoch:  149 | loss: 0.027192777023 \n",
      "Epoch:  150 | loss: 0.027192646638 \n",
      "Epoch:  151 | loss: 0.027192514390 \n",
      "Epoch:  152 | loss: 0.027192400768 \n",
      "Epoch:  153 | loss: 0.027192264795 \n",
      "Epoch:  154 | loss: 0.027192125097 \n",
      "Epoch:  155 | loss: 0.027192080393 \n",
      "Epoch:  156 | loss: 0.027191944420 \n",
      "Epoch:  157 | loss: 0.027191806585 \n",
      "Epoch:  158 | loss: 0.027191678062 \n",
      "Epoch:  159 | loss: 0.027191538364 \n",
      "Epoch:  160 | loss: 0.027191409841 \n",
      "Epoch:  161 | loss: 0.027191326022 \n",
      "Epoch:  162 | loss: 0.027191190049 \n",
      "Epoch:  163 | loss: 0.027191136032 \n",
      "Epoch:  164 | loss: 0.027191005647 \n",
      "Epoch:  165 | loss: 0.027190871537 \n",
      "Epoch:  166 | loss: 0.027190735564 \n",
      "Epoch:  167 | loss: 0.027190625668 \n",
      "Epoch:  168 | loss: 0.027190491557 \n",
      "Epoch:  169 | loss: 0.027190456167 \n",
      "Epoch:  170 | loss: 0.027190327644 \n",
      "Epoch:  171 | loss: 0.027190195397 \n",
      "Epoch:  172 | loss: 0.027190059423 \n",
      "Epoch:  173 | loss: 0.027189932764 \n",
      "Epoch:  174 | loss: 0.027189798653 \n",
      "Epoch:  175 | loss: 0.027189673856 \n",
      "Epoch:  176 | loss: 0.027189539745 \n",
      "Epoch:  177 | loss: 0.027189411223 \n",
      "Epoch:  178 | loss: 0.027189290151 \n",
      "Epoch:  179 | loss: 0.027189157903 \n",
      "Epoch:  180 | loss: 0.027189025655 \n",
      "Epoch:  181 | loss: 0.027188895270 \n",
      "Epoch:  182 | loss: 0.027188764885 \n",
      "Epoch:  183 | loss: 0.027188640088 \n",
      "Epoch:  184 | loss: 0.027188513428 \n",
      "Epoch:  185 | loss: 0.027188388631 \n",
      "Epoch:  186 | loss: 0.027188260108 \n",
      "Epoch:  187 | loss: 0.027188131586 \n",
      "Epoch:  188 | loss: 0.027188003063 \n",
      "Epoch:  189 | loss: 0.027187893167 \n",
      "Epoch:  190 | loss: 0.027187766507 \n",
      "Epoch:  191 | loss: 0.027187639847 \n",
      "Epoch:  192 | loss: 0.027187511325 \n",
      "Epoch:  193 | loss: 0.027187386528 \n",
      "Epoch:  194 | loss: 0.027187360451 \n",
      "Epoch:  195 | loss: 0.027187235653 \n",
      "Epoch:  196 | loss: 0.027187228203 \n",
      "Epoch:  197 | loss: 0.027187149972 \n",
      "Epoch:  198 | loss: 0.027187058702 \n",
      "Epoch:  199 | loss: 0.027186933905 \n",
      "Epoch:  200 | loss: 0.027186935768 \n",
      "Epoch:  201 | loss: 0.027186932042 \n",
      "Epoch:  202 | loss: 0.027186930180 \n",
      "Epoch:  203 | loss: 0.027186926454 \n",
      "Epoch:  204 | loss: 0.027186926454 \n",
      "Epoch:  205 | loss: 0.027186924592 \n",
      "Epoch:  206 | loss: 0.027186924592 \n",
      "Epoch:  207 | loss: 0.027186924592 \n",
      "Epoch:  208 | loss: 0.027186922729 \n",
      "Epoch:  209 | loss: 0.027186922729 \n",
      "Epoch:  210 | loss: 0.027186922729 \n",
      "Epoch:  211 | loss: 0.027186922729 \n",
      "Epoch:  212 | loss: 0.027186922729 \n",
      "Epoch:  213 | loss: 0.027186922729 \n",
      "Epoch:  214 | loss: 0.027186915278 \n",
      "Epoch:  215 | loss: 0.027186915278 \n",
      "Epoch:  216 | loss: 0.027186915278 \n",
      "Epoch:  217 | loss: 0.027186915278 \n",
      "Epoch:  218 | loss: 0.027186917141 \n",
      "Epoch:  219 | loss: 0.027186915278 \n",
      "Epoch:  220 | loss: 0.027186913416 \n",
      "Epoch:  221 | loss: 0.027186915278 \n",
      "Epoch:  222 | loss: 0.027186913416 \n",
      "Epoch:  223 | loss: 0.027186917141 \n",
      "Epoch:  224 | loss: 0.027186905965 \n",
      "Epoch:  225 | loss: 0.027186902240 \n",
      "Epoch:  226 | loss: 0.027186904103 \n",
      "Epoch:  227 | loss: 0.027186902240 \n",
      "Epoch:  228 | loss: 0.027186894789 \n",
      "Epoch:  229 | loss: 0.027186896652 \n",
      "Epoch:  230 | loss: 0.027186896652 \n",
      "Epoch:  231 | loss: 0.027186896652 \n",
      "Epoch:  232 | loss: 0.027186898515 \n",
      "Epoch:  233 | loss: 0.027186896652 \n",
      "Epoch:  234 | loss: 0.027186896652 \n",
      "Epoch:  235 | loss: 0.027186892927 \n",
      "Epoch:  236 | loss: 0.027186891064 \n",
      "Epoch:  237 | loss: 0.027186892927 \n",
      "Epoch:  238 | loss: 0.027186892927 \n",
      "Epoch:  239 | loss: 0.027186891064 \n",
      "Epoch:  240 | loss: 0.027186892927 \n",
      "Epoch:  241 | loss: 0.027186891064 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  242 | loss: 0.027186891064 \n",
      "Epoch:  243 | loss: 0.027186889201 \n",
      "Epoch:  244 | loss: 0.027186885476 \n",
      "Epoch:  245 | loss: 0.027186885476 \n",
      "Epoch:  246 | loss: 0.027186889201 \n",
      "Epoch:  247 | loss: 0.027186887339 \n",
      "Epoch:  248 | loss: 0.027186887339 \n",
      "Epoch:  249 | loss: 0.027186885476 \n",
      "Epoch:  250 | loss: 0.027186885476 \n",
      "Epoch:  251 | loss: 0.027186885476 \n",
      "Epoch:  252 | loss: 0.027186879888 \n",
      "Epoch:  253 | loss: 0.027186881751 \n",
      "Epoch:  254 | loss: 0.027186883613 \n",
      "Epoch:  255 | loss: 0.027186870575 \n",
      "Epoch:  256 | loss: 0.027186868712 \n",
      "Epoch:  257 | loss: 0.027186872438 \n",
      "Epoch:  258 | loss: 0.027186868712 \n",
      "Epoch:  259 | loss: 0.027186870575 \n",
      "Epoch:  260 | loss: 0.027186866850 \n",
      "Epoch:  261 | loss: 0.027186864987 \n",
      "Epoch:  262 | loss: 0.027186868712 \n",
      "Epoch:  263 | loss: 0.027186864987 \n",
      "Epoch:  264 | loss: 0.027186866850 \n",
      "Epoch:  265 | loss: 0.027186864987 \n",
      "Epoch:  266 | loss: 0.027186864987 \n",
      "Epoch:  267 | loss: 0.027186863124 \n",
      "Epoch:  268 | loss: 0.027186864987 \n",
      "Epoch:  269 | loss: 0.027186864987 \n",
      "Epoch:  270 | loss: 0.027186863124 \n",
      "Epoch:  271 | loss: 0.027186857536 \n",
      "Epoch:  272 | loss: 0.027186863124 \n",
      "Epoch:  273 | loss: 0.027186855674 \n",
      "Epoch:  274 | loss: 0.027186857536 \n",
      "Epoch:  275 | loss: 0.027186853811 \n",
      "Epoch:  276 | loss: 0.027186855674 \n",
      "Epoch:  277 | loss: 0.027186855674 \n",
      "Epoch:  278 | loss: 0.027186853811 \n",
      "Epoch:  279 | loss: 0.027186851948 \n",
      "Epoch:  280 | loss: 0.027186853811 \n",
      "Epoch:  281 | loss: 0.027186851948 \n",
      "Epoch:  282 | loss: 0.027186851948 \n",
      "Epoch:  283 | loss: 0.027186844498 \n",
      "Epoch:  284 | loss: 0.027186844498 \n",
      "Epoch:  285 | loss: 0.027186846361 \n",
      "Epoch:  286 | loss: 0.027186842635 \n",
      "Epoch:  287 | loss: 0.027186838910 \n",
      "Epoch:  288 | loss: 0.027186838910 \n",
      "Epoch:  289 | loss: 0.027186837047 \n",
      "Epoch:  290 | loss: 0.027186838910 \n",
      "Epoch:  291 | loss: 0.027186835185 \n",
      "Epoch:  292 | loss: 0.027186835185 \n",
      "Epoch:  293 | loss: 0.027186835185 \n",
      "Epoch:  294 | loss: 0.027186835185 \n",
      "Epoch:  295 | loss: 0.027186835185 \n",
      "Epoch:  296 | loss: 0.027186835185 \n",
      "Epoch:  297 | loss: 0.027186835185 \n",
      "Epoch:  298 | loss: 0.027186833322 \n",
      "Epoch:  299 | loss: 0.027186835185 \n",
      "time=176.91334199905396\n",
      "best_train_loss=0.027186833322048187\n",
      "test_loss=0.026139870285987854\n",
      "best_epoch=298\n",
      "w=25\n",
      "a_kerr=0.0\n",
      "M=1.0\n",
      "r_c=2.0\n",
      "Epoch:  0 | loss: 0.027186833322 \n",
      "Epoch:  1 | loss: 0.027186833322 \n",
      "Epoch:  2 | loss: 0.027186831459 \n",
      "Epoch:  3 | loss: 0.027186831459 \n",
      "Epoch:  4 | loss: 0.027186831459 \n",
      "Epoch:  5 | loss: 0.027186829597 \n",
      "Epoch:  6 | loss: 0.027186827734 \n",
      "Epoch:  7 | loss: 0.027186825871 \n",
      "Epoch:  8 | loss: 0.027186824009 \n",
      "Epoch:  9 | loss: 0.027186824009 \n",
      "Epoch:  10 | loss: 0.027186825871 \n",
      "Epoch:  11 | loss: 0.027186822146 \n",
      "Epoch:  12 | loss: 0.027186822146 \n",
      "Epoch:  13 | loss: 0.027186822146 \n",
      "Epoch:  14 | loss: 0.027186824009 \n",
      "Epoch:  15 | loss: 0.027186818421 \n",
      "Epoch:  16 | loss: 0.027186818421 \n",
      "Epoch:  17 | loss: 0.027186818421 \n",
      "Epoch:  18 | loss: 0.027186816558 \n",
      "Epoch:  19 | loss: 0.027186816558 \n",
      "Epoch:  20 | loss: 0.027186812833 \n",
      "Epoch:  21 | loss: 0.027186812833 \n",
      "Epoch:  22 | loss: 0.027186814696 \n",
      "Epoch:  23 | loss: 0.027186814696 \n",
      "Epoch:  24 | loss: 0.027186810970 \n",
      "Epoch:  25 | loss: 0.027186812833 \n",
      "Epoch:  26 | loss: 0.027186809108 \n",
      "Epoch:  27 | loss: 0.027186809108 \n",
      "Epoch:  28 | loss: 0.027186809108 \n",
      "Epoch:  29 | loss: 0.027186809108 \n",
      "Epoch:  30 | loss: 0.027186809108 \n",
      "Epoch:  31 | loss: 0.027186809108 \n",
      "Epoch:  32 | loss: 0.027186809108 \n",
      "Epoch:  33 | loss: 0.027186809108 \n",
      "Epoch:  34 | loss: 0.027186809108 \n",
      "Epoch:  35 | loss: 0.027186809108 \n",
      "Epoch:  36 | loss: 0.027186809108 \n",
      "Epoch:  37 | loss: 0.027186809108 \n",
      "Epoch:  38 | loss: 0.027186809108 \n",
      "Epoch:  39 | loss: 0.027186809108 \n",
      "Epoch:  40 | loss: 0.027186809108 \n",
      "Epoch:  41 | loss: 0.027186809108 \n",
      "Epoch:  42 | loss: 0.027186809108 \n",
      "Epoch:  43 | loss: 0.027186809108 \n",
      "Epoch:  44 | loss: 0.027186809108 \n",
      "Epoch:  45 | loss: 0.027186805382 \n",
      "Epoch:  46 | loss: 0.027186809108 \n",
      "Epoch:  47 | loss: 0.027186801657 \n",
      "Epoch:  48 | loss: 0.027186801657 \n",
      "Epoch:  49 | loss: 0.027186801657 \n",
      "Epoch:  50 | loss: 0.027186801657 \n",
      "Epoch:  51 | loss: 0.027186801657 \n",
      "Epoch:  52 | loss: 0.027186797932 \n",
      "Epoch:  53 | loss: 0.027186797932 \n",
      "Epoch:  54 | loss: 0.027186797932 \n",
      "Epoch:  55 | loss: 0.027186801657 \n",
      "Epoch:  56 | loss: 0.027186801657 \n",
      "Epoch:  57 | loss: 0.027186794207 \n",
      "Epoch:  58 | loss: 0.027186796069 \n",
      "Epoch:  59 | loss: 0.027186796069 \n",
      "Epoch:  60 | loss: 0.027186790481 \n",
      "Epoch:  61 | loss: 0.027186792344 \n",
      "Epoch:  62 | loss: 0.027186790481 \n",
      "Epoch:  63 | loss: 0.027186796069 \n",
      "Epoch:  64 | loss: 0.027186792344 \n",
      "Epoch:  65 | loss: 0.027186790481 \n",
      "Epoch:  66 | loss: 0.027186790481 \n",
      "Epoch:  67 | loss: 0.027186792344 \n",
      "Epoch:  68 | loss: 0.027186790481 \n",
      "Epoch:  69 | loss: 0.027186784893 \n",
      "Epoch:  70 | loss: 0.027186786756 \n",
      "Epoch:  71 | loss: 0.027186786756 \n",
      "Epoch:  72 | loss: 0.027186783031 \n",
      "Epoch:  73 | loss: 0.027186783031 \n",
      "Epoch:  74 | loss: 0.027186784893 \n",
      "Epoch:  75 | loss: 0.027186783031 \n",
      "Epoch:  76 | loss: 0.027186783031 \n",
      "Epoch:  77 | loss: 0.027186781168 \n",
      "Epoch:  78 | loss: 0.027186781168 \n",
      "Epoch:  79 | loss: 0.027186783031 \n",
      "Epoch:  80 | loss: 0.027186777443 \n",
      "Epoch:  81 | loss: 0.027186777443 \n",
      "Epoch:  82 | loss: 0.027186779305 \n",
      "Epoch:  83 | loss: 0.027186777443 \n",
      "Epoch:  84 | loss: 0.027186779305 \n",
      "Epoch:  85 | loss: 0.027186779305 \n",
      "Epoch:  86 | loss: 0.027186773717 \n",
      "Epoch:  87 | loss: 0.027186773717 \n",
      "Epoch:  88 | loss: 0.027186771855 \n",
      "Epoch:  89 | loss: 0.027186771855 \n",
      "Epoch:  90 | loss: 0.027186773717 \n",
      "Epoch:  91 | loss: 0.027186771855 \n",
      "Epoch:  92 | loss: 0.027186771855 \n",
      "Epoch:  93 | loss: 0.027186771855 \n",
      "Epoch:  94 | loss: 0.027186771855 \n",
      "Epoch:  95 | loss: 0.027186771855 \n",
      "Epoch:  96 | loss: 0.027186769992 \n",
      "Epoch:  97 | loss: 0.027186768129 \n",
      "Epoch:  98 | loss: 0.027186771855 \n",
      "Epoch:  99 | loss: 0.027186764404 \n",
      "Epoch:  100 | loss: 0.027186764404 \n",
      "Epoch:  101 | loss: 0.027186753228 \n",
      "Epoch:  102 | loss: 0.027186734602 \n",
      "Epoch:  103 | loss: 0.027186736465 \n",
      "Epoch:  104 | loss: 0.027186701074 \n",
      "Epoch:  105 | loss: 0.027186678723 \n",
      "Epoch:  106 | loss: 0.027186673135 \n",
      "Epoch:  107 | loss: 0.027186656371 \n",
      "Epoch:  108 | loss: 0.027186648920 \n",
      "Epoch:  109 | loss: 0.027186643332 \n",
      "Epoch:  110 | loss: 0.027186645195 \n",
      "Epoch:  111 | loss: 0.027186643332 \n",
      "Epoch:  112 | loss: 0.027186606079 \n",
      "Epoch:  113 | loss: 0.027186606079 \n",
      "Epoch:  114 | loss: 0.027186602354 \n",
      "Epoch:  115 | loss: 0.027186602354 \n",
      "Epoch:  116 | loss: 0.027186581865 \n",
      "Epoch:  117 | loss: 0.027186581865 \n",
      "Epoch:  118 | loss: 0.027186542749 \n",
      "Epoch:  119 | loss: 0.027186527848 \n",
      "Epoch:  120 | loss: 0.027186509222 \n",
      "Epoch:  121 | loss: 0.027186507359 \n",
      "Epoch:  122 | loss: 0.027186470106 \n",
      "Epoch:  123 | loss: 0.027186447755 \n",
      "Epoch:  124 | loss: 0.027186430991 \n",
      "Epoch:  125 | loss: 0.027186432853 \n",
      "Epoch:  126 | loss: 0.027186423540 \n",
      "Epoch:  127 | loss: 0.027186414227 \n",
      "Epoch:  128 | loss: 0.027186386287 \n",
      "Epoch:  129 | loss: 0.027186378837 \n",
      "Epoch:  130 | loss: 0.027186362073 \n",
      "Epoch:  131 | loss: 0.027186349034 \n",
      "Epoch:  132 | loss: 0.027186322957 \n",
      "Epoch:  133 | loss: 0.027186317369 \n",
      "Epoch:  134 | loss: 0.027186317369 \n",
      "Epoch:  135 | loss: 0.027186311781 \n",
      "Epoch:  136 | loss: 0.027186308056 \n",
      "Epoch:  137 | loss: 0.027186298743 \n",
      "Epoch:  138 | loss: 0.027186280116 \n",
      "Epoch:  139 | loss: 0.027186261490 \n",
      "Epoch:  140 | loss: 0.027186242864 \n",
      "Epoch:  141 | loss: 0.027186227962 \n",
      "Epoch:  142 | loss: 0.027186186984 \n",
      "Epoch:  143 | loss: 0.027186183259 \n",
      "Epoch:  144 | loss: 0.027186179534 \n",
      "Epoch:  145 | loss: 0.027186175808 \n",
      "Epoch:  146 | loss: 0.027186151594 \n",
      "Epoch:  147 | loss: 0.027186138555 \n",
      "Epoch:  148 | loss: 0.027186136693 \n",
      "Epoch:  149 | loss: 0.027186110616 \n",
      "Epoch:  150 | loss: 0.027186110616 \n",
      "Epoch:  151 | loss: 0.027186103165 \n",
      "Epoch:  152 | loss: 0.027186099440 \n",
      "Epoch:  153 | loss: 0.027186090127 \n",
      "Epoch:  154 | loss: 0.027186052874 \n",
      "Epoch:  155 | loss: 0.027186041698 \n",
      "Epoch:  156 | loss: 0.027186034247 \n",
      "Epoch:  157 | loss: 0.027186034247 \n",
      "Epoch:  158 | loss: 0.027186028659 \n",
      "Epoch:  159 | loss: 0.027186017483 \n",
      "Epoch:  160 | loss: 0.027186004445 \n",
      "Epoch:  161 | loss: 0.027185998857 \n",
      "Epoch:  162 | loss: 0.027185983956 \n",
      "Epoch:  163 | loss: 0.027185967192 \n",
      "Epoch:  164 | loss: 0.027185969055 \n",
      "Epoch:  165 | loss: 0.027185961604 \n",
      "Epoch:  166 | loss: 0.027185961604 \n",
      "Epoch:  167 | loss: 0.027185959741 \n",
      "Epoch:  168 | loss: 0.027185952291 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  169 | loss: 0.027185937390 \n",
      "Epoch:  170 | loss: 0.027185924351 \n",
      "Epoch:  171 | loss: 0.027185922489 \n",
      "Epoch:  172 | loss: 0.027185903862 \n",
      "Epoch:  173 | loss: 0.027185885236 \n",
      "Epoch:  174 | loss: 0.027185883373 \n",
      "Epoch:  175 | loss: 0.027185885236 \n",
      "Epoch:  176 | loss: 0.027185883373 \n",
      "Epoch:  177 | loss: 0.027185849845 \n",
      "Epoch:  178 | loss: 0.027185851708 \n",
      "Epoch:  179 | loss: 0.027185842395 \n",
      "Epoch:  180 | loss: 0.027185834944 \n",
      "Epoch:  181 | loss: 0.027185823768 \n",
      "Epoch:  182 | loss: 0.027185797691 \n",
      "Epoch:  183 | loss: 0.027185792103 \n",
      "Epoch:  184 | loss: 0.027185739949 \n",
      "Epoch:  185 | loss: 0.027185717598 \n",
      "Epoch:  186 | loss: 0.027185693383 \n",
      "Epoch:  187 | loss: 0.027185687795 \n",
      "Epoch:  188 | loss: 0.027185680345 \n",
      "Epoch:  189 | loss: 0.027185676619 \n",
      "Epoch:  190 | loss: 0.027185678482 \n",
      "Epoch:  191 | loss: 0.027185648680 \n",
      "Epoch:  192 | loss: 0.027185643092 \n",
      "Epoch:  193 | loss: 0.027185641229 \n",
      "Epoch:  194 | loss: 0.027185637504 \n",
      "Epoch:  195 | loss: 0.027185631916 \n",
      "Epoch:  196 | loss: 0.027185626328 \n",
      "Epoch:  197 | loss: 0.027185618877 \n",
      "Epoch:  198 | loss: 0.027185617015 \n",
      "Epoch:  199 | loss: 0.027185618877 \n",
      "Epoch:  200 | loss: 0.027185618877 \n",
      "Epoch:  201 | loss: 0.027185617015 \n",
      "Epoch:  202 | loss: 0.027185618877 \n",
      "Epoch:  203 | loss: 0.027185618877 \n",
      "Epoch:  204 | loss: 0.027185618877 \n",
      "Epoch:  205 | loss: 0.027185615152 \n",
      "Epoch:  206 | loss: 0.027185617015 \n",
      "Epoch:  207 | loss: 0.027185618877 \n",
      "Epoch:  208 | loss: 0.027185615152 \n",
      "Epoch:  209 | loss: 0.027185617015 \n",
      "Epoch:  210 | loss: 0.027185618877 \n",
      "Epoch:  211 | loss: 0.027185617015 \n",
      "Epoch:  212 | loss: 0.027185615152 \n",
      "Epoch:  213 | loss: 0.027185615152 \n",
      "Epoch:  214 | loss: 0.027185615152 \n",
      "Epoch:  215 | loss: 0.027185615152 \n",
      "Epoch:  216 | loss: 0.027185613289 \n",
      "Epoch:  217 | loss: 0.027185613289 \n",
      "Epoch:  218 | loss: 0.027185611427 \n",
      "Epoch:  219 | loss: 0.027185611427 \n",
      "Epoch:  220 | loss: 0.027185609564 \n",
      "Epoch:  221 | loss: 0.027185607702 \n",
      "Epoch:  222 | loss: 0.027185607702 \n",
      "Epoch:  223 | loss: 0.027185609564 \n",
      "Epoch:  224 | loss: 0.027185609564 \n",
      "Epoch:  225 | loss: 0.027185607702 \n",
      "Epoch:  226 | loss: 0.027185607702 \n",
      "Epoch:  227 | loss: 0.027185607702 \n",
      "Epoch:  228 | loss: 0.027185607702 \n",
      "Epoch:  229 | loss: 0.027185607702 \n",
      "Epoch:  230 | loss: 0.027185607702 \n",
      "Epoch:  231 | loss: 0.027185607702 \n",
      "Epoch:  232 | loss: 0.027185609564 \n",
      "Epoch:  233 | loss: 0.027185605839 \n",
      "Epoch:  234 | loss: 0.027185607702 \n",
      "Epoch:  235 | loss: 0.027185607702 \n",
      "Epoch:  236 | loss: 0.027185603976 \n",
      "Epoch:  237 | loss: 0.027185605839 \n",
      "Epoch:  238 | loss: 0.027185603976 \n",
      "Epoch:  239 | loss: 0.027185602114 \n",
      "Epoch:  240 | loss: 0.027185603976 \n",
      "Epoch:  241 | loss: 0.027185603976 \n",
      "Epoch:  242 | loss: 0.027185603976 \n",
      "Epoch:  243 | loss: 0.027185602114 \n",
      "Epoch:  244 | loss: 0.027185602114 \n",
      "Epoch:  245 | loss: 0.027185602114 \n",
      "Epoch:  246 | loss: 0.027185602114 \n",
      "Epoch:  247 | loss: 0.027185600251 \n",
      "Epoch:  248 | loss: 0.027185603976 \n",
      "Epoch:  249 | loss: 0.027185600251 \n",
      "Epoch:  250 | loss: 0.027185600251 \n",
      "Epoch:  251 | loss: 0.027185602114 \n",
      "Epoch:  252 | loss: 0.027185603976 \n",
      "Epoch:  253 | loss: 0.027185600251 \n",
      "Epoch:  254 | loss: 0.027185602114 \n",
      "Epoch:  255 | loss: 0.027185598388 \n",
      "Epoch:  256 | loss: 0.027185600251 \n",
      "Epoch:  257 | loss: 0.027185600251 \n",
      "Epoch:  258 | loss: 0.027185602114 \n",
      "Epoch:  259 | loss: 0.027185600251 \n",
      "Epoch:  260 | loss: 0.027185600251 \n",
      "Epoch:  261 | loss: 0.027185600251 \n",
      "Epoch:  262 | loss: 0.027185600251 \n",
      "Epoch:  263 | loss: 0.027185598388 \n",
      "Epoch:  264 | loss: 0.027185598388 \n",
      "Epoch:  265 | loss: 0.027185598388 \n",
      "Epoch:  266 | loss: 0.027185600251 \n",
      "Epoch:  267 | loss: 0.027185598388 \n",
      "Epoch:  268 | loss: 0.027185598388 \n",
      "Epoch:  269 | loss: 0.027185598388 \n",
      "Epoch:  270 | loss: 0.027185600251 \n",
      "Epoch:  271 | loss: 0.027185600251 \n",
      "Epoch:  272 | loss: 0.027185598388 \n",
      "Epoch:  273 | loss: 0.027185598388 \n",
      "Epoch:  274 | loss: 0.027185598388 \n",
      "Epoch:  275 | loss: 0.027185598388 \n",
      "Epoch:  276 | loss: 0.027185598388 \n",
      "Epoch:  277 | loss: 0.027185600251 \n",
      "Epoch:  278 | loss: 0.027185598388 \n",
      "Epoch:  279 | loss: 0.027185596526 \n",
      "Epoch:  280 | loss: 0.027185598388 \n",
      "Epoch:  281 | loss: 0.027185598388 \n",
      "Epoch:  282 | loss: 0.027185598388 \n",
      "Epoch:  283 | loss: 0.027185600251 \n",
      "Epoch:  284 | loss: 0.027185590938 \n",
      "Epoch:  285 | loss: 0.027185589075 \n",
      "Epoch:  286 | loss: 0.027185590938 \n",
      "Epoch:  287 | loss: 0.027185589075 \n",
      "Epoch:  288 | loss: 0.027185592800 \n",
      "Epoch:  289 | loss: 0.027185590938 \n",
      "Epoch:  290 | loss: 0.027185590938 \n",
      "Epoch:  291 | loss: 0.027185587212 \n",
      "Epoch:  292 | loss: 0.027185589075 \n",
      "Epoch:  293 | loss: 0.027185589075 \n",
      "Epoch:  294 | loss: 0.027185589075 \n",
      "Epoch:  295 | loss: 0.027185590938 \n",
      "Epoch:  296 | loss: 0.027185585350 \n",
      "Epoch:  297 | loss: 0.027185585350 \n",
      "Epoch:  298 | loss: 0.027185585350 \n",
      "Epoch:  299 | loss: 0.027185583487 \n",
      "time=53.494232177734375\n",
      "best_train_loss=0.027185583487153053\n",
      "test_loss=0.02613886445760727\n",
      "best_epoch=299\n",
      "w=30\n",
      "a_kerr=0.0\n",
      "M=1.0\n",
      "r_c=2.0\n",
      "Epoch:  0 | loss: 0.027185583487 \n",
      "Epoch:  1 | loss: 0.027185413986 \n",
      "Epoch:  2 | loss: 0.027185244486 \n",
      "Epoch:  3 | loss: 0.027185074985 \n",
      "Epoch:  4 | loss: 0.027184912935 \n",
      "Epoch:  5 | loss: 0.027184741572 \n",
      "Epoch:  6 | loss: 0.027184572071 \n",
      "Epoch:  7 | loss: 0.027184410021 \n",
      "Epoch:  8 | loss: 0.027184247971 \n",
      "Epoch:  9 | loss: 0.027184082195 \n",
      "Epoch:  10 | loss: 0.027183914557 \n",
      "Epoch:  11 | loss: 0.027183745056 \n",
      "Epoch:  12 | loss: 0.027183579281 \n",
      "Epoch:  13 | loss: 0.027183417231 \n",
      "Epoch:  14 | loss: 0.027183249593 \n",
      "Epoch:  15 | loss: 0.027183089405 \n",
      "Epoch:  16 | loss: 0.027182923630 \n",
      "Epoch:  17 | loss: 0.027182759717 \n",
      "Epoch:  18 | loss: 0.027182592079 \n",
      "Epoch:  19 | loss: 0.027182426304 \n",
      "Epoch:  20 | loss: 0.027182264253 \n",
      "Epoch:  21 | loss: 0.027182102203 \n",
      "Epoch:  22 | loss: 0.027181940153 \n",
      "Epoch:  23 | loss: 0.027181778103 \n",
      "Epoch:  24 | loss: 0.027181619778 \n",
      "Epoch:  25 | loss: 0.027181454003 \n",
      "Epoch:  26 | loss: 0.027181291953 \n",
      "Epoch:  27 | loss: 0.027181131765 \n",
      "Epoch:  28 | loss: 0.027180977166 \n",
      "Epoch:  29 | loss: 0.027180809528 \n",
      "Epoch:  30 | loss: 0.027180654928 \n",
      "Epoch:  31 | loss: 0.027180492878 \n",
      "Epoch:  32 | loss: 0.027180328965 \n",
      "Epoch:  33 | loss: 0.027180166915 \n",
      "Epoch:  34 | loss: 0.027180004865 \n",
      "Epoch:  35 | loss: 0.027179848403 \n",
      "Epoch:  36 | loss: 0.027179688215 \n",
      "Epoch:  37 | loss: 0.027179529890 \n",
      "Epoch:  38 | loss: 0.027179375291 \n",
      "Epoch:  39 | loss: 0.027179218829 \n",
      "Epoch:  40 | loss: 0.027179056779 \n",
      "Epoch:  41 | loss: 0.027178896591 \n",
      "Epoch:  42 | loss: 0.027178769931 \n",
      "Epoch:  43 | loss: 0.027178609744 \n",
      "Epoch:  44 | loss: 0.027178443968 \n",
      "Epoch:  45 | loss: 0.027178289369 \n",
      "Epoch:  46 | loss: 0.027178129181 \n",
      "Epoch:  47 | loss: 0.027177970856 \n",
      "Epoch:  48 | loss: 0.027177972719 \n",
      "Epoch:  49 | loss: 0.027177814394 \n",
      "Epoch:  50 | loss: 0.027177656069 \n",
      "Epoch:  51 | loss: 0.027177559212 \n",
      "Epoch:  52 | loss: 0.027177406475 \n",
      "Epoch:  53 | loss: 0.027177244425 \n",
      "Epoch:  54 | loss: 0.027177086100 \n",
      "Epoch:  55 | loss: 0.027176937088 \n",
      "Epoch:  56 | loss: 0.027176778764 \n",
      "Epoch:  57 | loss: 0.027176620439 \n",
      "Epoch:  58 | loss: 0.027176460251 \n",
      "Epoch:  59 | loss: 0.027176309377 \n",
      "Epoch:  60 | loss: 0.027176158503 \n",
      "Epoch:  61 | loss: 0.027176002041 \n",
      "Epoch:  62 | loss: 0.027175841853 \n",
      "Epoch:  63 | loss: 0.027175677940 \n",
      "Epoch:  64 | loss: 0.027175497264 \n",
      "Epoch:  65 | loss: 0.027175309137 \n",
      "Epoch:  66 | loss: 0.027175126597 \n",
      "Epoch:  67 | loss: 0.027174936607 \n",
      "Epoch:  68 | loss: 0.027174744755 \n",
      "Epoch:  69 | loss: 0.027174558491 \n",
      "Epoch:  70 | loss: 0.027174372226 \n",
      "Epoch:  71 | loss: 0.027174184099 \n",
      "Epoch:  72 | loss: 0.027174005285 \n",
      "Epoch:  73 | loss: 0.027173826471 \n",
      "Epoch:  74 | loss: 0.027173649520 \n",
      "Epoch:  75 | loss: 0.027173474431 \n",
      "Epoch:  76 | loss: 0.027173291892 \n",
      "Epoch:  77 | loss: 0.027173105627 \n",
      "Epoch:  78 | loss: 0.027173027396 \n",
      "Epoch:  79 | loss: 0.027172850445 \n",
      "Epoch:  80 | loss: 0.027172669768 \n",
      "Epoch:  81 | loss: 0.027172489092 \n",
      "Epoch:  82 | loss: 0.027172300965 \n",
      "Epoch:  83 | loss: 0.027172235772 \n",
      "Epoch:  84 | loss: 0.027172051370 \n",
      "Epoch:  85 | loss: 0.027171870694 \n",
      "Epoch:  86 | loss: 0.027171693742 \n",
      "Epoch:  87 | loss: 0.027171505615 \n",
      "Epoch:  88 | loss: 0.027171315625 \n",
      "Epoch:  89 | loss: 0.027171142399 \n",
      "Epoch:  90 | loss: 0.027170959860 \n",
      "Epoch:  91 | loss: 0.027170784771 \n",
      "Epoch:  92 | loss: 0.027170598507 \n",
      "Epoch:  93 | loss: 0.027170417830 \n",
      "Epoch:  94 | loss: 0.027170237154 \n",
      "Epoch:  95 | loss: 0.027170054615 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  96 | loss: 0.027169866487 \n",
      "Epoch:  97 | loss: 0.027169683948 \n",
      "Epoch:  98 | loss: 0.027169624344 \n",
      "Epoch:  99 | loss: 0.027169451118 \n",
      "Epoch:  100 | loss: 0.027169441804 \n",
      "Epoch:  101 | loss: 0.027169439942 \n",
      "Epoch:  102 | loss: 0.027169439942 \n",
      "Epoch:  103 | loss: 0.027169438079 \n",
      "Epoch:  104 | loss: 0.027169438079 \n",
      "Epoch:  105 | loss: 0.027169436216 \n",
      "Epoch:  106 | loss: 0.027169436216 \n",
      "Epoch:  107 | loss: 0.027169436216 \n",
      "Epoch:  108 | loss: 0.027169436216 \n",
      "Epoch:  109 | loss: 0.027169436216 \n",
      "Epoch:  110 | loss: 0.027169436216 \n",
      "Epoch:  111 | loss: 0.027169436216 \n",
      "Epoch:  112 | loss: 0.027169436216 \n",
      "Epoch:  113 | loss: 0.027169434354 \n",
      "Epoch:  114 | loss: 0.027169426903 \n",
      "Epoch:  115 | loss: 0.027169428766 \n",
      "Epoch:  116 | loss: 0.027169428766 \n",
      "Epoch:  117 | loss: 0.027169426903 \n",
      "Epoch:  118 | loss: 0.027169426903 \n",
      "Epoch:  119 | loss: 0.027169423178 \n",
      "Epoch:  120 | loss: 0.027169423178 \n",
      "Epoch:  121 | loss: 0.027169423178 \n",
      "Epoch:  122 | loss: 0.027169423178 \n",
      "Epoch:  123 | loss: 0.027169421315 \n",
      "Epoch:  124 | loss: 0.027169423178 \n",
      "Epoch:  125 | loss: 0.027169415727 \n",
      "Epoch:  126 | loss: 0.027169417590 \n",
      "Epoch:  127 | loss: 0.027169410139 \n",
      "Epoch:  128 | loss: 0.027169412002 \n",
      "Epoch:  129 | loss: 0.027169410139 \n",
      "Epoch:  130 | loss: 0.027169408277 \n",
      "Epoch:  131 | loss: 0.027169410139 \n",
      "Epoch:  132 | loss: 0.027169408277 \n",
      "Epoch:  133 | loss: 0.027169404551 \n",
      "Epoch:  134 | loss: 0.027169406414 \n",
      "Epoch:  135 | loss: 0.027169402689 \n",
      "Epoch:  136 | loss: 0.027169402689 \n",
      "Epoch:  137 | loss: 0.027169402689 \n",
      "Epoch:  138 | loss: 0.027169404551 \n",
      "Epoch:  139 | loss: 0.027169400826 \n",
      "Epoch:  140 | loss: 0.027169397101 \n",
      "Epoch:  141 | loss: 0.027169400826 \n",
      "Epoch:  142 | loss: 0.027169397101 \n",
      "Epoch:  143 | loss: 0.027169397101 \n",
      "Epoch:  144 | loss: 0.027169391513 \n",
      "Epoch:  145 | loss: 0.027169397101 \n",
      "Epoch:  146 | loss: 0.027169385925 \n",
      "Epoch:  147 | loss: 0.027169387788 \n",
      "Epoch:  148 | loss: 0.027169387788 \n",
      "Epoch:  149 | loss: 0.027169384062 \n",
      "Epoch:  150 | loss: 0.027169380337 \n",
      "Epoch:  151 | loss: 0.027169382200 \n",
      "Epoch:  152 | loss: 0.027169382200 \n",
      "Epoch:  153 | loss: 0.027169380337 \n",
      "Epoch:  154 | loss: 0.027169378474 \n",
      "Epoch:  155 | loss: 0.027169372886 \n",
      "Epoch:  156 | loss: 0.027169374749 \n",
      "Epoch:  157 | loss: 0.027169372886 \n",
      "Epoch:  158 | loss: 0.027169369161 \n",
      "Epoch:  159 | loss: 0.027169371024 \n",
      "Epoch:  160 | loss: 0.027169369161 \n",
      "Epoch:  161 | loss: 0.027169365436 \n",
      "Epoch:  162 | loss: 0.027169365436 \n",
      "Epoch:  163 | loss: 0.027169359848 \n",
      "Epoch:  164 | loss: 0.027169359848 \n",
      "Epoch:  165 | loss: 0.027169359848 \n",
      "Epoch:  166 | loss: 0.027169361711 \n",
      "Epoch:  167 | loss: 0.027169359848 \n",
      "Epoch:  168 | loss: 0.027169359848 \n",
      "Epoch:  169 | loss: 0.027169361711 \n",
      "Epoch:  170 | loss: 0.027169359848 \n",
      "Epoch:  171 | loss: 0.027169359848 \n",
      "Epoch:  172 | loss: 0.027169357985 \n",
      "Epoch:  173 | loss: 0.027169356123 \n",
      "Epoch:  174 | loss: 0.027169350535 \n",
      "Epoch:  175 | loss: 0.027169346809 \n",
      "Epoch:  176 | loss: 0.027169344947 \n",
      "Epoch:  177 | loss: 0.027169341221 \n",
      "Epoch:  178 | loss: 0.027169343084 \n",
      "Epoch:  179 | loss: 0.027169339359 \n",
      "Epoch:  180 | loss: 0.027169341221 \n",
      "Epoch:  181 | loss: 0.027169339359 \n",
      "Epoch:  182 | loss: 0.027169339359 \n",
      "Epoch:  183 | loss: 0.027169341221 \n",
      "Epoch:  184 | loss: 0.027169331908 \n",
      "Epoch:  185 | loss: 0.027169331908 \n",
      "Epoch:  186 | loss: 0.027169331908 \n",
      "Epoch:  187 | loss: 0.027169333771 \n",
      "Epoch:  188 | loss: 0.027169335634 \n",
      "Epoch:  189 | loss: 0.027169331908 \n",
      "Epoch:  190 | loss: 0.027169331908 \n",
      "Epoch:  191 | loss: 0.027169328183 \n",
      "Epoch:  192 | loss: 0.027169328183 \n",
      "Epoch:  193 | loss: 0.027169326320 \n",
      "Epoch:  194 | loss: 0.027169326320 \n",
      "Epoch:  195 | loss: 0.027169326320 \n",
      "Epoch:  196 | loss: 0.027169328183 \n",
      "Epoch:  197 | loss: 0.027169324458 \n",
      "Epoch:  198 | loss: 0.027169322595 \n",
      "Epoch:  199 | loss: 0.027169322595 \n",
      "Epoch:  200 | loss: 0.027169322595 \n",
      "Epoch:  201 | loss: 0.027169322595 \n",
      "Epoch:  202 | loss: 0.027169322595 \n",
      "Epoch:  203 | loss: 0.027169322595 \n",
      "Epoch:  204 | loss: 0.027169322595 \n",
      "Epoch:  205 | loss: 0.027169322595 \n",
      "Epoch:  206 | loss: 0.027169322595 \n",
      "Epoch:  207 | loss: 0.027169324458 \n",
      "Epoch:  208 | loss: 0.027169322595 \n",
      "Epoch:  209 | loss: 0.027169324458 \n",
      "Epoch:  210 | loss: 0.027169322595 \n",
      "Epoch:  211 | loss: 0.027169322595 \n",
      "Epoch:  212 | loss: 0.027169322595 \n",
      "Epoch:  213 | loss: 0.027169322595 \n",
      "Epoch:  214 | loss: 0.027169322595 \n",
      "Epoch:  215 | loss: 0.027169322595 \n",
      "Epoch:  216 | loss: 0.027169322595 \n",
      "Epoch:  217 | loss: 0.027169322595 \n",
      "Epoch:  218 | loss: 0.027169322595 \n",
      "Epoch:  219 | loss: 0.027169322595 \n",
      "Epoch:  220 | loss: 0.027169322595 \n",
      "Epoch:  221 | loss: 0.027169322595 \n",
      "Epoch:  222 | loss: 0.027169324458 \n",
      "Epoch:  223 | loss: 0.027169322595 \n",
      "Epoch:  224 | loss: 0.027169322595 \n",
      "Epoch:  225 | loss: 0.027169322595 \n",
      "Epoch:  226 | loss: 0.027169322595 \n",
      "Epoch:  227 | loss: 0.027169324458 \n",
      "Epoch:  228 | loss: 0.027169320732 \n",
      "Epoch:  229 | loss: 0.027169322595 \n",
      "Epoch:  230 | loss: 0.027169322595 \n",
      "Epoch:  231 | loss: 0.027169320732 \n",
      "Epoch:  232 | loss: 0.027169324458 \n",
      "Epoch:  233 | loss: 0.027169322595 \n",
      "Epoch:  234 | loss: 0.027169322595 \n",
      "Epoch:  235 | loss: 0.027169322595 \n",
      "Epoch:  236 | loss: 0.027169322595 \n",
      "Epoch:  237 | loss: 0.027169322595 \n",
      "Epoch:  238 | loss: 0.027169320732 \n",
      "Epoch:  239 | loss: 0.027169322595 \n",
      "Epoch:  240 | loss: 0.027169322595 \n",
      "Epoch:  241 | loss: 0.027169322595 \n",
      "Epoch:  242 | loss: 0.027169322595 \n",
      "Epoch:  243 | loss: 0.027169322595 \n",
      "Epoch:  244 | loss: 0.027169322595 \n",
      "Epoch:  245 | loss: 0.027169318870 \n",
      "Epoch:  246 | loss: 0.027169320732 \n",
      "Epoch:  247 | loss: 0.027169322595 \n",
      "Epoch:  248 | loss: 0.027169320732 \n",
      "Epoch:  249 | loss: 0.027169320732 \n",
      "Epoch:  250 | loss: 0.027169318870 \n",
      "Epoch:  251 | loss: 0.027169320732 \n",
      "Epoch:  252 | loss: 0.027169318870 \n",
      "Epoch:  253 | loss: 0.027169322595 \n",
      "Epoch:  254 | loss: 0.027169317007 \n",
      "Epoch:  255 | loss: 0.027169320732 \n",
      "Epoch:  256 | loss: 0.027169320732 \n",
      "Epoch:  257 | loss: 0.027169318870 \n",
      "Epoch:  258 | loss: 0.027169318870 \n",
      "Epoch:  259 | loss: 0.027169318870 \n",
      "Epoch:  260 | loss: 0.027169317007 \n",
      "Epoch:  261 | loss: 0.027169318870 \n",
      "Epoch:  262 | loss: 0.027169318870 \n",
      "Epoch:  263 | loss: 0.027169315144 \n",
      "Epoch:  264 | loss: 0.027169318870 \n",
      "Epoch:  265 | loss: 0.027169315144 \n",
      "Epoch:  266 | loss: 0.027169315144 \n",
      "Epoch:  267 | loss: 0.027169317007 \n",
      "Epoch:  268 | loss: 0.027169317007 \n",
      "Epoch:  269 | loss: 0.027169315144 \n",
      "Epoch:  270 | loss: 0.027169315144 \n",
      "Epoch:  271 | loss: 0.027169315144 \n",
      "Epoch:  272 | loss: 0.027169315144 \n",
      "Epoch:  273 | loss: 0.027169317007 \n",
      "Epoch:  274 | loss: 0.027169318870 \n",
      "Epoch:  275 | loss: 0.027169313282 \n",
      "Epoch:  276 | loss: 0.027169315144 \n",
      "Epoch:  277 | loss: 0.027169315144 \n",
      "Epoch:  278 | loss: 0.027169313282 \n",
      "Epoch:  279 | loss: 0.027169315144 \n",
      "Epoch:  280 | loss: 0.027169313282 \n",
      "Epoch:  281 | loss: 0.027169313282 \n",
      "Epoch:  282 | loss: 0.027169315144 \n",
      "Epoch:  283 | loss: 0.027169315144 \n",
      "Epoch:  284 | loss: 0.027169313282 \n",
      "Epoch:  285 | loss: 0.027169311419 \n",
      "Epoch:  286 | loss: 0.027169311419 \n",
      "Epoch:  287 | loss: 0.027169311419 \n",
      "Epoch:  288 | loss: 0.027169315144 \n",
      "Epoch:  289 | loss: 0.027169311419 \n",
      "Epoch:  290 | loss: 0.027169311419 \n",
      "Epoch:  291 | loss: 0.027169313282 \n",
      "Epoch:  292 | loss: 0.027169315144 \n",
      "Epoch:  293 | loss: 0.027169315144 \n",
      "Epoch:  294 | loss: 0.027169311419 \n",
      "Epoch:  295 | loss: 0.027169311419 \n",
      "Epoch:  296 | loss: 0.027169313282 \n",
      "Epoch:  297 | loss: 0.027169311419 \n",
      "Epoch:  298 | loss: 0.027169311419 \n",
      "Epoch:  299 | loss: 0.027169313282 \n",
      "time=185.39468431472778\n",
      "best_train_loss=0.02716931141912937\n",
      "test_loss=0.02612530253827572\n",
      "best_epoch=285\n",
      "w=35\n",
      "a_kerr=0.0\n",
      "M=1.0\n",
      "r_c=2.0\n",
      "Epoch:  0 | loss: 0.027169311419 \n",
      "Epoch:  1 | loss: 0.027169171721 \n",
      "Epoch:  2 | loss: 0.027169119567 \n",
      "Epoch:  3 | loss: 0.027169097215 \n",
      "Epoch:  4 | loss: 0.027168963104 \n",
      "Epoch:  5 | loss: 0.027168890461 \n",
      "Epoch:  6 | loss: 0.027168758214 \n",
      "Epoch:  7 | loss: 0.027168624103 \n",
      "Epoch:  8 | loss: 0.027168478817 \n",
      "Epoch:  9 | loss: 0.027168335393 \n",
      "Epoch:  10 | loss: 0.027168212458 \n",
      "Epoch:  11 | loss: 0.027168108150 \n",
      "Epoch:  12 | loss: 0.027167964727 \n",
      "Epoch:  13 | loss: 0.027167830616 \n",
      "Epoch:  14 | loss: 0.027167690918 \n",
      "Epoch:  15 | loss: 0.027167646214 \n",
      "Epoch:  16 | loss: 0.027167642489 \n",
      "Epoch:  17 | loss: 0.027167603374 \n",
      "Epoch:  18 | loss: 0.027167463675 \n",
      "Epoch:  19 | loss: 0.027167327702 \n",
      "Epoch:  20 | loss: 0.027167286724 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  21 | loss: 0.027167212218 \n",
      "Epoch:  22 | loss: 0.027167188004 \n",
      "Epoch:  23 | loss: 0.027167139575 \n",
      "Epoch:  24 | loss: 0.027167003602 \n",
      "Epoch:  25 | loss: 0.027166860178 \n",
      "Epoch:  26 | loss: 0.027166720480 \n",
      "Epoch:  27 | loss: 0.027166722342 \n",
      "Epoch:  28 | loss: 0.027166586369 \n",
      "Epoch:  29 | loss: 0.027166491374 \n",
      "Epoch:  30 | loss: 0.027166433632 \n",
      "Epoch:  31 | loss: 0.027166413143 \n",
      "Epoch:  32 | loss: 0.027166280895 \n",
      "Epoch:  33 | loss: 0.027166144922 \n",
      "Epoch:  34 | loss: 0.027166098356 \n",
      "Epoch:  35 | loss: 0.027165964246 \n",
      "Epoch:  36 | loss: 0.027165943757 \n",
      "Epoch:  37 | loss: 0.027165800333 \n",
      "Epoch:  38 | loss: 0.027165664360 \n",
      "Epoch:  39 | loss: 0.027165630832 \n",
      "Epoch:  40 | loss: 0.027165487409 \n",
      "Epoch:  41 | loss: 0.027165453881 \n",
      "Epoch:  42 | loss: 0.027165316045 \n",
      "Epoch:  43 | loss: 0.027165178210 \n",
      "Epoch:  44 | loss: 0.027165049687 \n",
      "Epoch:  45 | loss: 0.027164911851 \n",
      "Epoch:  46 | loss: 0.027164869010 \n",
      "Epoch:  47 | loss: 0.027164813131 \n",
      "Epoch:  48 | loss: 0.027164740488 \n",
      "Epoch:  49 | loss: 0.027164598927 \n",
      "Epoch:  50 | loss: 0.027164535597 \n",
      "Epoch:  51 | loss: 0.027164401487 \n",
      "Epoch:  52 | loss: 0.027164261788 \n",
      "Epoch:  53 | loss: 0.027164224535 \n",
      "Epoch:  54 | loss: 0.027164086699 \n",
      "Epoch:  55 | loss: 0.027163997293 \n",
      "Epoch:  56 | loss: 0.027163859457 \n",
      "Epoch:  57 | loss: 0.027163719758 \n",
      "Epoch:  58 | loss: 0.027163583785 \n",
      "Epoch:  59 | loss: 0.027163470164 \n",
      "Epoch:  60 | loss: 0.027163336053 \n",
      "Epoch:  61 | loss: 0.027163190767 \n",
      "Epoch:  62 | loss: 0.027163052931 \n",
      "Epoch:  63 | loss: 0.027162967250 \n",
      "Epoch:  64 | loss: 0.027162957937 \n",
      "Epoch:  65 | loss: 0.027162922546 \n",
      "Epoch:  66 | loss: 0.027162784711 \n",
      "Epoch:  67 | loss: 0.027162726969 \n",
      "Epoch:  68 | loss: 0.027162581682 \n",
      "Epoch:  69 | loss: 0.027162443846 \n",
      "Epoch:  70 | loss: 0.027162430808 \n",
      "Epoch:  71 | loss: 0.027162414044 \n",
      "Epoch:  72 | loss: 0.027162270620 \n",
      "Epoch:  73 | loss: 0.027162130922 \n",
      "Epoch:  74 | loss: 0.027162013575 \n",
      "Epoch:  75 | loss: 0.027161875740 \n",
      "Epoch:  76 | loss: 0.027161825448 \n",
      "Epoch:  77 | loss: 0.027161769569 \n",
      "Epoch:  78 | loss: 0.027161635458 \n",
      "Epoch:  79 | loss: 0.027161523700 \n",
      "Epoch:  80 | loss: 0.027161506936 \n",
      "Epoch:  81 | loss: 0.027161365375 \n",
      "Epoch:  82 | loss: 0.027161337435 \n",
      "Epoch:  83 | loss: 0.027161199600 \n",
      "Epoch:  84 | loss: 0.027161195874 \n",
      "Epoch:  85 | loss: 0.027161143720 \n",
      "Epoch:  86 | loss: 0.027161115780 \n",
      "Epoch:  87 | loss: 0.027160983533 \n",
      "Epoch:  88 | loss: 0.027160936967 \n",
      "Epoch:  89 | loss: 0.027160823345 \n",
      "Epoch:  90 | loss: 0.027160784230 \n",
      "Epoch:  91 | loss: 0.027160646394 \n",
      "Epoch:  92 | loss: 0.027160504833 \n",
      "Epoch:  93 | loss: 0.027160488069 \n",
      "Epoch:  94 | loss: 0.027160409838 \n",
      "Epoch:  95 | loss: 0.027160277590 \n",
      "Epoch:  96 | loss: 0.027160141617 \n",
      "Epoch:  97 | loss: 0.027160033584 \n",
      "Epoch:  98 | loss: 0.027160011232 \n",
      "Epoch:  99 | loss: 0.027160014957 \n",
      "Epoch:  100 | loss: 0.027160014957 \n",
      "Epoch:  101 | loss: 0.027160014957 \n",
      "Epoch:  102 | loss: 0.027160013095 \n",
      "Epoch:  103 | loss: 0.027160011232 \n",
      "Epoch:  104 | loss: 0.027160013095 \n",
      "Epoch:  105 | loss: 0.027160011232 \n",
      "Epoch:  106 | loss: 0.027160011232 \n",
      "Epoch:  107 | loss: 0.027160009369 \n",
      "Epoch:  108 | loss: 0.027160005644 \n",
      "Epoch:  109 | loss: 0.027160007507 \n",
      "Epoch:  110 | loss: 0.027160003781 \n",
      "Epoch:  111 | loss: 0.027160000056 \n",
      "Epoch:  112 | loss: 0.027159998193 \n",
      "Epoch:  113 | loss: 0.027159996331 \n",
      "Epoch:  114 | loss: 0.027159998193 \n",
      "Epoch:  115 | loss: 0.027159998193 \n",
      "Epoch:  116 | loss: 0.027159996331 \n",
      "Epoch:  117 | loss: 0.027159996331 \n",
      "Epoch:  118 | loss: 0.027159998193 \n",
      "Epoch:  119 | loss: 0.027159996331 \n",
      "Epoch:  120 | loss: 0.027159994468 \n",
      "Epoch:  121 | loss: 0.027159996331 \n",
      "Epoch:  122 | loss: 0.027159994468 \n",
      "Epoch:  123 | loss: 0.027159992605 \n",
      "Epoch:  124 | loss: 0.027159992605 \n",
      "Epoch:  125 | loss: 0.027159992605 \n",
      "Epoch:  126 | loss: 0.027159992605 \n",
      "Epoch:  127 | loss: 0.027159992605 \n",
      "Epoch:  128 | loss: 0.027159992605 \n",
      "Epoch:  129 | loss: 0.027159990743 \n",
      "Epoch:  130 | loss: 0.027159990743 \n",
      "Epoch:  131 | loss: 0.027159992605 \n",
      "Epoch:  132 | loss: 0.027159990743 \n",
      "Epoch:  133 | loss: 0.027159990743 \n",
      "Epoch:  134 | loss: 0.027159990743 \n",
      "Epoch:  135 | loss: 0.027159992605 \n",
      "Epoch:  136 | loss: 0.027159988880 \n",
      "Epoch:  137 | loss: 0.027159987018 \n",
      "Epoch:  138 | loss: 0.027159988880 \n",
      "Epoch:  139 | loss: 0.027159988880 \n",
      "Epoch:  140 | loss: 0.027159987018 \n",
      "Epoch:  141 | loss: 0.027159985155 \n",
      "Epoch:  142 | loss: 0.027159983292 \n",
      "Epoch:  143 | loss: 0.027159981430 \n",
      "Epoch:  144 | loss: 0.027159979567 \n",
      "Epoch:  145 | loss: 0.027159981430 \n",
      "Epoch:  146 | loss: 0.027159977704 \n",
      "Epoch:  147 | loss: 0.027159979567 \n",
      "Epoch:  148 | loss: 0.027159973979 \n",
      "Epoch:  149 | loss: 0.027159973979 \n",
      "Epoch:  150 | loss: 0.027159973979 \n",
      "Epoch:  151 | loss: 0.027159975842 \n",
      "Epoch:  152 | loss: 0.027159973979 \n",
      "Epoch:  153 | loss: 0.027159975842 \n",
      "Epoch:  154 | loss: 0.027159972116 \n",
      "Epoch:  155 | loss: 0.027159973979 \n",
      "Epoch:  156 | loss: 0.027159973979 \n",
      "Epoch:  157 | loss: 0.027159973979 \n",
      "Epoch:  158 | loss: 0.027159973979 \n",
      "Epoch:  159 | loss: 0.027159972116 \n",
      "Epoch:  160 | loss: 0.027159970254 \n",
      "Epoch:  161 | loss: 0.027159966528 \n",
      "Epoch:  162 | loss: 0.027159968391 \n",
      "Epoch:  163 | loss: 0.027159962803 \n",
      "Epoch:  164 | loss: 0.027159960940 \n",
      "Epoch:  165 | loss: 0.027159960940 \n",
      "Epoch:  166 | loss: 0.027159960940 \n",
      "Epoch:  167 | loss: 0.027159962803 \n",
      "Epoch:  168 | loss: 0.027159959078 \n",
      "Epoch:  169 | loss: 0.027159955353 \n",
      "Epoch:  170 | loss: 0.027159955353 \n",
      "Epoch:  171 | loss: 0.027159953490 \n",
      "Epoch:  172 | loss: 0.027159951627 \n",
      "Epoch:  173 | loss: 0.027159951627 \n",
      "Epoch:  174 | loss: 0.027159953490 \n",
      "Epoch:  175 | loss: 0.027159949765 \n",
      "Epoch:  176 | loss: 0.027159947902 \n",
      "Epoch:  177 | loss: 0.027159946039 \n",
      "Epoch:  178 | loss: 0.027159946039 \n",
      "Epoch:  179 | loss: 0.027159947902 \n",
      "Epoch:  180 | loss: 0.027159946039 \n",
      "Epoch:  181 | loss: 0.027159947902 \n",
      "Epoch:  182 | loss: 0.027159946039 \n",
      "Epoch:  183 | loss: 0.027159949765 \n",
      "Epoch:  184 | loss: 0.027159946039 \n",
      "Epoch:  185 | loss: 0.027159946039 \n",
      "Epoch:  186 | loss: 0.027159946039 \n",
      "Epoch:  187 | loss: 0.027159944177 \n",
      "Epoch:  188 | loss: 0.027159944177 \n",
      "Epoch:  189 | loss: 0.027159940451 \n",
      "Epoch:  190 | loss: 0.027159940451 \n",
      "Epoch:  191 | loss: 0.027159942314 \n",
      "Epoch:  192 | loss: 0.027159942314 \n",
      "Epoch:  193 | loss: 0.027159936726 \n",
      "Epoch:  194 | loss: 0.027159938589 \n",
      "Epoch:  195 | loss: 0.027159936726 \n",
      "Epoch:  196 | loss: 0.027159936726 \n",
      "Epoch:  197 | loss: 0.027159938589 \n",
      "Epoch:  198 | loss: 0.027159936726 \n",
      "Epoch:  199 | loss: 0.027159936726 \n",
      "Epoch:  200 | loss: 0.027159936726 \n",
      "Epoch:  201 | loss: 0.027159938589 \n",
      "Epoch:  202 | loss: 0.027159936726 \n",
      "Epoch:  203 | loss: 0.027159934863 \n",
      "Epoch:  204 | loss: 0.027159936726 \n",
      "Epoch:  205 | loss: 0.027159936726 \n",
      "Epoch:  206 | loss: 0.027159936726 \n",
      "Epoch:  207 | loss: 0.027159936726 \n",
      "Epoch:  208 | loss: 0.027159936726 \n",
      "Epoch:  209 | loss: 0.027159936726 \n",
      "Epoch:  210 | loss: 0.027159936726 \n",
      "Epoch:  211 | loss: 0.027159934863 \n",
      "Epoch:  212 | loss: 0.027159936726 \n",
      "Epoch:  213 | loss: 0.027159936726 \n",
      "Epoch:  214 | loss: 0.027159936726 \n",
      "Epoch:  215 | loss: 0.027159936726 \n",
      "Epoch:  216 | loss: 0.027159936726 \n",
      "Epoch:  217 | loss: 0.027159936726 \n",
      "Epoch:  218 | loss: 0.027159934863 \n",
      "Epoch:  219 | loss: 0.027159936726 \n",
      "Epoch:  220 | loss: 0.027159936726 \n",
      "Epoch:  221 | loss: 0.027159936726 \n",
      "Epoch:  222 | loss: 0.027159936726 \n",
      "Epoch:  223 | loss: 0.027159936726 \n",
      "Epoch:  224 | loss: 0.027159936726 \n",
      "Epoch:  225 | loss: 0.027159936726 \n",
      "Epoch:  226 | loss: 0.027159936726 \n",
      "Epoch:  227 | loss: 0.027159936726 \n",
      "Epoch:  228 | loss: 0.027159936726 \n",
      "Epoch:  229 | loss: 0.027159936726 \n",
      "Epoch:  230 | loss: 0.027159936726 \n",
      "Epoch:  231 | loss: 0.027159936726 \n",
      "Epoch:  232 | loss: 0.027159936726 \n",
      "Epoch:  233 | loss: 0.027159936726 \n",
      "Epoch:  234 | loss: 0.027159936726 \n",
      "Epoch:  235 | loss: 0.027159936726 \n",
      "Epoch:  236 | loss: 0.027159936726 \n",
      "Epoch:  237 | loss: 0.027159936726 \n",
      "Epoch:  238 | loss: 0.027159936726 \n",
      "Epoch:  239 | loss: 0.027159936726 \n",
      "Epoch:  240 | loss: 0.027159934863 \n",
      "Epoch:  241 | loss: 0.027159936726 \n",
      "Epoch:  242 | loss: 0.027159936726 \n",
      "Epoch:  243 | loss: 0.027159936726 \n",
      "Epoch:  244 | loss: 0.027159936726 \n",
      "Epoch:  245 | loss: 0.027159936726 \n",
      "Epoch:  246 | loss: 0.027159936726 \n",
      "Epoch:  247 | loss: 0.027159936726 \n",
      "Epoch:  248 | loss: 0.027159936726 \n",
      "Epoch:  249 | loss: 0.027159936726 \n",
      "Epoch:  250 | loss: 0.027159936726 \n",
      "Epoch:  251 | loss: 0.027159934863 \n",
      "Epoch:  252 | loss: 0.027159934863 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  253 | loss: 0.027159936726 \n",
      "Epoch:  254 | loss: 0.027159933001 \n",
      "Epoch:  255 | loss: 0.027159934863 \n",
      "Epoch:  256 | loss: 0.027159934863 \n",
      "Epoch:  257 | loss: 0.027159933001 \n",
      "Epoch:  258 | loss: 0.027159934863 \n",
      "Epoch:  259 | loss: 0.027159934863 \n",
      "Epoch:  260 | loss: 0.027159933001 \n",
      "Epoch:  261 | loss: 0.027159936726 \n",
      "Epoch:  262 | loss: 0.027159934863 \n",
      "Epoch:  263 | loss: 0.027159933001 \n",
      "Epoch:  264 | loss: 0.027159934863 \n",
      "Epoch:  265 | loss: 0.027159931138 \n",
      "Epoch:  266 | loss: 0.027159933001 \n",
      "Epoch:  267 | loss: 0.027159933001 \n",
      "Epoch:  268 | loss: 0.027159933001 \n",
      "Epoch:  269 | loss: 0.027159931138 \n",
      "Epoch:  270 | loss: 0.027159933001 \n",
      "Epoch:  271 | loss: 0.027159934863 \n",
      "Epoch:  272 | loss: 0.027159933001 \n",
      "Epoch:  273 | loss: 0.027159931138 \n",
      "Epoch:  274 | loss: 0.027159934863 \n",
      "Epoch:  275 | loss: 0.027159933001 \n",
      "Epoch:  276 | loss: 0.027159931138 \n",
      "Epoch:  277 | loss: 0.027159934863 \n",
      "Epoch:  278 | loss: 0.027159933001 \n",
      "Epoch:  279 | loss: 0.027159934863 \n",
      "Epoch:  280 | loss: 0.027159933001 \n",
      "Epoch:  281 | loss: 0.027159931138 \n",
      "Epoch:  282 | loss: 0.027159933001 \n",
      "Epoch:  283 | loss: 0.027159931138 \n",
      "Epoch:  284 | loss: 0.027159933001 \n",
      "Epoch:  285 | loss: 0.027159931138 \n",
      "Epoch:  286 | loss: 0.027159931138 \n",
      "Epoch:  287 | loss: 0.027159933001 \n",
      "Epoch:  288 | loss: 0.027159931138 \n",
      "Epoch:  289 | loss: 0.027159933001 \n",
      "Epoch:  290 | loss: 0.027159933001 \n",
      "Epoch:  291 | loss: 0.027159933001 \n",
      "Epoch:  292 | loss: 0.027159929276 \n",
      "Epoch:  293 | loss: 0.027159931138 \n",
      "Epoch:  294 | loss: 0.027159931138 \n",
      "Epoch:  295 | loss: 0.027159931138 \n",
      "Epoch:  296 | loss: 0.027159931138 \n",
      "Epoch:  297 | loss: 0.027159931138 \n",
      "Epoch:  298 | loss: 0.027159931138 \n",
      "Epoch:  299 | loss: 0.027159929276 \n",
      "time=168.73479866981506\n",
      "best_train_loss=0.027159929275512695\n",
      "test_loss=0.026117268949747086\n",
      "best_epoch=292\n",
      "w=40\n",
      "a_kerr=0.0\n",
      "M=1.0\n",
      "r_c=2.0\n",
      "Epoch:  0 | loss: 0.027159931138 \n",
      "Epoch:  1 | loss: 0.027159918100 \n",
      "Epoch:  2 | loss: 0.027159849182 \n",
      "Epoch:  3 | loss: 0.027159789577 \n",
      "Epoch:  4 | loss: 0.027159765363 \n",
      "Epoch:  5 | loss: 0.027159698308 \n",
      "Epoch:  6 | loss: 0.027159690857 \n",
      "Epoch:  7 | loss: 0.027159690857 \n",
      "Epoch:  8 | loss: 0.027159688994 \n",
      "Epoch:  9 | loss: 0.027159657329 \n",
      "Epoch:  10 | loss: 0.027159655467 \n",
      "Epoch:  11 | loss: 0.027159627527 \n",
      "Epoch:  12 | loss: 0.027159614488 \n",
      "Epoch:  13 | loss: 0.027159603313 \n",
      "Epoch:  14 | loss: 0.027159595862 \n",
      "Epoch:  15 | loss: 0.027159595862 \n",
      "Epoch:  16 | loss: 0.027159595862 \n",
      "Epoch:  17 | loss: 0.027159595862 \n",
      "Epoch:  18 | loss: 0.027159592137 \n",
      "Epoch:  19 | loss: 0.027159580961 \n",
      "Epoch:  20 | loss: 0.027159577236 \n",
      "Epoch:  21 | loss: 0.027159564197 \n",
      "Epoch:  22 | loss: 0.027159526944 \n",
      "Epoch:  23 | loss: 0.027159510180 \n",
      "Epoch:  24 | loss: 0.027159504592 \n",
      "Epoch:  25 | loss: 0.027159463614 \n",
      "Epoch:  26 | loss: 0.027159459889 \n",
      "Epoch:  27 | loss: 0.027159437537 \n",
      "Epoch:  28 | loss: 0.027159435675 \n",
      "Epoch:  29 | loss: 0.027159351856 \n",
      "Epoch:  30 | loss: 0.027159348130 \n",
      "Epoch:  31 | loss: 0.027159316465 \n",
      "Epoch:  32 | loss: 0.027159305289 \n",
      "Epoch:  33 | loss: 0.027159299701 \n",
      "Epoch:  34 | loss: 0.027159292251 \n",
      "Epoch:  35 | loss: 0.027159294114 \n",
      "Epoch:  36 | loss: 0.027159286663 \n",
      "Epoch:  37 | loss: 0.027159281075 \n",
      "Epoch:  38 | loss: 0.027159238234 \n",
      "Epoch:  39 | loss: 0.027159219608 \n",
      "Epoch:  40 | loss: 0.027159195393 \n",
      "Epoch:  41 | loss: 0.027159197256 \n",
      "Epoch:  42 | loss: 0.027159180492 \n",
      "Epoch:  43 | loss: 0.027159176767 \n",
      "Epoch:  44 | loss: 0.027159167454 \n",
      "Epoch:  45 | loss: 0.027159104124 \n",
      "Epoch:  46 | loss: 0.027159089223 \n",
      "Epoch:  47 | loss: 0.027159031481 \n",
      "Epoch:  48 | loss: 0.027159020305 \n",
      "Epoch:  49 | loss: 0.027159016579 \n",
      "Epoch:  50 | loss: 0.027158945799 \n",
      "Epoch:  51 | loss: 0.027158942074 \n",
      "Epoch:  52 | loss: 0.027158938348 \n",
      "Epoch:  53 | loss: 0.027158917859 \n",
      "Epoch:  54 | loss: 0.027158880606 \n",
      "Epoch:  55 | loss: 0.027158869430 \n",
      "Epoch:  56 | loss: 0.027158847079 \n",
      "Epoch:  57 | loss: 0.027158813551 \n",
      "Epoch:  58 | loss: 0.027158772573 \n",
      "Epoch:  59 | loss: 0.027158770710 \n",
      "Epoch:  60 | loss: 0.027158740908 \n",
      "Epoch:  61 | loss: 0.027158731595 \n",
      "Epoch:  62 | loss: 0.027158733457 \n",
      "Epoch:  63 | loss: 0.027158705518 \n",
      "Epoch:  64 | loss: 0.027158694342 \n",
      "Epoch:  65 | loss: 0.027158690616 \n",
      "Epoch:  66 | loss: 0.027158632874 \n",
      "Epoch:  67 | loss: 0.027158632874 \n",
      "Epoch:  68 | loss: 0.027158619836 \n",
      "Epoch:  69 | loss: 0.027158621699 \n",
      "Epoch:  70 | loss: 0.027158582583 \n",
      "Epoch:  71 | loss: 0.027158562094 \n",
      "Epoch:  72 | loss: 0.027158558369 \n",
      "Epoch:  73 | loss: 0.027158560231 \n",
      "Epoch:  74 | loss: 0.027158536017 \n",
      "Epoch:  75 | loss: 0.027158463374 \n",
      "Epoch:  76 | loss: 0.027158461511 \n",
      "Epoch:  77 | loss: 0.027158439159 \n",
      "Epoch:  78 | loss: 0.027158433571 \n",
      "Epoch:  79 | loss: 0.027158433571 \n",
      "Epoch:  80 | loss: 0.027158338577 \n",
      "Epoch:  81 | loss: 0.027158293873 \n",
      "Epoch:  82 | loss: 0.027158288285 \n",
      "Epoch:  83 | loss: 0.027158252895 \n",
      "Epoch:  84 | loss: 0.027158230543 \n",
      "Epoch:  85 | loss: 0.027158202603 \n",
      "Epoch:  86 | loss: 0.027158169076 \n",
      "Epoch:  87 | loss: 0.027158165351 \n",
      "Epoch:  88 | loss: 0.027158154175 \n",
      "Epoch:  89 | loss: 0.027158154175 \n",
      "Epoch:  90 | loss: 0.027158154175 \n",
      "Epoch:  91 | loss: 0.027158111334 \n",
      "Epoch:  92 | loss: 0.027158107609 \n",
      "Epoch:  93 | loss: 0.027158111334 \n",
      "Epoch:  94 | loss: 0.027158105746 \n",
      "Epoch:  95 | loss: 0.027158103883 \n",
      "Epoch:  96 | loss: 0.027158098295 \n",
      "Epoch:  97 | loss: 0.027158074081 \n",
      "Epoch:  98 | loss: 0.027158048004 \n",
      "Epoch:  99 | loss: 0.027157997712 \n",
      "Epoch:  100 | loss: 0.027157997712 \n",
      "Epoch:  101 | loss: 0.027157997712 \n",
      "Epoch:  102 | loss: 0.027157995850 \n",
      "Epoch:  103 | loss: 0.027157995850 \n",
      "Epoch:  104 | loss: 0.027157995850 \n",
      "Epoch:  105 | loss: 0.027157995850 \n",
      "Epoch:  106 | loss: 0.027157993987 \n",
      "Epoch:  107 | loss: 0.027157997712 \n",
      "Epoch:  108 | loss: 0.027157993987 \n",
      "Epoch:  109 | loss: 0.027157995850 \n",
      "Epoch:  110 | loss: 0.027157995850 \n",
      "Epoch:  111 | loss: 0.027157993987 \n",
      "Epoch:  112 | loss: 0.027157992125 \n",
      "Epoch:  113 | loss: 0.027157992125 \n",
      "Epoch:  114 | loss: 0.027157992125 \n",
      "Epoch:  115 | loss: 0.027157992125 \n",
      "Epoch:  116 | loss: 0.027157995850 \n",
      "Epoch:  117 | loss: 0.027157992125 \n",
      "Epoch:  118 | loss: 0.027157993987 \n",
      "Epoch:  119 | loss: 0.027157992125 \n",
      "Epoch:  120 | loss: 0.027157995850 \n",
      "Epoch:  121 | loss: 0.027157992125 \n",
      "Epoch:  122 | loss: 0.027157992125 \n",
      "Epoch:  123 | loss: 0.027157992125 \n",
      "Epoch:  124 | loss: 0.027157992125 \n",
      "Epoch:  125 | loss: 0.027157992125 \n",
      "Epoch:  126 | loss: 0.027157992125 \n",
      "Epoch:  127 | loss: 0.027157992125 \n",
      "Epoch:  128 | loss: 0.027157992125 \n",
      "Epoch:  129 | loss: 0.027157992125 \n",
      "Epoch:  130 | loss: 0.027157990262 \n",
      "Epoch:  131 | loss: 0.027157988399 \n",
      "Epoch:  132 | loss: 0.027157992125 \n",
      "Epoch:  133 | loss: 0.027157990262 \n",
      "Epoch:  134 | loss: 0.027157990262 \n",
      "Epoch:  135 | loss: 0.027157990262 \n",
      "Epoch:  136 | loss: 0.027157992125 \n",
      "Epoch:  137 | loss: 0.027157990262 \n",
      "Epoch:  138 | loss: 0.027157988399 \n",
      "Epoch:  139 | loss: 0.027157990262 \n",
      "Epoch:  140 | loss: 0.027157990262 \n",
      "Epoch:  141 | loss: 0.027157990262 \n",
      "Epoch:  142 | loss: 0.027157990262 \n",
      "Epoch:  143 | loss: 0.027157988399 \n",
      "Epoch:  144 | loss: 0.027157988399 \n",
      "Epoch:  145 | loss: 0.027157986537 \n",
      "Epoch:  146 | loss: 0.027157986537 \n",
      "Epoch:  147 | loss: 0.027157988399 \n",
      "Epoch:  148 | loss: 0.027157988399 \n",
      "Epoch:  149 | loss: 0.027157986537 \n",
      "Epoch:  150 | loss: 0.027157984674 \n",
      "Epoch:  151 | loss: 0.027157986537 \n",
      "Epoch:  152 | loss: 0.027157986537 \n",
      "Epoch:  153 | loss: 0.027157986537 \n",
      "Epoch:  154 | loss: 0.027157984674 \n",
      "Epoch:  155 | loss: 0.027157984674 \n",
      "Epoch:  156 | loss: 0.027157986537 \n",
      "Epoch:  157 | loss: 0.027157982811 \n",
      "Epoch:  158 | loss: 0.027157982811 \n",
      "Epoch:  159 | loss: 0.027157982811 \n",
      "Epoch:  160 | loss: 0.027157982811 \n",
      "Epoch:  161 | loss: 0.027157984674 \n",
      "Epoch:  162 | loss: 0.027157982811 \n",
      "Epoch:  163 | loss: 0.027157982811 \n",
      "Epoch:  164 | loss: 0.027157982811 \n",
      "Epoch:  165 | loss: 0.027157982811 \n",
      "Epoch:  166 | loss: 0.027157982811 \n",
      "Epoch:  167 | loss: 0.027157982811 \n",
      "Epoch:  168 | loss: 0.027157982811 \n",
      "Epoch:  169 | loss: 0.027157982811 \n",
      "Epoch:  170 | loss: 0.027157982811 \n",
      "Epoch:  171 | loss: 0.027157982811 \n",
      "Epoch:  172 | loss: 0.027157982811 \n",
      "Epoch:  173 | loss: 0.027157982811 \n",
      "Epoch:  174 | loss: 0.027157982811 \n",
      "Epoch:  175 | loss: 0.027157982811 \n",
      "Epoch:  176 | loss: 0.027157982811 \n",
      "Epoch:  177 | loss: 0.027157982811 \n",
      "Epoch:  178 | loss: 0.027157984674 \n",
      "Epoch:  179 | loss: 0.027157982811 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  180 | loss: 0.027157982811 \n",
      "Epoch:  181 | loss: 0.027157980949 \n",
      "Epoch:  182 | loss: 0.027157982811 \n",
      "Epoch:  183 | loss: 0.027157980949 \n",
      "Epoch:  184 | loss: 0.027157980949 \n",
      "Epoch:  185 | loss: 0.027157980949 \n",
      "Epoch:  186 | loss: 0.027157979086 \n",
      "Epoch:  187 | loss: 0.027157980949 \n",
      "Epoch:  188 | loss: 0.027157980949 \n",
      "Epoch:  189 | loss: 0.027157979086 \n",
      "Epoch:  190 | loss: 0.027157980949 \n",
      "Epoch:  191 | loss: 0.027157977223 \n",
      "Epoch:  192 | loss: 0.027157979086 \n",
      "Epoch:  193 | loss: 0.027157977223 \n",
      "Epoch:  194 | loss: 0.027157979086 \n",
      "Epoch:  195 | loss: 0.027157977223 \n",
      "Epoch:  196 | loss: 0.027157975361 \n",
      "Epoch:  197 | loss: 0.027157977223 \n",
      "Epoch:  198 | loss: 0.027157975361 \n",
      "Epoch:  199 | loss: 0.027157973498 \n",
      "Epoch:  200 | loss: 0.027157973498 \n",
      "Epoch:  201 | loss: 0.027157973498 \n",
      "Epoch:  202 | loss: 0.027157971635 \n",
      "Epoch:  203 | loss: 0.027157973498 \n",
      "Epoch:  204 | loss: 0.027157975361 \n",
      "Epoch:  205 | loss: 0.027157975361 \n",
      "Epoch:  206 | loss: 0.027157973498 \n",
      "Epoch:  207 | loss: 0.027157973498 \n",
      "Epoch:  208 | loss: 0.027157975361 \n",
      "Epoch:  209 | loss: 0.027157975361 \n",
      "Epoch:  210 | loss: 0.027157975361 \n",
      "Epoch:  211 | loss: 0.027157975361 \n",
      "Epoch:  212 | loss: 0.027157975361 \n",
      "Epoch:  213 | loss: 0.027157975361 \n",
      "Epoch:  214 | loss: 0.027157975361 \n",
      "Epoch:  215 | loss: 0.027157971635 \n",
      "Epoch:  216 | loss: 0.027157973498 \n",
      "Epoch:  217 | loss: 0.027157971635 \n",
      "Epoch:  218 | loss: 0.027157975361 \n",
      "Epoch:  219 | loss: 0.027157973498 \n",
      "Epoch:  220 | loss: 0.027157975361 \n",
      "Epoch:  221 | loss: 0.027157973498 \n",
      "Epoch:  222 | loss: 0.027157971635 \n",
      "Epoch:  223 | loss: 0.027157973498 \n",
      "Epoch:  224 | loss: 0.027157973498 \n",
      "Epoch:  225 | loss: 0.027157973498 \n",
      "Epoch:  226 | loss: 0.027157975361 \n",
      "Epoch:  227 | loss: 0.027157975361 \n",
      "Epoch:  228 | loss: 0.027157973498 \n",
      "Epoch:  229 | loss: 0.027157975361 \n",
      "Epoch:  230 | loss: 0.027157973498 \n",
      "Epoch:  231 | loss: 0.027157971635 \n",
      "Epoch:  232 | loss: 0.027157971635 \n",
      "Epoch:  233 | loss: 0.027157973498 \n",
      "Epoch:  234 | loss: 0.027157973498 \n",
      "Epoch:  235 | loss: 0.027157971635 \n",
      "Epoch:  236 | loss: 0.027157973498 \n",
      "Epoch:  237 | loss: 0.027157973498 \n",
      "Epoch:  238 | loss: 0.027157971635 \n",
      "Epoch:  239 | loss: 0.027157973498 \n",
      "Epoch:  240 | loss: 0.027157973498 \n",
      "Epoch:  241 | loss: 0.027157973498 \n",
      "Epoch:  242 | loss: 0.027157973498 \n",
      "Epoch:  243 | loss: 0.027157975361 \n",
      "Epoch:  244 | loss: 0.027157973498 \n",
      "Epoch:  245 | loss: 0.027157971635 \n",
      "Epoch:  246 | loss: 0.027157973498 \n",
      "Epoch:  247 | loss: 0.027157971635 \n",
      "Epoch:  248 | loss: 0.027157973498 \n",
      "Epoch:  249 | loss: 0.027157973498 \n",
      "Epoch:  250 | loss: 0.027157971635 \n",
      "Epoch:  251 | loss: 0.027157975361 \n",
      "Epoch:  252 | loss: 0.027157971635 \n",
      "Epoch:  253 | loss: 0.027157973498 \n",
      "Epoch:  254 | loss: 0.027157971635 \n",
      "Epoch:  255 | loss: 0.027157971635 \n",
      "Epoch:  256 | loss: 0.027157971635 \n",
      "Epoch:  257 | loss: 0.027157973498 \n",
      "Epoch:  258 | loss: 0.027157973498 \n",
      "Epoch:  259 | loss: 0.027157975361 \n",
      "Epoch:  260 | loss: 0.027157971635 \n",
      "Epoch:  261 | loss: 0.027157969773 \n",
      "Epoch:  262 | loss: 0.027157971635 \n",
      "Epoch:  263 | loss: 0.027157973498 \n",
      "Epoch:  264 | loss: 0.027157971635 \n",
      "Epoch:  265 | loss: 0.027157971635 \n",
      "Epoch:  266 | loss: 0.027157969773 \n",
      "Epoch:  267 | loss: 0.027157971635 \n",
      "Epoch:  268 | loss: 0.027157971635 \n",
      "Epoch:  269 | loss: 0.027157971635 \n",
      "Epoch:  270 | loss: 0.027157971635 \n",
      "Epoch:  271 | loss: 0.027157971635 \n",
      "Epoch:  272 | loss: 0.027157971635 \n",
      "Epoch:  273 | loss: 0.027157973498 \n",
      "Epoch:  274 | loss: 0.027157971635 \n",
      "Epoch:  275 | loss: 0.027157969773 \n",
      "Epoch:  276 | loss: 0.027157971635 \n",
      "Epoch:  277 | loss: 0.027157971635 \n",
      "Epoch:  278 | loss: 0.027157971635 \n",
      "Epoch:  279 | loss: 0.027157971635 \n",
      "Epoch:  280 | loss: 0.027157969773 \n",
      "Epoch:  281 | loss: 0.027157971635 \n",
      "Epoch:  282 | loss: 0.027157969773 \n",
      "Epoch:  283 | loss: 0.027157969773 \n",
      "Epoch:  284 | loss: 0.027157971635 \n",
      "Epoch:  285 | loss: 0.027157971635 \n",
      "Epoch:  286 | loss: 0.027157971635 \n",
      "Epoch:  287 | loss: 0.027157971635 \n",
      "Epoch:  288 | loss: 0.027157971635 \n",
      "Epoch:  289 | loss: 0.027157971635 \n",
      "Epoch:  290 | loss: 0.027157971635 \n",
      "Epoch:  291 | loss: 0.027157969773 \n",
      "Epoch:  292 | loss: 0.027157971635 \n",
      "Epoch:  293 | loss: 0.027157971635 \n",
      "Epoch:  294 | loss: 0.027157971635 \n",
      "Epoch:  295 | loss: 0.027157971635 \n",
      "Epoch:  296 | loss: 0.027157971635 \n",
      "Epoch:  297 | loss: 0.027157971635 \n",
      "Epoch:  298 | loss: 0.027157971635 \n",
      "Epoch:  299 | loss: 0.027157971635 \n",
      "time=79.64194631576538\n",
      "best_train_loss=0.027157969772815704\n",
      "test_loss=0.026115577667951584\n",
      "best_epoch=261\n",
      "w=45\n",
      "a_kerr=0.0\n",
      "M=1.0\n",
      "r_c=2.0\n",
      "Epoch:  0 | loss: 0.027157971635 \n",
      "Epoch:  1 | loss: 0.027157962322 \n",
      "Epoch:  2 | loss: 0.027157951146 \n",
      "Epoch:  3 | loss: 0.027157934383 \n",
      "Epoch:  4 | loss: 0.027157934383 \n",
      "Epoch:  5 | loss: 0.027157934383 \n",
      "Epoch:  6 | loss: 0.027157934383 \n",
      "Epoch:  7 | loss: 0.027157936245 \n",
      "Epoch:  8 | loss: 0.027157925069 \n",
      "Epoch:  9 | loss: 0.027157897130 \n",
      "Epoch:  10 | loss: 0.027157859877 \n",
      "Epoch:  11 | loss: 0.027157854289 \n",
      "Epoch:  12 | loss: 0.027157846838 \n",
      "Epoch:  13 | loss: 0.027157789096 \n",
      "Epoch:  14 | loss: 0.027157763019 \n",
      "Epoch:  15 | loss: 0.027157764882 \n",
      "Epoch:  16 | loss: 0.027157759294 \n",
      "Epoch:  17 | loss: 0.027157744393 \n",
      "Epoch:  18 | loss: 0.027157709002 \n",
      "Epoch:  19 | loss: 0.027157675475 \n",
      "Epoch:  20 | loss: 0.027157677338 \n",
      "Epoch:  21 | loss: 0.027157669887 \n",
      "Epoch:  22 | loss: 0.027157669887 \n",
      "Epoch:  23 | loss: 0.027157669887 \n",
      "Epoch:  24 | loss: 0.027157653123 \n",
      "Epoch:  25 | loss: 0.027157632634 \n",
      "Epoch:  26 | loss: 0.027157615870 \n",
      "Epoch:  27 | loss: 0.027157610282 \n",
      "Epoch:  28 | loss: 0.027157610282 \n",
      "Epoch:  29 | loss: 0.027157610282 \n",
      "Epoch:  30 | loss: 0.027157610282 \n",
      "Epoch:  31 | loss: 0.027157595381 \n",
      "Epoch:  32 | loss: 0.027157582343 \n",
      "Epoch:  33 | loss: 0.027157580480 \n",
      "Epoch:  34 | loss: 0.027157565579 \n",
      "Epoch:  35 | loss: 0.027157550678 \n",
      "Epoch:  36 | loss: 0.027157502249 \n",
      "Epoch:  37 | loss: 0.027157496661 \n",
      "Epoch:  38 | loss: 0.027157500386 \n",
      "Epoch:  39 | loss: 0.027157483622 \n",
      "Epoch:  40 | loss: 0.027157478034 \n",
      "Epoch:  41 | loss: 0.027157479897 \n",
      "Epoch:  42 | loss: 0.027157474309 \n",
      "Epoch:  43 | loss: 0.027157466859 \n",
      "Epoch:  44 | loss: 0.027157424018 \n",
      "Epoch:  45 | loss: 0.027157407254 \n",
      "Epoch:  46 | loss: 0.027157360688 \n",
      "Epoch:  47 | loss: 0.027157355100 \n",
      "Epoch:  48 | loss: 0.027157353237 \n",
      "Epoch:  49 | loss: 0.027157329023 \n",
      "Epoch:  50 | loss: 0.027157329023 \n",
      "Epoch:  51 | loss: 0.027157329023 \n",
      "Epoch:  52 | loss: 0.027157273144 \n",
      "Epoch:  53 | loss: 0.027157271281 \n",
      "Epoch:  54 | loss: 0.027157258242 \n",
      "Epoch:  55 | loss: 0.027157241479 \n",
      "Epoch:  56 | loss: 0.027157206088 \n",
      "Epoch:  57 | loss: 0.027157180011 \n",
      "Epoch:  58 | loss: 0.027157159522 \n",
      "Epoch:  59 | loss: 0.027157142758 \n",
      "Epoch:  60 | loss: 0.027157135308 \n",
      "Epoch:  61 | loss: 0.027157105505 \n",
      "Epoch:  62 | loss: 0.027157105505 \n",
      "Epoch:  63 | loss: 0.027157066390 \n",
      "Epoch:  64 | loss: 0.027156990021 \n",
      "Epoch:  65 | loss: 0.027156990021 \n",
      "Epoch:  66 | loss: 0.027156991884 \n",
      "Epoch:  67 | loss: 0.027156971395 \n",
      "Epoch:  68 | loss: 0.027156934142 \n",
      "Epoch:  69 | loss: 0.027156917378 \n",
      "Epoch:  70 | loss: 0.027156915516 \n",
      "Epoch:  71 | loss: 0.027156913653 \n",
      "Epoch:  72 | loss: 0.027156887576 \n",
      "Epoch:  73 | loss: 0.027156889439 \n",
      "Epoch:  74 | loss: 0.027156874537 \n",
      "Epoch:  75 | loss: 0.027156874537 \n",
      "Epoch:  76 | loss: 0.027156865224 \n",
      "Epoch:  77 | loss: 0.027156857774 \n",
      "Epoch:  78 | loss: 0.027156857774 \n",
      "Epoch:  79 | loss: 0.027156857774 \n",
      "Epoch:  80 | loss: 0.027156839147 \n",
      "Epoch:  81 | loss: 0.027156839147 \n",
      "Epoch:  82 | loss: 0.027156839147 \n",
      "Epoch:  83 | loss: 0.027156841010 \n",
      "Epoch:  84 | loss: 0.027156824246 \n",
      "Epoch:  85 | loss: 0.027156827971 \n",
      "Epoch:  86 | loss: 0.027156805620 \n",
      "Epoch:  87 | loss: 0.027156803757 \n",
      "Epoch:  88 | loss: 0.027156801894 \n",
      "Epoch:  89 | loss: 0.027156792581 \n",
      "Epoch:  90 | loss: 0.027156786993 \n",
      "Epoch:  91 | loss: 0.027156764641 \n",
      "Epoch:  92 | loss: 0.027156766504 \n",
      "Epoch:  93 | loss: 0.027156764641 \n",
      "Epoch:  94 | loss: 0.027156727389 \n",
      "Epoch:  95 | loss: 0.027156688273 \n",
      "Epoch:  96 | loss: 0.027156691998 \n",
      "Epoch:  97 | loss: 0.027156652883 \n",
      "Epoch:  98 | loss: 0.027156647295 \n",
      "Epoch:  99 | loss: 0.027156639844 \n",
      "Epoch:  100 | loss: 0.027156641707 \n",
      "Epoch:  101 | loss: 0.027156639844 \n",
      "Epoch:  102 | loss: 0.027156639844 \n",
      "Epoch:  103 | loss: 0.027156639844 \n",
      "Epoch:  104 | loss: 0.027156639844 \n",
      "Epoch:  105 | loss: 0.027156641707 \n",
      "Epoch:  106 | loss: 0.027156641707 \n",
      "Epoch:  107 | loss: 0.027156639844 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  108 | loss: 0.027156639844 \n",
      "Epoch:  109 | loss: 0.027156637982 \n",
      "Epoch:  110 | loss: 0.027156643569 \n",
      "Epoch:  111 | loss: 0.027156639844 \n",
      "Epoch:  112 | loss: 0.027156636119 \n",
      "Epoch:  113 | loss: 0.027156639844 \n",
      "Epoch:  114 | loss: 0.027156636119 \n",
      "Epoch:  115 | loss: 0.027156636119 \n",
      "Epoch:  116 | loss: 0.027156636119 \n",
      "Epoch:  117 | loss: 0.027156637982 \n",
      "Epoch:  118 | loss: 0.027156634256 \n",
      "Epoch:  119 | loss: 0.027156632394 \n",
      "Epoch:  120 | loss: 0.027156632394 \n",
      "Epoch:  121 | loss: 0.027156632394 \n",
      "Epoch:  122 | loss: 0.027156632394 \n",
      "Epoch:  123 | loss: 0.027156632394 \n",
      "Epoch:  124 | loss: 0.027156632394 \n",
      "Epoch:  125 | loss: 0.027156632394 \n",
      "Epoch:  126 | loss: 0.027156634256 \n",
      "Epoch:  127 | loss: 0.027156632394 \n",
      "Epoch:  128 | loss: 0.027156630531 \n",
      "Epoch:  129 | loss: 0.027156632394 \n",
      "Epoch:  130 | loss: 0.027156632394 \n",
      "Epoch:  131 | loss: 0.027156630531 \n",
      "Epoch:  132 | loss: 0.027156630531 \n",
      "Epoch:  133 | loss: 0.027156630531 \n",
      "Epoch:  134 | loss: 0.027156632394 \n",
      "Epoch:  135 | loss: 0.027156626806 \n",
      "Epoch:  136 | loss: 0.027156630531 \n",
      "Epoch:  137 | loss: 0.027156626806 \n",
      "Epoch:  138 | loss: 0.027156626806 \n",
      "Epoch:  139 | loss: 0.027156626806 \n",
      "Epoch:  140 | loss: 0.027156626806 \n",
      "Epoch:  141 | loss: 0.027156628668 \n",
      "Epoch:  142 | loss: 0.027156626806 \n",
      "Epoch:  143 | loss: 0.027156624943 \n",
      "Epoch:  144 | loss: 0.027156626806 \n",
      "Epoch:  145 | loss: 0.027156628668 \n",
      "Epoch:  146 | loss: 0.027156626806 \n",
      "Epoch:  147 | loss: 0.027156624943 \n",
      "Epoch:  148 | loss: 0.027156624943 \n",
      "Epoch:  149 | loss: 0.027156626806 \n",
      "Epoch:  150 | loss: 0.027156623080 \n",
      "Epoch:  151 | loss: 0.027156623080 \n",
      "Epoch:  152 | loss: 0.027156623080 \n",
      "Epoch:  153 | loss: 0.027156624943 \n",
      "Epoch:  154 | loss: 0.027156623080 \n",
      "Epoch:  155 | loss: 0.027156623080 \n",
      "Epoch:  156 | loss: 0.027156626806 \n",
      "Epoch:  157 | loss: 0.027156619355 \n",
      "Epoch:  158 | loss: 0.027156621218 \n",
      "Epoch:  159 | loss: 0.027156619355 \n",
      "Epoch:  160 | loss: 0.027156617492 \n",
      "Epoch:  161 | loss: 0.027156617492 \n",
      "Epoch:  162 | loss: 0.027156615630 \n",
      "Epoch:  163 | loss: 0.027156617492 \n",
      "Epoch:  164 | loss: 0.027156619355 \n",
      "Epoch:  165 | loss: 0.027156617492 \n",
      "Epoch:  166 | loss: 0.027156615630 \n",
      "Epoch:  167 | loss: 0.027156611905 \n",
      "Epoch:  168 | loss: 0.027156615630 \n",
      "Epoch:  169 | loss: 0.027156615630 \n",
      "Epoch:  170 | loss: 0.027156611905 \n",
      "Epoch:  171 | loss: 0.027156613767 \n",
      "Epoch:  172 | loss: 0.027156613767 \n",
      "Epoch:  173 | loss: 0.027156613767 \n",
      "Epoch:  174 | loss: 0.027156611905 \n",
      "Epoch:  175 | loss: 0.027156611905 \n",
      "Epoch:  176 | loss: 0.027156610042 \n",
      "Epoch:  177 | loss: 0.027156610042 \n",
      "Epoch:  178 | loss: 0.027156610042 \n",
      "Epoch:  179 | loss: 0.027156611905 \n",
      "Epoch:  180 | loss: 0.027156606317 \n",
      "Epoch:  181 | loss: 0.027156611905 \n",
      "Epoch:  182 | loss: 0.027156611905 \n",
      "Epoch:  183 | loss: 0.027156610042 \n",
      "Epoch:  184 | loss: 0.027156611905 \n",
      "Epoch:  185 | loss: 0.027156610042 \n",
      "Epoch:  186 | loss: 0.027156611905 \n",
      "Epoch:  187 | loss: 0.027156610042 \n",
      "Epoch:  188 | loss: 0.027156610042 \n",
      "Epoch:  189 | loss: 0.027156606317 \n",
      "Epoch:  190 | loss: 0.027156604454 \n",
      "Epoch:  191 | loss: 0.027156606317 \n",
      "Epoch:  192 | loss: 0.027156606317 \n",
      "Epoch:  193 | loss: 0.027156606317 \n",
      "Epoch:  194 | loss: 0.027156602591 \n",
      "Epoch:  195 | loss: 0.027156604454 \n",
      "Epoch:  196 | loss: 0.027156604454 \n",
      "Epoch:  197 | loss: 0.027156604454 \n",
      "Epoch:  198 | loss: 0.027156600729 \n",
      "Epoch:  199 | loss: 0.027156602591 \n",
      "Epoch:  200 | loss: 0.027156604454 \n",
      "Epoch:  201 | loss: 0.027156600729 \n",
      "Epoch:  202 | loss: 0.027156602591 \n",
      "Epoch:  203 | loss: 0.027156602591 \n",
      "Epoch:  204 | loss: 0.027156602591 \n",
      "Epoch:  205 | loss: 0.027156600729 \n",
      "Epoch:  206 | loss: 0.027156602591 \n",
      "Epoch:  207 | loss: 0.027156602591 \n",
      "Epoch:  208 | loss: 0.027156600729 \n",
      "Epoch:  209 | loss: 0.027156598866 \n",
      "Epoch:  210 | loss: 0.027156602591 \n",
      "Epoch:  211 | loss: 0.027156604454 \n",
      "Epoch:  212 | loss: 0.027156600729 \n",
      "Epoch:  213 | loss: 0.027156602591 \n",
      "Epoch:  214 | loss: 0.027156604454 \n",
      "Epoch:  215 | loss: 0.027156600729 \n",
      "Epoch:  216 | loss: 0.027156602591 \n",
      "Epoch:  217 | loss: 0.027156602591 \n",
      "Epoch:  218 | loss: 0.027156602591 \n",
      "Epoch:  219 | loss: 0.027156598866 \n",
      "Epoch:  220 | loss: 0.027156598866 \n",
      "Epoch:  221 | loss: 0.027156598866 \n",
      "Epoch:  222 | loss: 0.027156600729 \n",
      "Epoch:  223 | loss: 0.027156598866 \n",
      "Epoch:  224 | loss: 0.027156598866 \n",
      "Epoch:  225 | loss: 0.027156598866 \n",
      "Epoch:  226 | loss: 0.027156600729 \n",
      "Epoch:  227 | loss: 0.027156598866 \n",
      "Epoch:  228 | loss: 0.027156598866 \n",
      "Epoch:  229 | loss: 0.027156598866 \n",
      "Epoch:  230 | loss: 0.027156598866 \n",
      "Epoch:  231 | loss: 0.027156598866 \n",
      "Epoch:  232 | loss: 0.027156602591 \n",
      "Epoch:  233 | loss: 0.027156598866 \n",
      "Epoch:  234 | loss: 0.027156602591 \n",
      "Epoch:  235 | loss: 0.027156598866 \n",
      "Epoch:  236 | loss: 0.027156598866 \n",
      "Epoch:  237 | loss: 0.027156598866 \n",
      "Epoch:  238 | loss: 0.027156600729 \n",
      "Epoch:  239 | loss: 0.027156600729 \n",
      "Epoch:  240 | loss: 0.027156600729 \n",
      "Epoch:  241 | loss: 0.027156598866 \n",
      "Epoch:  242 | loss: 0.027156600729 \n",
      "Epoch:  243 | loss: 0.027156598866 \n",
      "Epoch:  244 | loss: 0.027156598866 \n",
      "Epoch:  245 | loss: 0.027156600729 \n",
      "Epoch:  246 | loss: 0.027156602591 \n",
      "Epoch:  247 | loss: 0.027156598866 \n",
      "Epoch:  248 | loss: 0.027156602591 \n",
      "Epoch:  249 | loss: 0.027156598866 \n",
      "Epoch:  250 | loss: 0.027156600729 \n",
      "Epoch:  251 | loss: 0.027156598866 \n",
      "Epoch:  252 | loss: 0.027156598866 \n",
      "Epoch:  253 | loss: 0.027156600729 \n",
      "Epoch:  254 | loss: 0.027156602591 \n",
      "Epoch:  255 | loss: 0.027156598866 \n",
      "Epoch:  256 | loss: 0.027156600729 \n",
      "Epoch:  257 | loss: 0.027156600729 \n",
      "Epoch:  258 | loss: 0.027156598866 \n",
      "Epoch:  259 | loss: 0.027156598866 \n",
      "Epoch:  260 | loss: 0.027156600729 \n",
      "Epoch:  261 | loss: 0.027156602591 \n",
      "Epoch:  262 | loss: 0.027156600729 \n",
      "Epoch:  263 | loss: 0.027156600729 \n",
      "Epoch:  264 | loss: 0.027156600729 \n",
      "Epoch:  265 | loss: 0.027156598866 \n",
      "Epoch:  266 | loss: 0.027156600729 \n",
      "Epoch:  267 | loss: 0.027156598866 \n",
      "Epoch:  268 | loss: 0.027156598866 \n",
      "Epoch:  269 | loss: 0.027156598866 \n",
      "Epoch:  270 | loss: 0.027156598866 \n",
      "Epoch:  271 | loss: 0.027156598866 \n",
      "Epoch:  272 | loss: 0.027156598866 \n",
      "Epoch:  273 | loss: 0.027156598866 \n",
      "Epoch:  274 | loss: 0.027156598866 \n",
      "Epoch:  275 | loss: 0.027156602591 \n",
      "Epoch:  276 | loss: 0.027156600729 \n",
      "Epoch:  277 | loss: 0.027156598866 \n",
      "Epoch:  278 | loss: 0.027156598866 \n",
      "Epoch:  279 | loss: 0.027156598866 \n",
      "Epoch:  280 | loss: 0.027156600729 \n",
      "Epoch:  281 | loss: 0.027156597003 \n",
      "Epoch:  282 | loss: 0.027156598866 \n",
      "Epoch:  283 | loss: 0.027156598866 \n",
      "Epoch:  284 | loss: 0.027156598866 \n",
      "Epoch:  285 | loss: 0.027156598866 \n",
      "Epoch:  286 | loss: 0.027156598866 \n",
      "Epoch:  287 | loss: 0.027156598866 \n",
      "Epoch:  288 | loss: 0.027156597003 \n",
      "Epoch:  289 | loss: 0.027156597003 \n",
      "Epoch:  290 | loss: 0.027156600729 \n",
      "Epoch:  291 | loss: 0.027156598866 \n",
      "Epoch:  292 | loss: 0.027156600729 \n",
      "Epoch:  293 | loss: 0.027156598866 \n",
      "Epoch:  294 | loss: 0.027156598866 \n",
      "Epoch:  295 | loss: 0.027156598866 \n",
      "Epoch:  296 | loss: 0.027156598866 \n",
      "Epoch:  297 | loss: 0.027156600729 \n",
      "Epoch:  298 | loss: 0.027156598866 \n",
      "Epoch:  299 | loss: 0.027156597003 \n",
      "time=72.88952016830444\n",
      "best_train_loss=0.02715659700334072\n",
      "test_loss=0.026114368811249733\n",
      "best_epoch=281\n",
      "w=50\n",
      "a_kerr=0.0\n",
      "M=1.0\n",
      "r_c=2.0\n",
      "Epoch:  0 | loss: 0.027156598866 \n",
      "Epoch:  1 | loss: 0.027156593278 \n",
      "Epoch:  2 | loss: 0.027156585827 \n",
      "Epoch:  3 | loss: 0.027156589553 \n",
      "Epoch:  4 | loss: 0.027156580240 \n",
      "Epoch:  5 | loss: 0.027156576514 \n",
      "Epoch:  6 | loss: 0.027156576514 \n",
      "Epoch:  7 | loss: 0.027156537399 \n",
      "Epoch:  8 | loss: 0.027156522498 \n",
      "Epoch:  9 | loss: 0.027156470343 \n",
      "Epoch:  10 | loss: 0.027156472206 \n",
      "Epoch:  11 | loss: 0.027156472206 \n",
      "Epoch:  12 | loss: 0.027156461030 \n",
      "Epoch:  13 | loss: 0.027156440541 \n",
      "Epoch:  14 | loss: 0.027156434953 \n",
      "Epoch:  15 | loss: 0.027156431228 \n",
      "Epoch:  16 | loss: 0.027156421915 \n",
      "Epoch:  17 | loss: 0.027156414464 \n",
      "Epoch:  18 | loss: 0.027156393975 \n",
      "Epoch:  19 | loss: 0.027156388387 \n",
      "Epoch:  20 | loss: 0.027156379074 \n",
      "Epoch:  21 | loss: 0.027156358585 \n",
      "Epoch:  22 | loss: 0.027156347409 \n",
      "Epoch:  23 | loss: 0.027156308293 \n",
      "Epoch:  24 | loss: 0.027156310156 \n",
      "Epoch:  25 | loss: 0.027156284079 \n",
      "Epoch:  26 | loss: 0.027156271040 \n",
      "Epoch:  27 | loss: 0.027156246826 \n",
      "Epoch:  28 | loss: 0.027156198397 \n",
      "Epoch:  29 | loss: 0.027156194672 \n",
      "Epoch:  30 | loss: 0.027156153694 \n",
      "Epoch:  31 | loss: 0.027156151831 \n",
      "Epoch:  32 | loss: 0.027156151831 \n",
      "Epoch:  33 | loss: 0.027156138793 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  34 | loss: 0.027156084776 \n",
      "Epoch:  35 | loss: 0.027156064287 \n",
      "Epoch:  36 | loss: 0.027156066149 \n",
      "Epoch:  37 | loss: 0.027156054974 \n",
      "Epoch:  38 | loss: 0.027156041935 \n",
      "Epoch:  39 | loss: 0.027156043798 \n",
      "Epoch:  40 | loss: 0.027156043798 \n",
      "Epoch:  41 | loss: 0.027156030759 \n",
      "Epoch:  42 | loss: 0.027156030759 \n",
      "Epoch:  43 | loss: 0.027156028897 \n",
      "Epoch:  44 | loss: 0.027156010270 \n",
      "Epoch:  45 | loss: 0.027156006545 \n",
      "Epoch:  46 | loss: 0.027155991644 \n",
      "Epoch:  47 | loss: 0.027155974880 \n",
      "Epoch:  48 | loss: 0.027155971155 \n",
      "Epoch:  49 | loss: 0.027155954391 \n",
      "Epoch:  50 | loss: 0.027155950665 \n",
      "Epoch:  51 | loss: 0.027155948803 \n",
      "Epoch:  52 | loss: 0.027155891061 \n",
      "Epoch:  53 | loss: 0.027155887336 \n",
      "Epoch:  54 | loss: 0.027155883610 \n",
      "Epoch:  55 | loss: 0.027155857533 \n",
      "Epoch:  56 | loss: 0.027155859396 \n",
      "Epoch:  57 | loss: 0.027155840769 \n",
      "Epoch:  58 | loss: 0.027155824006 \n",
      "Epoch:  59 | loss: 0.027155764401 \n",
      "Epoch:  60 | loss: 0.027155764401 \n",
      "Epoch:  61 | loss: 0.027155663818 \n",
      "Epoch:  62 | loss: 0.027155632153 \n",
      "Epoch:  63 | loss: 0.027155632153 \n",
      "Epoch:  64 | loss: 0.027155630291 \n",
      "Epoch:  65 | loss: 0.027155615389 \n",
      "Epoch:  66 | loss: 0.027155606076 \n",
      "Epoch:  67 | loss: 0.027155602351 \n",
      "Epoch:  68 | loss: 0.027155589312 \n",
      "Epoch:  69 | loss: 0.027155578136 \n",
      "Epoch:  70 | loss: 0.027155578136 \n",
      "Epoch:  71 | loss: 0.027155546471 \n",
      "Epoch:  72 | loss: 0.027155531570 \n",
      "Epoch:  73 | loss: 0.027155525982 \n",
      "Epoch:  74 | loss: 0.027155505493 \n",
      "Epoch:  75 | loss: 0.027155496180 \n",
      "Epoch:  76 | loss: 0.027155479416 \n",
      "Epoch:  77 | loss: 0.027155481279 \n",
      "Epoch:  78 | loss: 0.027155477554 \n",
      "Epoch:  79 | loss: 0.027155473828 \n",
      "Epoch:  80 | loss: 0.027155458927 \n",
      "Epoch:  81 | loss: 0.027155449614 \n",
      "Epoch:  82 | loss: 0.027155423537 \n",
      "Epoch:  83 | loss: 0.027155421674 \n",
      "Epoch:  84 | loss: 0.027155384421 \n",
      "Epoch:  85 | loss: 0.027155365795 \n",
      "Epoch:  86 | loss: 0.027155330405 \n",
      "Epoch:  87 | loss: 0.027155328542 \n",
      "Epoch:  88 | loss: 0.027155324817 \n",
      "Epoch:  89 | loss: 0.027155285701 \n",
      "Epoch:  90 | loss: 0.027155283839 \n",
      "Epoch:  91 | loss: 0.027155255899 \n",
      "Epoch:  92 | loss: 0.027155257761 \n",
      "Epoch:  93 | loss: 0.027155257761 \n",
      "Epoch:  94 | loss: 0.027155255899 \n",
      "Epoch:  95 | loss: 0.027155246586 \n",
      "Epoch:  96 | loss: 0.027155214921 \n",
      "Epoch:  97 | loss: 0.027155216783 \n",
      "Epoch:  98 | loss: 0.027155196294 \n",
      "Epoch:  99 | loss: 0.027155196294 \n",
      "Epoch:  100 | loss: 0.027155196294 \n",
      "Epoch:  101 | loss: 0.027155196294 \n",
      "Epoch:  102 | loss: 0.027155196294 \n",
      "Epoch:  103 | loss: 0.027155198157 \n",
      "Epoch:  104 | loss: 0.027155196294 \n",
      "Epoch:  105 | loss: 0.027155196294 \n",
      "Epoch:  106 | loss: 0.027155198157 \n",
      "Epoch:  107 | loss: 0.027155196294 \n",
      "Epoch:  108 | loss: 0.027155196294 \n",
      "Epoch:  109 | loss: 0.027155198157 \n",
      "Epoch:  110 | loss: 0.027155196294 \n",
      "Epoch:  111 | loss: 0.027155196294 \n",
      "Epoch:  112 | loss: 0.027155196294 \n",
      "Epoch:  113 | loss: 0.027155196294 \n",
      "Epoch:  114 | loss: 0.027155196294 \n",
      "Epoch:  115 | loss: 0.027155196294 \n",
      "Epoch:  116 | loss: 0.027155196294 \n",
      "Epoch:  117 | loss: 0.027155196294 \n",
      "Epoch:  118 | loss: 0.027155196294 \n",
      "Epoch:  119 | loss: 0.027155194432 \n",
      "Epoch:  120 | loss: 0.027155196294 \n",
      "Epoch:  121 | loss: 0.027155194432 \n",
      "Epoch:  122 | loss: 0.027155196294 \n",
      "Epoch:  123 | loss: 0.027155196294 \n",
      "Epoch:  124 | loss: 0.027155192569 \n",
      "Epoch:  125 | loss: 0.027155192569 \n",
      "Epoch:  126 | loss: 0.027155194432 \n",
      "Epoch:  127 | loss: 0.027155190706 \n",
      "Epoch:  128 | loss: 0.027155188844 \n",
      "Epoch:  129 | loss: 0.027155188844 \n",
      "Epoch:  130 | loss: 0.027155186981 \n",
      "Epoch:  131 | loss: 0.027155186981 \n",
      "Epoch:  132 | loss: 0.027155185118 \n",
      "Epoch:  133 | loss: 0.027155185118 \n",
      "Epoch:  134 | loss: 0.027155188844 \n",
      "Epoch:  135 | loss: 0.027155185118 \n",
      "Epoch:  136 | loss: 0.027155183256 \n",
      "Epoch:  137 | loss: 0.027155183256 \n",
      "Epoch:  138 | loss: 0.027155185118 \n",
      "Epoch:  139 | loss: 0.027155183256 \n",
      "Epoch:  140 | loss: 0.027155183256 \n",
      "Epoch:  141 | loss: 0.027155179530 \n",
      "Epoch:  142 | loss: 0.027155181393 \n",
      "Epoch:  143 | loss: 0.027155183256 \n",
      "Epoch:  144 | loss: 0.027155183256 \n",
      "Epoch:  145 | loss: 0.027155181393 \n",
      "Epoch:  146 | loss: 0.027155181393 \n",
      "Epoch:  147 | loss: 0.027155181393 \n",
      "Epoch:  148 | loss: 0.027155179530 \n",
      "Epoch:  149 | loss: 0.027155181393 \n",
      "Epoch:  150 | loss: 0.027155181393 \n",
      "Epoch:  151 | loss: 0.027155179530 \n",
      "Epoch:  152 | loss: 0.027155181393 \n",
      "Epoch:  153 | loss: 0.027155181393 \n",
      "Epoch:  154 | loss: 0.027155181393 \n",
      "Epoch:  155 | loss: 0.027155177668 \n",
      "Epoch:  156 | loss: 0.027155179530 \n",
      "Epoch:  157 | loss: 0.027155179530 \n",
      "Epoch:  158 | loss: 0.027155177668 \n",
      "Epoch:  159 | loss: 0.027155179530 \n",
      "Epoch:  160 | loss: 0.027155177668 \n",
      "Epoch:  161 | loss: 0.027155179530 \n",
      "Epoch:  162 | loss: 0.027155177668 \n",
      "Epoch:  163 | loss: 0.027155177668 \n",
      "Epoch:  164 | loss: 0.027155179530 \n",
      "Epoch:  165 | loss: 0.027155177668 \n",
      "Epoch:  166 | loss: 0.027155179530 \n",
      "Epoch:  167 | loss: 0.027155177668 \n",
      "Epoch:  168 | loss: 0.027155177668 \n",
      "Epoch:  169 | loss: 0.027155177668 \n",
      "Epoch:  170 | loss: 0.027155177668 \n",
      "Epoch:  171 | loss: 0.027155179530 \n",
      "Epoch:  172 | loss: 0.027155177668 \n",
      "Epoch:  173 | loss: 0.027155177668 \n",
      "Epoch:  174 | loss: 0.027155177668 \n",
      "Epoch:  175 | loss: 0.027155177668 \n",
      "Epoch:  176 | loss: 0.027155177668 \n",
      "Epoch:  177 | loss: 0.027155177668 \n",
      "Epoch:  178 | loss: 0.027155177668 \n",
      "Epoch:  179 | loss: 0.027155177668 \n",
      "Epoch:  180 | loss: 0.027155177668 \n",
      "Epoch:  181 | loss: 0.027155177668 \n",
      "Epoch:  182 | loss: 0.027155177668 \n",
      "Epoch:  183 | loss: 0.027155175805 \n",
      "Epoch:  184 | loss: 0.027155175805 \n",
      "Epoch:  185 | loss: 0.027155172080 \n",
      "Epoch:  186 | loss: 0.027155172080 \n",
      "Epoch:  187 | loss: 0.027155170217 \n",
      "Epoch:  188 | loss: 0.027155172080 \n",
      "Epoch:  189 | loss: 0.027155168355 \n",
      "Epoch:  190 | loss: 0.027155166492 \n",
      "Epoch:  191 | loss: 0.027155164629 \n",
      "Epoch:  192 | loss: 0.027155166492 \n",
      "Epoch:  193 | loss: 0.027155166492 \n",
      "Epoch:  194 | loss: 0.027155164629 \n",
      "Epoch:  195 | loss: 0.027155162767 \n",
      "Epoch:  196 | loss: 0.027155164629 \n",
      "Epoch:  197 | loss: 0.027155164629 \n",
      "Epoch:  198 | loss: 0.027155164629 \n",
      "Epoch:  199 | loss: 0.027155159041 \n",
      "Epoch:  200 | loss: 0.027155159041 \n",
      "Epoch:  201 | loss: 0.027155159041 \n",
      "Epoch:  202 | loss: 0.027155159041 \n",
      "Epoch:  203 | loss: 0.027155159041 \n",
      "Epoch:  204 | loss: 0.027155159041 \n",
      "Epoch:  205 | loss: 0.027155159041 \n",
      "Epoch:  206 | loss: 0.027155159041 \n",
      "Epoch:  207 | loss: 0.027155159041 \n",
      "Epoch:  208 | loss: 0.027155159041 \n",
      "Epoch:  209 | loss: 0.027155159041 \n",
      "Epoch:  210 | loss: 0.027155159041 \n",
      "Epoch:  211 | loss: 0.027155159041 \n",
      "Epoch:  212 | loss: 0.027155159041 \n",
      "Epoch:  213 | loss: 0.027155159041 \n",
      "Epoch:  214 | loss: 0.027155159041 \n",
      "Epoch:  215 | loss: 0.027155159041 \n",
      "Epoch:  216 | loss: 0.027155159041 \n",
      "Epoch:  217 | loss: 0.027155159041 \n",
      "Epoch:  218 | loss: 0.027155159041 \n",
      "Epoch:  219 | loss: 0.027155159041 \n",
      "Epoch:  220 | loss: 0.027155159041 \n",
      "Epoch:  221 | loss: 0.027155159041 \n",
      "Epoch:  222 | loss: 0.027155159041 \n",
      "Epoch:  223 | loss: 0.027155159041 \n",
      "Epoch:  224 | loss: 0.027155159041 \n",
      "Epoch:  225 | loss: 0.027155159041 \n",
      "Epoch:  226 | loss: 0.027155159041 \n",
      "Epoch:  227 | loss: 0.027155159041 \n",
      "Epoch:  228 | loss: 0.027155159041 \n",
      "Epoch:  229 | loss: 0.027155159041 \n",
      "Epoch:  230 | loss: 0.027155159041 \n",
      "Epoch:  231 | loss: 0.027155159041 \n",
      "Epoch:  232 | loss: 0.027155159041 \n",
      "Epoch:  233 | loss: 0.027155157179 \n",
      "Epoch:  234 | loss: 0.027155157179 \n",
      "Epoch:  235 | loss: 0.027155157179 \n",
      "Epoch:  236 | loss: 0.027155159041 \n",
      "Epoch:  237 | loss: 0.027155159041 \n",
      "Epoch:  238 | loss: 0.027155159041 \n",
      "Epoch:  239 | loss: 0.027155157179 \n",
      "Epoch:  240 | loss: 0.027155159041 \n",
      "Epoch:  241 | loss: 0.027155159041 \n",
      "Epoch:  242 | loss: 0.027155159041 \n",
      "Epoch:  243 | loss: 0.027155159041 \n",
      "Epoch:  244 | loss: 0.027155155316 \n",
      "Epoch:  245 | loss: 0.027155157179 \n",
      "Epoch:  246 | loss: 0.027155155316 \n",
      "Epoch:  247 | loss: 0.027155159041 \n",
      "Epoch:  248 | loss: 0.027155157179 \n",
      "Epoch:  249 | loss: 0.027155157179 \n",
      "Epoch:  250 | loss: 0.027155159041 \n",
      "Epoch:  251 | loss: 0.027155159041 \n",
      "Epoch:  252 | loss: 0.027155159041 \n",
      "Epoch:  253 | loss: 0.027155157179 \n",
      "Epoch:  254 | loss: 0.027155159041 \n",
      "Epoch:  255 | loss: 0.027155157179 \n",
      "Epoch:  256 | loss: 0.027155159041 \n",
      "Epoch:  257 | loss: 0.027155159041 \n",
      "Epoch:  258 | loss: 0.027155157179 \n",
      "Epoch:  259 | loss: 0.027155157179 \n",
      "Epoch:  260 | loss: 0.027155159041 \n",
      "Epoch:  261 | loss: 0.027155157179 \n",
      "Epoch:  262 | loss: 0.027155159041 \n",
      "Epoch:  263 | loss: 0.027155159041 \n",
      "Epoch:  264 | loss: 0.027155157179 \n",
      "Epoch:  265 | loss: 0.027155157179 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  266 | loss: 0.027155159041 \n",
      "Epoch:  267 | loss: 0.027155159041 \n",
      "Epoch:  268 | loss: 0.027155157179 \n",
      "Epoch:  269 | loss: 0.027155159041 \n",
      "Epoch:  270 | loss: 0.027155157179 \n",
      "Epoch:  271 | loss: 0.027155159041 \n",
      "Epoch:  272 | loss: 0.027155157179 \n",
      "Epoch:  273 | loss: 0.027155157179 \n",
      "Epoch:  274 | loss: 0.027155155316 \n",
      "Epoch:  275 | loss: 0.027155159041 \n",
      "Epoch:  276 | loss: 0.027155159041 \n",
      "Epoch:  277 | loss: 0.027155157179 \n",
      "Epoch:  278 | loss: 0.027155159041 \n",
      "Epoch:  279 | loss: 0.027155157179 \n",
      "Epoch:  280 | loss: 0.027155155316 \n",
      "Epoch:  281 | loss: 0.027155159041 \n",
      "Epoch:  282 | loss: 0.027155157179 \n",
      "Epoch:  283 | loss: 0.027155159041 \n",
      "Epoch:  284 | loss: 0.027155157179 \n",
      "Epoch:  285 | loss: 0.027155155316 \n",
      "Epoch:  286 | loss: 0.027155155316 \n",
      "Epoch:  287 | loss: 0.027155155316 \n",
      "Epoch:  288 | loss: 0.027155157179 \n",
      "Epoch:  289 | loss: 0.027155155316 \n",
      "Epoch:  290 | loss: 0.027155157179 \n",
      "Epoch:  291 | loss: 0.027155155316 \n",
      "Epoch:  292 | loss: 0.027155153453 \n",
      "Epoch:  293 | loss: 0.027155159041 \n",
      "Epoch:  294 | loss: 0.027155155316 \n",
      "Epoch:  295 | loss: 0.027155155316 \n",
      "Epoch:  296 | loss: 0.027155157179 \n",
      "Epoch:  297 | loss: 0.027155155316 \n",
      "Epoch:  298 | loss: 0.027155155316 \n",
      "Epoch:  299 | loss: 0.027155155316 \n",
      "time=71.53075623512268\n",
      "best_train_loss=0.027155153453350067\n",
      "test_loss=0.026113087311387062\n",
      "best_epoch=292\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# w/wo growing network\n",
    "# Tanh, Relu, Relu2 activation, last-layer act(other relu). Even: Should we use spline networks?\n",
    "# Hybrid: Adam + LBFGS\n",
    "# Possible overfitting: train & test gap\n",
    "# LBFGS usage\n",
    "\n",
    "#ws = [50,60,70,80,90,100,120,140,160,180,200,250,300,350,400]\n",
    "ws = [5,10,15,20,25,30,35,40,45,50]\n",
    "#ws = [14]\n",
    "#ws = [2,4]\n",
    "losses_w = []\n",
    "losses_w_test = []\n",
    "times = []\n",
    "\n",
    "a_s = [0.0]\n",
    "\n",
    "w_i = 0\n",
    "\n",
    "for w in ws:\n",
    "    \n",
    "    print(\"w={}\".format(w))\n",
    "    if w_i == 0:\n",
    "        t = T(w=w, a=0.0, M=1.0)\n",
    "    else:\n",
    "        sd = t.state_dict()\n",
    "        t_old = T(w=w_old, a=0.0, M=1.0)\n",
    "        t_old.load_state_dict(sd)\n",
    "        t = T(w=w, a=0.0, M=1.0)\n",
    "        t = grow(t_old, t, w_old, w)\n",
    "    w_i = w_i + 1\n",
    "\n",
    "\n",
    "    losses_a = []\n",
    "    losses_all = []\n",
    "    a_i = 0\n",
    "\n",
    "\n",
    "\n",
    "    for a_kerr in a_s:\n",
    "        M = 1.0\n",
    "        r_c = M + np.sqrt(M**2-a_kerr**2)\n",
    "        print(\"a_kerr={}\".format(a_kerr))\n",
    "        print(\"M={}\".format(M))\n",
    "        print(\"r_c={}\".format(r_c))\n",
    "        #t = T(w=400, a=a_kerr, M=M)\n",
    "        t.set_a(a_kerr)\n",
    "        a_i += 1\n",
    "\n",
    "\n",
    "        # Kerr Metric\n",
    "        def g(x_):\n",
    "            a = a_kerr\n",
    "            bs = x_.shape[0]\n",
    "            t = x_[:,0]\n",
    "            x = x_[:,1]\n",
    "            y = x_[:,2]\n",
    "            z = x_[:,3]\n",
    "            rho = torch.sqrt(x**2+y**2)\n",
    "            r = torch.sqrt(x**2+y**2+z**2)\n",
    "            costheta = z/r\n",
    "            sintheta = rho/r\n",
    "            sin2theta = 2*sintheta*costheta\n",
    "            cos2theta = 2*costheta**2 - 1\n",
    "            cosphi = x/rho\n",
    "            sinphi = y/rho\n",
    "            sigma = r**2 + a**2*costheta**2\n",
    "            zeta = torch.sqrt(r**2+a**2)\n",
    "            sq2 = torch.sqrt(torch.tensor(2., dtype=torch.float, requires_grad=False))\n",
    "\n",
    "            u1 = a**2 + 2*r**2 + a**2*cos2theta\n",
    "            zeta = torch.sqrt(a**2+r**2)\n",
    "            u2 = u1/(zeta*torch.sqrt(M*r))\n",
    "            u3 = 8*a*M/sigma\n",
    "            u4 = zeta**2 + 2*a**2*M*r*sintheta**2/sigma\n",
    "\n",
    "            g00 = -1 + 2*M*r/sigma\n",
    "            g01 = g10 = 1/4*sintheta*(sq2*u2*cosphi+u3*sinphi)\n",
    "            g02 = g20 = 1/4*sintheta*(-u3*cosphi+sq2*u2*sinphi)\n",
    "            g03 = g30 = costheta*u2/sq2**3\n",
    "            g11 = (8*costheta**2*cosphi**2*sigma-u2**2*r**2*cosphi**2*sintheta**2+8*u4*sinphi**2)/(8*r**2)\n",
    "            g12 = g21 = cosphi*(8*costheta**2*sigma-u2**2*r**2*sintheta**2-8*u4)*sinphi/(8*r**2)\n",
    "            g13 = g31 = costheta*(-8*sigma-u2**2*r**2)*cosphi*sintheta/(8*r**2)\n",
    "            g22 = (8*cosphi**2*u4+8*costheta**2*sigma*sinphi**2-u2**2*r**2*sintheta**2*sinphi**2)/(8*r**2)\n",
    "            g23 = g32 = costheta*(-8*sigma-u2**2*r**2)*sintheta*sinphi/(8*r**2)\n",
    "            g33 = sintheta**2 + costheta**2*(-u2**2*r**2+8*a**2*sintheta**2)/(8*r**2)\n",
    "\n",
    "            stack1 = torch.stack([g00, g01, g02, g03])\n",
    "            stack2 = torch.stack([g10, g11, g12, g13])\n",
    "            stack3 = torch.stack([g20, g21, g22, g23])\n",
    "            stack4 = torch.stack([g30, g31, g32, g33])\n",
    "\n",
    "            gs = - torch.stack([stack1, stack2, stack3, stack4]).permute(2,0,1)\n",
    "            return gs\n",
    "        \n",
    "        def euclidean_loss(t,inputs):\n",
    "            gp = t.transform_g(inputs)\n",
    "            bs = gp.shape[0]\n",
    "            minkowski_metric = torch.unsqueeze(torch.unsqueeze(torch.ones(bs,),dim=1),dim=2) * torch.unsqueeze(torch.diag(torch.tensor([-1.,-1.,-1.], dtype=torch.float, requires_grad=True)), dim=0)\n",
    "            return torch.mean((gp[:,1:,1:]-minkowski_metric)**2)\n",
    "\n",
    "\n",
    "\n",
    "        lr = 0.1*(5/w)**2\n",
    "        '''if w_i == 0:\n",
    "            epochs = 2500\n",
    "            BFGS_epoch = 2000\n",
    "        else:\n",
    "            epochs = 500\n",
    "            BFGS_epoch = 0'''\n",
    "        \n",
    "        # We need to understand LBFGS better\n",
    "        epochs = 300\n",
    "        switch_epoch = 100\n",
    "        BFGS_epoch = 1000000\n",
    "\n",
    "        #optimizer = optim.Adam(t.parameters(), lr=lr, eps=1e-8)\n",
    "        #optimizer = optim.SGD(t.parameters(),lr=lr)\n",
    "        optimizer = optim.LBFGS(t.parameters(), lr=lr, max_iter=100, max_eval=None, tolerance_grad=1e-30, tolerance_change=1e-30, history_size=100, line_search_fn='strong_wolfe')\n",
    "        #optimizer = optim.LBFGS(t.parameters(), lr=0.1, max_iter=100, max_eval=1e10, tolerance_grad=1e-30, tolerance_change=1e-30, history_size=100)\n",
    "\n",
    "\n",
    "\n",
    "        #epochs = 100\n",
    "        #optimizer = optim.LBFGS(t.parameters(), lr=1, max_iter=100, max_eval=None, tolerance_grad=1e-30, tolerance_change=1e-30, history_size=100, line_search_fn=\"strong_wolfe\")\n",
    "        #optimizer = optim.LBFGS(t.parameters(), lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn=None)\n",
    "\n",
    "        log_save = 100000\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        n_train = 1000\n",
    "        \n",
    "        best_loss = 10000\n",
    "        \n",
    "        start = time.time()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            if (epoch+1) % switch_epoch == 0:\n",
    "                for opt_param in optimizer.param_groups:\n",
    "                    lr = lr * 0.5\n",
    "                    opt_param['lr'] = lr\n",
    "\n",
    "            if epoch == BFGS_epoch:\n",
    "                # BFGS learning rate. How to set?\n",
    "                optimizer = optim.LBFGS(t.parameters(), lr=0.1, max_iter=100, max_eval=1e10, tolerance_grad=1e-30, tolerance_change=1e-30, history_size=100, line_search_fn='strong_wolfe')\n",
    "\n",
    "            if epoch < BFGS_epoch:\n",
    "                log = 1\n",
    "                batch_size = n_train\n",
    "            else:\n",
    "                log = 1\n",
    "                batch_size = n_train\n",
    "            t.train()\n",
    "\n",
    "\n",
    "            choices = np.random.choice(n_train, batch_size, replace=False)\n",
    "            inputs = input_[choices]\n",
    "\n",
    "            # -------------------------------------------\n",
    "            def loss_closure():\n",
    "                if torch.is_grad_enabled():\n",
    "                    optimizer.zero_grad()\n",
    "                loss_inner = euclidean_loss(t,inputs)\n",
    "                #if loss_inner.requires_grad:\n",
    "                    #loss_inner.backward(retain_graph=True)\n",
    "                    #loss_inner.backward()\n",
    "                loss_inner.backward(retain_graph=True)\n",
    "                return loss_inner\n",
    "            # -------------------------------------------\n",
    "            loss = loss_closure()\n",
    "            # best_loss is trick for better scaling laws\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_epoch = epoch\n",
    "                def loss_closure_test():\n",
    "                    if torch.is_grad_enabled():\n",
    "                        optimizer.zero_grad()\n",
    "                    loss_inner = euclidean_loss(t,input_test_)\n",
    "                    return loss_inner\n",
    "                loss_test = loss_closure_test()\n",
    "            optimizer.step(loss_closure)  # get loss, use to update wts\n",
    "\n",
    "            losses.append(loss.detach().numpy())\n",
    "            losses_all.append(loss.detach().numpy())\n",
    "            '''loss = euclidean_loss(g,t,inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()'''\n",
    "            \n",
    "\n",
    "            if epoch%log == 0:\n",
    "                print('Epoch:  %d | loss: %.12f ' %(epoch, loss))\n",
    "\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    print(\"time={}\".format(duration))\n",
    "    times.append(duration)\n",
    "\n",
    "\n",
    "                \n",
    "    w_old = w\n",
    "    losses_w.append(best_loss.detach().numpy())\n",
    "    losses_w_test.append(loss_test.detach().numpy())\n",
    "    print(\"best_train_loss={}\".format(best_loss.detach().numpy()))\n",
    "    print(\"test_loss={}\".format(loss_test.detach().numpy()))\n",
    "    print(\"best_epoch={}\".format(best_epoch))\n",
    "    \n",
    "np.save('./results_nn/grow_relu2last_bfgs_lrdecay',np.array([ws, losses_w, times, losses_w_test]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.00000000e+00, 1.00000000e+01, 1.50000000e+01, 2.00000000e+01,\n",
       "        2.50000000e+01, 3.00000000e+01, 3.50000000e+01, 4.00000000e+01,\n",
       "        4.50000000e+01, 5.00000000e+01],\n",
       "       [2.72356886e-02, 2.72330232e-02, 2.72009391e-02, 2.71868333e-02,\n",
       "        2.71855835e-02, 2.71693114e-02, 2.71599293e-02, 2.71579698e-02,\n",
       "        2.71565970e-02, 2.71551535e-02],\n",
       "       [2.59542639e+01, 6.46682608e+01, 1.93629422e+02, 1.76913342e+02,\n",
       "        5.34942322e+01, 1.85394684e+02, 1.68734799e+02, 7.96419463e+01,\n",
       "        7.28895202e+01, 7.15307562e+01],\n",
       "       [2.61764061e-02, 2.61743758e-02, 2.61507109e-02, 2.61398703e-02,\n",
       "        2.61388645e-02, 2.61253025e-02, 2.61172689e-02, 2.61155777e-02,\n",
       "        2.61143688e-02, 2.61130873e-02]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('./results_nn/grow_relu2last_bfgs_lrdecay.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Next try: relu2 (not just last layer)\n",
    "\n",
    "plt.plot(ws, losses_w, marker=\"o\")\n",
    "#plt.plot(ws, losses_w_test, marker=\"o\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('NN width',fontsize=15)\n",
    "plt.ylabel('MSE',fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Epoch: 0 | Loss: 0.4259971752205497\n",
      "Epoch: 5 | Loss: 1.7749924537105484e-06\n",
      "Epoch: 10 | Loss: 1.771590917027854e-06\n",
      "Epoch: 15 | Loss: 1.7685937495901208e-06\n",
      "Epoch: 20 | Loss: 1.7659425188772155e-06\n",
      "Epoch: 25 | Loss: 1.7635906913253813e-06\n",
      "Epoch: 30 | Loss: 1.7614775849480406e-06\n",
      "Epoch: 35 | Loss: 1.7595603597980748e-06\n",
      "Epoch: 40 | Loss: 1.7578216172965326e-06\n",
      "Epoch: 45 | Loss: 1.7562246431031315e-06\n",
      "Epoch: 50 | Loss: 1.7547542927015177e-06\n",
      "Epoch: 55 | Loss: 1.753385131432509e-06\n",
      "Epoch: 60 | Loss: 1.7521115041062026e-06\n",
      "Epoch: 65 | Loss: 1.7509205927980376e-06\n",
      "Epoch: 70 | Loss: 1.7497965321949475e-06\n",
      "Epoch: 75 | Loss: 1.7487425798696848e-06\n",
      "Epoch: 80 | Loss: 1.747747072461419e-06\n",
      "Epoch: 85 | Loss: 1.7468102698097896e-06\n",
      "Epoch: 90 | Loss: 1.745930970363519e-06\n",
      "Epoch: 95 | Loss: 1.745103988735206e-06\n",
      "Epoch: 100 | Loss: 1.7444131067950095e-06\n",
      "Epoch: 105 | Loss: 1.7441615234577112e-06\n",
      "Epoch: 110 | Loss: 1.7439662436218442e-06\n",
      "Epoch: 115 | Loss: 1.743763472169576e-06\n",
      "Epoch: 120 | Loss: 1.7435623111390166e-06\n",
      "Epoch: 125 | Loss: 1.7433407494556106e-06\n",
      "Epoch: 130 | Loss: 1.7431663815308398e-06\n",
      "Epoch: 135 | Loss: 1.742956225284467e-06\n",
      "Epoch: 140 | Loss: 1.7427547954779283e-06\n",
      "Epoch: 145 | Loss: 1.7425804507683144e-06\n",
      "Epoch: 150 | Loss: 1.74238969781005e-06\n",
      "Epoch: 155 | Loss: 1.7422057775983141e-06\n",
      "Epoch: 160 | Loss: 1.7420140226683982e-06\n",
      "Epoch: 165 | Loss: 1.7418292841758562e-06\n",
      "Epoch: 170 | Loss: 1.741649853138103e-06\n",
      "Epoch: 175 | Loss: 1.7414687839269187e-06\n",
      "Epoch: 180 | Loss: 1.7412770804989084e-06\n",
      "Epoch: 185 | Loss: 1.7410967907233896e-06\n",
      "Epoch: 190 | Loss: 1.74092959910395e-06\n",
      "Epoch: 195 | Loss: 1.7407565729033398e-06\n",
      "Epoch: 200 | Loss: 1.7405976217044829e-06\n",
      "Epoch: 205 | Loss: 1.7404748543892406e-06\n",
      "Epoch: 210 | Loss: 1.7403503443156728e-06\n",
      "Epoch: 215 | Loss: 1.7402198358849808e-06\n",
      "Epoch: 220 | Loss: 1.7400860407488197e-06\n",
      "Epoch: 225 | Loss: 1.7399472361323966e-06\n",
      "Epoch: 230 | Loss: 1.7398273260327879e-06\n",
      "Epoch: 235 | Loss: 1.7397168478850174e-06\n",
      "Epoch: 240 | Loss: 1.739583990407997e-06\n",
      "Epoch: 245 | Loss: 1.7394753839770595e-06\n",
      "Epoch: 250 | Loss: 1.7393409821213093e-06\n",
      "Epoch: 255 | Loss: 1.739229332124313e-06\n",
      "Epoch: 260 | Loss: 1.7391133186237915e-06\n",
      "Epoch: 265 | Loss: 1.739006725796213e-06\n",
      "Epoch: 270 | Loss: 1.7388937507150313e-06\n",
      "Epoch: 275 | Loss: 1.7387791587740125e-06\n",
      "Epoch: 280 | Loss: 1.738661831587235e-06\n",
      "Epoch: 285 | Loss: 1.7385452504084723e-06\n",
      "Epoch: 290 | Loss: 1.73843073172007e-06\n",
      "Epoch: 295 | Loss: 1.7383343562632486e-06\n",
      "time=323.636568069458\n",
      "error=1.7382559526919144e-06\n",
      "best loss=1.7382559526919144e-06\n",
      "best epoch=299\n",
      "8\n",
      "Epoch: 0 | Loss: 0.10038620979797305\n",
      "Epoch: 5 | Loss: 9.494289626895788e-07\n",
      "Epoch: 10 | Loss: 8.078346302676329e-07\n",
      "Epoch: 15 | Loss: 8.074051219935205e-07\n",
      "Epoch: 20 | Loss: 8.071586273566375e-07\n",
      "Epoch: 25 | Loss: 8.069846136894789e-07\n",
      "Epoch: 30 | Loss: 8.068466488967803e-07\n",
      "Epoch: 35 | Loss: 8.067279773036241e-07\n",
      "Epoch: 40 | Loss: 8.066205824256456e-07\n",
      "Epoch: 45 | Loss: 8.065204147596045e-07\n",
      "Epoch: 50 | Loss: 8.064253834097565e-07\n",
      "Epoch: 55 | Loss: 8.063343791730639e-07\n",
      "Epoch: 60 | Loss: 8.062466325975404e-07\n",
      "Epoch: 65 | Loss: 8.061620866117299e-07\n",
      "Epoch: 70 | Loss: 8.060803564284135e-07\n",
      "Epoch: 75 | Loss: 8.060012751389296e-07\n",
      "Epoch: 80 | Loss: 8.059247116634408e-07\n",
      "Epoch: 85 | Loss: 8.058505549929977e-07\n",
      "Epoch: 90 | Loss: 8.05778706171716e-07\n",
      "Epoch: 95 | Loss: 8.057090741697163e-07\n",
      "Epoch: 100 | Loss: 8.056402376995537e-07\n",
      "Epoch: 105 | Loss: 8.056279036457575e-07\n",
      "Epoch: 110 | Loss: 8.056147477903766e-07\n",
      "Epoch: 115 | Loss: 8.055981522457186e-07\n",
      "Epoch: 120 | Loss: 8.055856420506158e-07\n",
      "Epoch: 125 | Loss: 8.055728859396243e-07\n",
      "Epoch: 130 | Loss: 8.055607807366792e-07\n",
      "Epoch: 135 | Loss: 8.055482650689074e-07\n",
      "Epoch: 140 | Loss: 8.055355944276084e-07\n",
      "Epoch: 145 | Loss: 8.05523022021268e-07\n",
      "Epoch: 150 | Loss: 8.055117078327532e-07\n",
      "Epoch: 155 | Loss: 8.055000496984393e-07\n",
      "Epoch: 160 | Loss: 8.054891236554119e-07\n",
      "Epoch: 165 | Loss: 8.054762707821597e-07\n",
      "Epoch: 170 | Loss: 8.054654029155788e-07\n",
      "Epoch: 175 | Loss: 8.05452321446154e-07\n",
      "Epoch: 180 | Loss: 8.054403223069288e-07\n",
      "Epoch: 185 | Loss: 8.054298023870103e-07\n",
      "Epoch: 190 | Loss: 8.054172057069915e-07\n",
      "Epoch: 195 | Loss: 8.05407619412181e-07\n",
      "Epoch: 200 | Loss: 8.053943642374573e-07\n",
      "Epoch: 205 | Loss: 8.05378707115362e-07\n",
      "Epoch: 210 | Loss: 8.053605842297762e-07\n",
      "Epoch: 215 | Loss: 8.053440516164687e-07\n",
      "Epoch: 220 | Loss: 8.053267121120456e-07\n",
      "Epoch: 225 | Loss: 8.053079027756452e-07\n",
      "Epoch: 230 | Loss: 8.052906326096912e-07\n",
      "Epoch: 235 | Loss: 8.052750106964105e-07\n",
      "Epoch: 240 | Loss: 8.052602156807228e-07\n",
      "Epoch: 245 | Loss: 8.052432009466622e-07\n",
      "Epoch: 250 | Loss: 8.052244803795637e-07\n",
      "Epoch: 255 | Loss: 8.052097708600851e-07\n",
      "Epoch: 260 | Loss: 8.051919595360776e-07\n",
      "Epoch: 265 | Loss: 8.051754961854837e-07\n",
      "Epoch: 270 | Loss: 8.0515656265904e-07\n",
      "Epoch: 275 | Loss: 8.051413090593384e-07\n",
      "Epoch: 280 | Loss: 8.051207283498762e-07\n",
      "Epoch: 285 | Loss: 8.051050194224304e-07\n",
      "Epoch: 290 | Loss: 8.050913293390746e-07\n",
      "Epoch: 295 | Loss: 8.050755705402565e-07\n",
      "time=373.6613209247589\n",
      "error=8.050630044806191e-07\n",
      "best loss=8.050630044806191e-07\n",
      "best epoch=299\n",
      "12\n",
      "Epoch: 0 | Loss: 0.0035525384113833935\n",
      "Epoch: 5 | Loss: 6.904205592811154e-07\n",
      "Epoch: 10 | Loss: 2.5739944733772757e-07\n",
      "Epoch: 15 | Loss: 2.564351960294195e-07\n",
      "Epoch: 20 | Loss: 2.5583398034541534e-07\n",
      "Epoch: 25 | Loss: 2.553363682728345e-07\n",
      "Epoch: 30 | Loss: 2.5489526640181874e-07\n",
      "Epoch: 35 | Loss: 2.5449309308875677e-07\n",
      "Epoch: 40 | Loss: 2.5411798246545015e-07\n",
      "Epoch: 45 | Loss: 2.5376591650279763e-07\n",
      "Epoch: 50 | Loss: 2.534331794161373e-07\n",
      "Epoch: 55 | Loss: 2.5311804838153926e-07\n",
      "Epoch: 60 | Loss: 2.528165510030209e-07\n",
      "Epoch: 65 | Loss: 2.525280287694949e-07\n",
      "Epoch: 70 | Loss: 2.522518992883511e-07\n",
      "Epoch: 75 | Loss: 2.519848896214735e-07\n",
      "Epoch: 80 | Loss: 2.5172727418834646e-07\n",
      "Epoch: 85 | Loss: 2.514783819016239e-07\n",
      "Epoch: 90 | Loss: 2.512371055696068e-07\n",
      "Epoch: 95 | Loss: 2.5100220914708884e-07\n",
      "Epoch: 100 | Loss: 2.5072031643290233e-07\n",
      "Epoch: 105 | Loss: 2.5042553486587853e-07\n",
      "Epoch: 110 | Loss: 2.501473776331517e-07\n",
      "Epoch: 115 | Loss: 2.4989107734631764e-07\n",
      "Epoch: 120 | Loss: 2.495804232632222e-07\n",
      "Epoch: 125 | Loss: 2.4932797569054693e-07\n",
      "Epoch: 130 | Loss: 2.490294116595913e-07\n",
      "Epoch: 135 | Loss: 2.4876248074560134e-07\n",
      "Epoch: 140 | Loss: 2.485436059030446e-07\n",
      "Epoch: 145 | Loss: 2.4828998242743204e-07\n",
      "Epoch: 150 | Loss: 2.480608027157999e-07\n",
      "Epoch: 155 | Loss: 2.4784918237660735e-07\n",
      "Epoch: 160 | Loss: 2.4764608694271216e-07\n",
      "Epoch: 165 | Loss: 2.4738682528408444e-07\n",
      "Epoch: 170 | Loss: 2.4716285587648627e-07\n",
      "Epoch: 175 | Loss: 2.46937958802212e-07\n",
      "Epoch: 180 | Loss: 2.467430741663212e-07\n",
      "Epoch: 185 | Loss: 2.465493183601059e-07\n",
      "Epoch: 190 | Loss: 2.462911526662098e-07\n",
      "Epoch: 195 | Loss: 2.4610369329853704e-07\n",
      "Epoch: 200 | Loss: 2.4594939094860275e-07\n",
      "Epoch: 205 | Loss: 2.458537493075778e-07\n",
      "Epoch: 210 | Loss: 2.4570282337502824e-07\n",
      "Epoch: 215 | Loss: 2.454389826909304e-07\n",
      "Epoch: 220 | Loss: 2.452108737116803e-07\n",
      "Epoch: 225 | Loss: 2.450974541348295e-07\n",
      "Epoch: 230 | Loss: 2.449735227783307e-07\n",
      "Epoch: 235 | Loss: 2.4486819571159604e-07\n",
      "Epoch: 240 | Loss: 2.447414669832858e-07\n",
      "Epoch: 245 | Loss: 2.4463742716198487e-07\n",
      "Epoch: 250 | Loss: 2.445085958052312e-07\n",
      "Epoch: 255 | Loss: 2.4440478848639337e-07\n",
      "Epoch: 260 | Loss: 2.442596135314105e-07\n",
      "Epoch: 265 | Loss: 2.441571114180106e-07\n",
      "Epoch: 270 | Loss: 2.440748399964011e-07\n",
      "Epoch: 275 | Loss: 2.439768943762057e-07\n",
      "Epoch: 280 | Loss: 2.4385637278818833e-07\n",
      "Epoch: 285 | Loss: 2.4374250127049383e-07\n",
      "Epoch: 290 | Loss: 2.4364892400220864e-07\n",
      "Epoch: 295 | Loss: 2.435174783906492e-07\n",
      "time=433.91895604133606\n",
      "error=2.434467435512266e-07\n",
      "best loss=2.434467435512266e-07\n",
      "best epoch=299\n",
      "16\n",
      "Epoch: 0 | Loss: 0.00012207420564810392\n",
      "Epoch: 5 | Loss: 3.279174806795453e-07\n",
      "Epoch: 10 | Loss: 1.2438661284924083e-07\n",
      "Epoch: 15 | Loss: 8.656458039692666e-08\n",
      "Epoch: 20 | Loss: 8.576702712093498e-08\n",
      "Epoch: 25 | Loss: 8.526861516472769e-08\n",
      "Epoch: 30 | Loss: 8.489467491869084e-08\n",
      "Epoch: 35 | Loss: 8.459043657954276e-08\n",
      "Epoch: 40 | Loss: 8.43319580984347e-08\n",
      "Epoch: 45 | Loss: 8.410689027849662e-08\n",
      "Epoch: 50 | Loss: 8.390669798216432e-08\n",
      "Epoch: 55 | Loss: 8.372589489360584e-08\n",
      "Epoch: 60 | Loss: 8.356109420818373e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65 | Loss: 8.34090493845967e-08\n",
      "Epoch: 70 | Loss: 8.326816016293261e-08\n",
      "Epoch: 75 | Loss: 8.313631162786611e-08\n",
      "Epoch: 80 | Loss: 8.30120343297004e-08\n",
      "Epoch: 85 | Loss: 8.289546018905506e-08\n",
      "Epoch: 90 | Loss: 8.278462161280445e-08\n",
      "Epoch: 95 | Loss: 8.26794686243028e-08\n",
      "Epoch: 100 | Loss: 8.257429154594792e-08\n",
      "Epoch: 105 | Loss: 8.240362960536062e-08\n",
      "Epoch: 110 | Loss: 8.226464498752364e-08\n",
      "Epoch: 115 | Loss: 8.216170240499427e-08\n",
      "Epoch: 120 | Loss: 8.203701679480061e-08\n",
      "Epoch: 125 | Loss: 8.192612324690748e-08\n",
      "Epoch: 130 | Loss: 8.184144013312304e-08\n",
      "Epoch: 135 | Loss: 8.175110324492637e-08\n",
      "Epoch: 140 | Loss: 8.167788335268943e-08\n",
      "Epoch: 145 | Loss: 8.159858460244582e-08\n",
      "Epoch: 150 | Loss: 8.147287036600738e-08\n",
      "Epoch: 155 | Loss: 8.136568453918943e-08\n",
      "Epoch: 160 | Loss: 8.130206749484837e-08\n",
      "Epoch: 165 | Loss: 8.124552412300724e-08\n",
      "Epoch: 170 | Loss: 8.117478561563535e-08\n",
      "Epoch: 175 | Loss: 8.110220541101134e-08\n",
      "Epoch: 180 | Loss: 8.103856876719084e-08\n",
      "Epoch: 185 | Loss: 8.095664554344232e-08\n",
      "Epoch: 190 | Loss: 8.089679837398349e-08\n",
      "Epoch: 195 | Loss: 8.07994154480033e-08\n",
      "Epoch: 200 | Loss: 8.067029743717606e-08\n",
      "Epoch: 205 | Loss: 8.064782783542953e-08\n",
      "Epoch: 210 | Loss: 8.04363254105559e-08\n",
      "Epoch: 215 | Loss: 8.008942868933475e-08\n",
      "Epoch: 220 | Loss: 7.997791454956796e-08\n",
      "Epoch: 225 | Loss: 7.971355050225158e-08\n",
      "Epoch: 230 | Loss: 7.957307087094404e-08\n",
      "Epoch: 235 | Loss: 7.945397739095683e-08\n",
      "Epoch: 240 | Loss: 7.937107723825914e-08\n",
      "Epoch: 245 | Loss: 7.929319615679395e-08\n",
      "Epoch: 250 | Loss: 7.770129609937613e-08\n",
      "Epoch: 255 | Loss: 7.76694291332944e-08\n",
      "Epoch: 260 | Loss: 7.765709635769511e-08\n",
      "Epoch: 265 | Loss: 7.764416245173737e-08\n",
      "Epoch: 270 | Loss: 7.763335660865372e-08\n",
      "Epoch: 275 | Loss: 7.762118907915385e-08\n",
      "Epoch: 280 | Loss: 7.761000349191987e-08\n",
      "Epoch: 285 | Loss: 7.759925519851161e-08\n",
      "Epoch: 290 | Loss: 7.758946478708047e-08\n",
      "Epoch: 295 | Loss: 7.757889504993442e-08\n",
      "time=477.884859085083\n",
      "error=7.757112805103608e-08\n",
      "best loss=7.757112805103608e-08\n",
      "best epoch=299\n",
      "20\n",
      "Epoch: 0 | Loss: 1.4712549303488574e-05\n",
      "Epoch: 5 | Loss: 6.672870102185614e-08\n",
      "Epoch: 10 | Loss: 6.53198272268086e-08\n",
      "Epoch: 15 | Loss: 6.466735561850494e-08\n",
      "Epoch: 20 | Loss: 6.420348881518438e-08\n",
      "Epoch: 25 | Loss: 6.383866848931032e-08\n",
      "Epoch: 30 | Loss: 6.353620444784336e-08\n",
      "Epoch: 35 | Loss: 6.327747419556118e-08\n",
      "Epoch: 40 | Loss: 6.30505976639057e-08\n",
      "Epoch: 45 | Loss: 6.284853307021078e-08\n",
      "Epoch: 50 | Loss: 6.266604732266235e-08\n",
      "Epoch: 55 | Loss: 6.24994057912889e-08\n",
      "Epoch: 60 | Loss: 6.234620132932187e-08\n",
      "Epoch: 65 | Loss: 6.220426934665203e-08\n",
      "Epoch: 70 | Loss: 6.207202073197238e-08\n",
      "Epoch: 75 | Loss: 6.194808881911982e-08\n",
      "Epoch: 80 | Loss: 6.183163834905999e-08\n",
      "Epoch: 85 | Loss: 6.172171995058737e-08\n",
      "Epoch: 90 | Loss: 6.161762223467356e-08\n",
      "Epoch: 95 | Loss: 6.151866764003879e-08\n",
      "Epoch: 100 | Loss: 6.136938189982665e-08\n",
      "Epoch: 105 | Loss: 6.111986415570699e-08\n",
      "Epoch: 110 | Loss: 6.102843039361153e-08\n",
      "Epoch: 115 | Loss: 6.094816187426206e-08\n",
      "Epoch: 120 | Loss: 6.085642292064606e-08\n",
      "Epoch: 125 | Loss: 6.078865392517959e-08\n",
      "Epoch: 130 | Loss: 6.072028797548765e-08\n",
      "Epoch: 135 | Loss: 6.062049704381266e-08\n",
      "Epoch: 140 | Loss: 6.055023136231948e-08\n",
      "Epoch: 145 | Loss: 6.048618224472189e-08\n",
      "Epoch: 150 | Loss: 6.043028363819856e-08\n",
      "Epoch: 155 | Loss: 6.036670938444327e-08\n",
      "Epoch: 160 | Loss: 6.03085356301788e-08\n",
      "Epoch: 165 | Loss: 6.023245017465449e-08\n",
      "Epoch: 170 | Loss: 6.013772656462767e-08\n",
      "Epoch: 175 | Loss: 6.00877028770797e-08\n",
      "Epoch: 180 | Loss: 6.004202965757897e-08\n",
      "Epoch: 185 | Loss: 5.998812735749266e-08\n",
      "Epoch: 190 | Loss: 5.993492084154403e-08\n",
      "Epoch: 195 | Loss: 5.986298341952364e-08\n",
      "Epoch: 200 | Loss: 5.979333871091873e-08\n",
      "Epoch: 205 | Loss: 5.970479056999356e-08\n",
      "Epoch: 210 | Loss: 5.965038737815849e-08\n",
      "Epoch: 215 | Loss: 5.960809572378567e-08\n",
      "Epoch: 220 | Loss: 5.95466864893792e-08\n",
      "Epoch: 225 | Loss: 5.951372441646321e-08\n",
      "Epoch: 230 | Loss: 5.94842615322226e-08\n",
      "Epoch: 235 | Loss: 5.944597134004818e-08\n",
      "Epoch: 240 | Loss: 5.9410599891557475e-08\n",
      "Epoch: 245 | Loss: 5.9388939479754704e-08\n",
      "Epoch: 250 | Loss: 5.9357277952838585e-08\n",
      "Epoch: 255 | Loss: 5.931122755987321e-08\n",
      "Epoch: 260 | Loss: 5.926383448653598e-08\n",
      "Epoch: 265 | Loss: 5.924556260615296e-08\n",
      "Epoch: 270 | Loss: 5.9230884140400435e-08\n",
      "Epoch: 275 | Loss: 5.91774489953327e-08\n",
      "Epoch: 280 | Loss: 5.9148329267332754e-08\n",
      "Epoch: 285 | Loss: 5.911940830583913e-08\n",
      "Epoch: 290 | Loss: 5.906407272733883e-08\n",
      "Epoch: 295 | Loss: 5.902875173590932e-08\n",
      "time=375.68745398521423\n",
      "error=5.897215029300787e-08\n",
      "best loss=5.897215029300787e-08\n",
      "best epoch=299\n",
      "24\n",
      "Epoch: 0 | Loss: 4.840658861708214e-06\n",
      "Epoch: 5 | Loss: 9.943170992115225e-08\n",
      "Epoch: 10 | Loss: 9.624167522168785e-08\n",
      "Epoch: 15 | Loss: 9.498577475976955e-08\n",
      "Epoch: 20 | Loss: 9.416047708661928e-08\n",
      "Epoch: 25 | Loss: 9.355015995570307e-08\n",
      "Epoch: 30 | Loss: 9.306347651186526e-08\n",
      "Epoch: 35 | Loss: 9.265657111815015e-08\n",
      "Epoch: 40 | Loss: 9.230508256659483e-08\n",
      "Epoch: 45 | Loss: 9.199431153018975e-08\n",
      "Epoch: 50 | Loss: 9.171480351021878e-08\n",
      "Epoch: 55 | Loss: 9.146014066338663e-08\n",
      "Epoch: 60 | Loss: 9.122576048104624e-08\n",
      "Epoch: 65 | Loss: 9.10075578770979e-08\n",
      "Epoch: 70 | Loss: 9.080439163560509e-08\n",
      "Epoch: 75 | Loss: 9.061355089491969e-08\n",
      "Epoch: 80 | Loss: 9.043340374110407e-08\n",
      "Epoch: 85 | Loss: 9.026261674403803e-08\n",
      "Epoch: 90 | Loss: 9.010008406792777e-08\n",
      "Epoch: 95 | Loss: 8.994487732907874e-08\n",
      "Epoch: 100 | Loss: 8.618842763656606e-08\n",
      "Epoch: 105 | Loss: 8.602149914914766e-08\n",
      "Epoch: 110 | Loss: 8.581042402253121e-08\n",
      "Epoch: 115 | Loss: 8.563230247707486e-08\n",
      "Epoch: 120 | Loss: 8.540961230034672e-08\n",
      "Epoch: 125 | Loss: 8.525796992076637e-08\n",
      "Epoch: 130 | Loss: 8.509966168038366e-08\n",
      "Epoch: 135 | Loss: 8.493888193386147e-08\n",
      "Epoch: 140 | Loss: 8.478633555771688e-08\n",
      "Epoch: 145 | Loss: 8.465452269439249e-08\n",
      "Epoch: 150 | Loss: 8.445209940113704e-08\n",
      "Epoch: 155 | Loss: 8.428967922778379e-08\n",
      "Epoch: 160 | Loss: 8.417871877301161e-08\n",
      "Epoch: 165 | Loss: 8.407936735086813e-08\n",
      "Epoch: 170 | Loss: 8.397686174351285e-08\n",
      "Epoch: 175 | Loss: 8.385774507456547e-08\n",
      "Epoch: 180 | Loss: 8.371324304067944e-08\n",
      "Epoch: 185 | Loss: 8.355610879904626e-08\n",
      "Epoch: 190 | Loss: 8.343922878049258e-08\n",
      "Epoch: 195 | Loss: 8.327436230707381e-08\n",
      "Epoch: 200 | Loss: 7.467400596264048e-08\n",
      "Epoch: 205 | Loss: 7.435060137623377e-08\n",
      "Epoch: 210 | Loss: 7.397138933785241e-08\n",
      "Epoch: 215 | Loss: 7.359919813878579e-08\n",
      "Epoch: 220 | Loss: 7.331275670366752e-08\n",
      "Epoch: 225 | Loss: 7.294727477256112e-08\n",
      "Epoch: 230 | Loss: 7.270371019907083e-08\n",
      "Epoch: 235 | Loss: 7.235053931202333e-08\n",
      "Epoch: 240 | Loss: 7.201203680986352e-08\n",
      "Epoch: 245 | Loss: 7.171914545529717e-08\n",
      "Epoch: 250 | Loss: 7.141795794571031e-08\n",
      "Epoch: 255 | Loss: 7.118690979367734e-08\n",
      "Epoch: 260 | Loss: 7.098649540043747e-08\n",
      "Epoch: 265 | Loss: 7.067605141837624e-08\n",
      "Epoch: 270 | Loss: 7.044334999284842e-08\n",
      "Epoch: 275 | Loss: 7.015102093166514e-08\n",
      "Epoch: 280 | Loss: 6.993043636228594e-08\n",
      "Epoch: 285 | Loss: 6.968310822871852e-08\n",
      "Epoch: 290 | Loss: 6.94454920190453e-08\n",
      "Epoch: 295 | Loss: 6.923572318346432e-08\n",
      "time=421.39604806900024\n",
      "error=6.910212386763652e-08\n",
      "best loss=6.910212386763652e-08\n",
      "best epoch=299\n",
      "28\n",
      "Epoch: 0 | Loss: 2.3418355740640618e-06\n",
      "Epoch: 5 | Loss: 1.1279639665003204e-07\n",
      "Epoch: 10 | Loss: 1.0911534793227773e-07\n",
      "Epoch: 15 | Loss: 1.0784303836234848e-07\n",
      "Epoch: 20 | Loss: 1.0690716540896911e-07\n",
      "Epoch: 25 | Loss: 1.0615200562030613e-07\n",
      "Epoch: 30 | Loss: 1.0551511465354616e-07\n",
      "Epoch: 35 | Loss: 1.0496358790371411e-07\n",
      "Epoch: 40 | Loss: 1.0447745640358606e-07\n",
      "Epoch: 45 | Loss: 1.0404340743052899e-07\n",
      "Epoch: 50 | Loss: 1.0365195784581672e-07\n",
      "Epoch: 55 | Loss: 1.0329602061363984e-07\n",
      "Epoch: 60 | Loss: 1.0297011005392247e-07\n",
      "Epoch: 65 | Loss: 1.0266986823700531e-07\n",
      "Epoch: 70 | Loss: 1.0239176493551799e-07\n",
      "Epoch: 75 | Loss: 1.021320772777308e-07\n",
      "Epoch: 80 | Loss: 1.0188998739523e-07\n",
      "Epoch: 85 | Loss: 1.0166271474820244e-07\n",
      "Epoch: 90 | Loss: 1.0144853254951536e-07\n",
      "Epoch: 95 | Loss: 1.0124596993580123e-07\n",
      "Epoch: 100 | Loss: 9.727716341573379e-08\n",
      "Epoch: 105 | Loss: 9.68839913545956e-08\n",
      "Epoch: 110 | Loss: 9.660080474268468e-08\n",
      "Epoch: 115 | Loss: 9.637024715494274e-08\n",
      "Epoch: 120 | Loss: 9.612100348994064e-08\n",
      "Epoch: 125 | Loss: 9.591559472107185e-08\n",
      "Epoch: 130 | Loss: 9.57532284338844e-08\n",
      "Epoch: 135 | Loss: 9.555518644590076e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 140 | Loss: 9.535369952465863e-08\n",
      "Epoch: 145 | Loss: 9.520187797395367e-08\n",
      "Epoch: 150 | Loss: 9.490597855863287e-08\n",
      "Epoch: 155 | Loss: 9.469598722830214e-08\n",
      "Epoch: 160 | Loss: 9.448043768315425e-08\n",
      "Epoch: 165 | Loss: 9.434086474352945e-08\n",
      "Epoch: 170 | Loss: 9.412892067060506e-08\n",
      "Epoch: 175 | Loss: 9.402105010010216e-08\n",
      "Epoch: 180 | Loss: 9.383768231524456e-08\n",
      "Epoch: 185 | Loss: 9.37354117933687e-08\n",
      "Epoch: 190 | Loss: 9.346268575969955e-08\n",
      "Epoch: 195 | Loss: 9.332865125005512e-08\n",
      "Epoch: 200 | Loss: 9.289893627866961e-08\n",
      "Epoch: 205 | Loss: 9.273851549651039e-08\n",
      "Epoch: 210 | Loss: 9.256556527412947e-08\n",
      "Epoch: 215 | Loss: 9.232225701158278e-08\n",
      "Epoch: 220 | Loss: 9.213232980041596e-08\n",
      "Epoch: 225 | Loss: 9.196950397012078e-08\n",
      "Epoch: 230 | Loss: 9.181290829898104e-08\n",
      "Epoch: 235 | Loss: 9.162769372656541e-08\n",
      "Epoch: 240 | Loss: 9.143896272437151e-08\n",
      "Epoch: 245 | Loss: 9.113657190213736e-08\n",
      "Epoch: 250 | Loss: 9.10021951225229e-08\n",
      "Epoch: 255 | Loss: 9.075006001244035e-08\n",
      "Epoch: 260 | Loss: 9.057867827706064e-08\n",
      "Epoch: 265 | Loss: 9.040321877151693e-08\n",
      "Epoch: 270 | Loss: 9.023615998873379e-08\n",
      "Epoch: 275 | Loss: 9.006797094554987e-08\n",
      "Epoch: 280 | Loss: 8.990501759256267e-08\n",
      "Epoch: 285 | Loss: 8.975874517382329e-08\n",
      "Epoch: 290 | Loss: 8.957740626050736e-08\n",
      "Epoch: 295 | Loss: 8.939920139301196e-08\n",
      "time=457.3020248413086\n",
      "error=8.927199989867885e-08\n",
      "best loss=8.927199989867885e-08\n",
      "best epoch=299\n",
      "32\n",
      "Epoch: 0 | Loss: 1.593026321298333e-06\n",
      "Epoch: 5 | Loss: 1.4989837206473993e-07\n",
      "Epoch: 10 | Loss: 1.369220618426232e-07\n",
      "Epoch: 15 | Loss: 1.3561243447721627e-07\n",
      "Epoch: 20 | Loss: 1.3470404340709177e-07\n",
      "Epoch: 25 | Loss: 1.3399045087428387e-07\n",
      "Epoch: 30 | Loss: 1.333935321848964e-07\n",
      "Epoch: 35 | Loss: 1.3287372104505082e-07\n",
      "Epoch: 40 | Loss: 1.3241150385612897e-07\n",
      "Epoch: 45 | Loss: 1.3199079932439058e-07\n",
      "Epoch: 50 | Loss: 1.3160505402769637e-07\n",
      "Epoch: 55 | Loss: 1.3124562550203992e-07\n",
      "Epoch: 60 | Loss: 1.3091008871152846e-07\n",
      "Epoch: 65 | Loss: 1.3059296328429224e-07\n",
      "Epoch: 70 | Loss: 1.3029355033043025e-07\n",
      "Epoch: 75 | Loss: 1.3000789450729155e-07\n",
      "Epoch: 80 | Loss: 1.297361238085012e-07\n",
      "Epoch: 85 | Loss: 1.2947511497743815e-07\n",
      "Epoch: 90 | Loss: 1.2922542820126817e-07\n",
      "Epoch: 95 | Loss: 1.2898443912462014e-07\n",
      "Epoch: 100 | Loss: 1.2873725778187706e-07\n",
      "Epoch: 105 | Loss: 1.2843497852451424e-07\n",
      "Epoch: 110 | Loss: 1.2814727449513542e-07\n",
      "Epoch: 115 | Loss: 1.2789064460006892e-07\n",
      "Epoch: 120 | Loss: 1.276635108021017e-07\n",
      "Epoch: 125 | Loss: 1.2736420793642474e-07\n",
      "Epoch: 130 | Loss: 1.2711214681715156e-07\n",
      "Epoch: 135 | Loss: 1.2689512602643866e-07\n",
      "Epoch: 140 | Loss: 1.2668947545183848e-07\n",
      "Epoch: 145 | Loss: 1.264280927772767e-07\n",
      "Epoch: 150 | Loss: 1.2624320104105787e-07\n",
      "Epoch: 155 | Loss: 1.2604804520636748e-07\n",
      "Epoch: 160 | Loss: 1.2581972512253735e-07\n",
      "Epoch: 165 | Loss: 1.1767964105165334e-07\n",
      "Epoch: 170 | Loss: 1.1752140017757866e-07\n",
      "Epoch: 175 | Loss: 1.1737203134695945e-07\n",
      "Epoch: 180 | Loss: 1.172059632734667e-07\n",
      "Epoch: 185 | Loss: 1.1706447328285836e-07\n",
      "Epoch: 190 | Loss: 1.1685016647326387e-07\n",
      "Epoch: 195 | Loss: 1.1666924123899334e-07\n",
      "Epoch: 200 | Loss: 1.1648291323468604e-07\n",
      "Epoch: 205 | Loss: 1.161846761712344e-07\n",
      "Epoch: 210 | Loss: 1.159781286140668e-07\n",
      "Epoch: 215 | Loss: 1.1572713252229641e-07\n",
      "Epoch: 220 | Loss: 1.1544573573780162e-07\n",
      "Epoch: 225 | Loss: 1.1525693109643221e-07\n",
      "Epoch: 230 | Loss: 1.1496622916740362e-07\n",
      "Epoch: 235 | Loss: 1.1475662676838093e-07\n",
      "Epoch: 240 | Loss: 1.1453551496434383e-07\n",
      "Epoch: 245 | Loss: 1.1432764960950772e-07\n",
      "Epoch: 250 | Loss: 1.1411657783683874e-07\n",
      "Epoch: 255 | Loss: 1.1389916411167544e-07\n",
      "Epoch: 260 | Loss: 1.1368526397027323e-07\n",
      "Epoch: 265 | Loss: 1.134983053663156e-07\n",
      "Epoch: 270 | Loss: 1.1327311259816413e-07\n",
      "Epoch: 275 | Loss: 1.1313142849098357e-07\n",
      "Epoch: 280 | Loss: 1.1294376006538293e-07\n",
      "Epoch: 285 | Loss: 1.1279271749460327e-07\n",
      "Epoch: 290 | Loss: 1.1261429538471804e-07\n",
      "Epoch: 295 | Loss: 1.1241582310085063e-07\n",
      "time=490.54156017303467\n",
      "error=1.1225528452681441e-07\n",
      "best loss=1.1225528452681441e-07\n",
      "best epoch=299\n",
      "36\n",
      "Epoch: 0 | Loss: 1.0586968075927825e-06\n",
      "Epoch: 5 | Loss: 1.8080633124667957e-07\n",
      "Epoch: 10 | Loss: 1.7638413316509647e-07\n",
      "Epoch: 15 | Loss: 1.7365361812778844e-07\n",
      "Epoch: 20 | Loss: 1.715277618599195e-07\n",
      "Epoch: 25 | Loss: 1.6977756212826194e-07\n",
      "Epoch: 30 | Loss: 1.6828761589923205e-07\n",
      "Epoch: 35 | Loss: 1.6698757192572963e-07\n",
      "Epoch: 40 | Loss: 1.6583153236552833e-07\n",
      "Epoch: 45 | Loss: 1.6478827020378777e-07\n",
      "Epoch: 50 | Loss: 1.6383582495737665e-07\n",
      "Epoch: 55 | Loss: 1.6295582083727672e-07\n",
      "Epoch: 60 | Loss: 1.621415100132232e-07\n",
      "Epoch: 65 | Loss: 1.6138127276455268e-07\n",
      "Epoch: 70 | Loss: 1.606680825356701e-07\n",
      "Epoch: 75 | Loss: 1.5999633947227718e-07\n",
      "Epoch: 80 | Loss: 1.5935967717962685e-07\n",
      "Epoch: 85 | Loss: 1.5875801539359728e-07\n",
      "Epoch: 90 | Loss: 1.5818632427222418e-07\n",
      "Epoch: 95 | Loss: 1.5764190890515898e-07\n",
      "Epoch: 100 | Loss: 1.458270564665293e-07\n",
      "Epoch: 105 | Loss: 1.4554299511744792e-07\n",
      "Epoch: 110 | Loss: 1.4528373478024973e-07\n",
      "Epoch: 115 | Loss: 1.4491454771554556e-07\n",
      "Epoch: 120 | Loss: 1.4465950137709877e-07\n",
      "Epoch: 125 | Loss: 1.4443997264885958e-07\n",
      "Epoch: 130 | Loss: 1.44115851422936e-07\n",
      "Epoch: 135 | Loss: 1.439141526315558e-07\n",
      "Epoch: 140 | Loss: 1.4364323895261166e-07\n",
      "Epoch: 145 | Loss: 1.4342205541480528e-07\n",
      "Epoch: 150 | Loss: 1.4319519598878492e-07\n",
      "Epoch: 155 | Loss: 1.4293632966493882e-07\n",
      "Epoch: 160 | Loss: 1.4275065080638268e-07\n",
      "Epoch: 165 | Loss: 1.4257741634178586e-07\n",
      "Epoch: 170 | Loss: 1.4235568093410776e-07\n",
      "Epoch: 175 | Loss: 1.4215928517648948e-07\n",
      "Epoch: 180 | Loss: 1.4197661354153162e-07\n",
      "Epoch: 185 | Loss: 1.417689549736284e-07\n",
      "Epoch: 190 | Loss: 1.4161432983873318e-07\n",
      "Epoch: 195 | Loss: 1.4145273690325594e-07\n",
      "Epoch: 200 | Loss: 1.4118645330088903e-07\n",
      "Epoch: 205 | Loss: 1.408760070036065e-07\n",
      "Epoch: 210 | Loss: 1.4053887358586315e-07\n",
      "Epoch: 215 | Loss: 1.4025178414484027e-07\n",
      "Epoch: 220 | Loss: 1.399879948605345e-07\n",
      "Epoch: 225 | Loss: 1.3979970714288246e-07\n",
      "Epoch: 230 | Loss: 1.3957689807811894e-07\n",
      "Epoch: 235 | Loss: 1.3931956104044037e-07\n",
      "Epoch: 240 | Loss: 1.3912861885252018e-07\n",
      "Epoch: 245 | Loss: 1.3893484576258552e-07\n",
      "Epoch: 250 | Loss: 1.3868114997709412e-07\n",
      "Epoch: 255 | Loss: 1.3844150333188669e-07\n",
      "Epoch: 260 | Loss: 1.3819777179564636e-07\n",
      "Epoch: 265 | Loss: 1.2480824142953066e-07\n",
      "Epoch: 270 | Loss: 1.2452738570546254e-07\n",
      "Epoch: 275 | Loss: 1.2420410940646357e-07\n",
      "Epoch: 280 | Loss: 1.238505058219791e-07\n",
      "Epoch: 285 | Loss: 1.2349465814419705e-07\n",
      "Epoch: 290 | Loss: 1.232010189694006e-07\n",
      "Epoch: 295 | Loss: 1.2290603299912066e-07\n",
      "time=461.4831790924072\n",
      "error=1.2253848141206057e-07\n",
      "best loss=1.2253848141206057e-07\n",
      "best epoch=299\n",
      "40\n",
      "Epoch: 0 | Loss: 7.325953821785196e-07\n",
      "Epoch: 5 | Loss: 1.6519342751099417e-07\n",
      "Epoch: 10 | Loss: 1.6204248846937766e-07\n",
      "Epoch: 15 | Loss: 1.605710509046219e-07\n",
      "Epoch: 20 | Loss: 1.5941403570100126e-07\n",
      "Epoch: 25 | Loss: 1.5844002647593712e-07\n",
      "Epoch: 30 | Loss: 1.5759927946486456e-07\n",
      "Epoch: 35 | Loss: 1.5685232720523916e-07\n",
      "Epoch: 40 | Loss: 1.561819548447941e-07\n",
      "Epoch: 45 | Loss: 1.5556797023925828e-07\n",
      "Epoch: 50 | Loss: 1.5500346356933856e-07\n",
      "Epoch: 55 | Loss: 1.544762757918806e-07\n",
      "Epoch: 60 | Loss: 1.5398380168827222e-07\n",
      "Epoch: 65 | Loss: 1.5351789667573102e-07\n",
      "Epoch: 70 | Loss: 1.5307794474733322e-07\n",
      "Epoch: 75 | Loss: 1.52658026315086e-07\n",
      "Epoch: 80 | Loss: 1.5225848822863718e-07\n",
      "Epoch: 85 | Loss: 1.5187474516385787e-07\n",
      "Epoch: 90 | Loss: 1.515076162535974e-07\n",
      "Epoch: 95 | Loss: 1.5115337264937585e-07\n",
      "Epoch: 100 | Loss: 1.5080911711133156e-07\n",
      "Epoch: 105 | Loss: 1.5029232880206903e-07\n",
      "Epoch: 110 | Loss: 1.4980915239269224e-07\n",
      "Epoch: 115 | Loss: 1.4943191256026635e-07\n",
      "Epoch: 120 | Loss: 1.3919899699343448e-07\n",
      "Epoch: 125 | Loss: 1.3895573297657067e-07\n",
      "Epoch: 130 | Loss: 1.387152238684147e-07\n",
      "Epoch: 135 | Loss: 1.3835047726395018e-07\n",
      "Epoch: 140 | Loss: 1.380197030422042e-07\n",
      "Epoch: 145 | Loss: 1.3770359009225144e-07\n",
      "Epoch: 150 | Loss: 1.372390026449057e-07\n",
      "Epoch: 155 | Loss: 1.3701740587755642e-07\n",
      "Epoch: 160 | Loss: 1.3678323749010451e-07\n",
      "Epoch: 165 | Loss: 1.3656104009855839e-07\n",
      "Epoch: 170 | Loss: 1.3637570209733352e-07\n",
      "Epoch: 175 | Loss: 1.3603914578758753e-07\n",
      "Epoch: 180 | Loss: 1.3570507870411697e-07\n",
      "Epoch: 185 | Loss: 1.35487398931409e-07\n",
      "Epoch: 190 | Loss: 1.352463339613522e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 195 | Loss: 1.3500727384801518e-07\n",
      "Epoch: 200 | Loss: 1.3463742912932016e-07\n",
      "Epoch: 205 | Loss: 1.3432936389888357e-07\n",
      "Epoch: 210 | Loss: 1.340797784927932e-07\n",
      "Epoch: 215 | Loss: 1.1923373792681955e-07\n",
      "Epoch: 220 | Loss: 9.69302140901968e-08\n",
      "Epoch: 225 | Loss: 6.60387097824517e-08\n",
      "Epoch: 230 | Loss: 4.3721188508563305e-08\n",
      "Epoch: 235 | Loss: 3.145542676173172e-08\n",
      "Epoch: 240 | Loss: 2.72099391136043e-08\n",
      "Epoch: 245 | Loss: 2.623984710566663e-08\n",
      "Epoch: 250 | Loss: 2.5543202994310415e-08\n",
      "Epoch: 255 | Loss: 2.4959779841260023e-08\n",
      "Epoch: 260 | Loss: 2.4481034736162003e-08\n",
      "Epoch: 265 | Loss: 2.4043607300273812e-08\n",
      "Epoch: 270 | Loss: 1.8930654287607934e-08\n",
      "Epoch: 275 | Loss: 1.5928261630309317e-08\n",
      "Epoch: 280 | Loss: 1.2790689241486018e-08\n",
      "Epoch: 285 | Loss: 1.2256790210556785e-08\n",
      "Epoch: 290 | Loss: 1.1997451761000287e-08\n",
      "Epoch: 295 | Loss: 1.1801106403986041e-08\n",
      "time=554.3265552520752\n",
      "error=1.1596785528276286e-08\n",
      "best loss=1.1596785528276286e-08\n",
      "best epoch=299\n",
      "44\n",
      "Epoch: 0 | Loss: 1.0420787190500924e-06\n",
      "Epoch: 5 | Loss: 1.8383197782618437e-08\n",
      "Epoch: 10 | Loss: 1.1817043327736658e-08\n",
      "Epoch: 15 | Loss: 1.146605875388077e-08\n",
      "Epoch: 20 | Loss: 1.1202738437846921e-08\n",
      "Epoch: 25 | Loss: 1.0979122639029347e-08\n",
      "Epoch: 30 | Loss: 1.0779288757317774e-08\n",
      "Epoch: 35 | Loss: 1.0599388552020731e-08\n",
      "Epoch: 40 | Loss: 1.0433278120671329e-08\n",
      "Epoch: 45 | Loss: 1.0280120135109349e-08\n",
      "Epoch: 50 | Loss: 1.0138934240290793e-08\n",
      "Epoch: 55 | Loss: 1.0006727833022563e-08\n",
      "Epoch: 60 | Loss: 9.883616221874466e-09\n",
      "Epoch: 65 | Loss: 9.769050739492135e-09\n",
      "Epoch: 70 | Loss: 9.661237379145459e-09\n",
      "Epoch: 75 | Loss: 9.56079299176259e-09\n",
      "Epoch: 80 | Loss: 9.465827106998568e-09\n",
      "Epoch: 85 | Loss: 9.376663600822552e-09\n",
      "Epoch: 90 | Loss: 9.29313149862419e-09\n",
      "Epoch: 95 | Loss: 9.21420526463551e-09\n",
      "Epoch: 100 | Loss: 8.992858266259487e-09\n",
      "Epoch: 105 | Loss: 8.841714209153322e-09\n",
      "Epoch: 110 | Loss: 8.738097107847843e-09\n",
      "Epoch: 115 | Loss: 8.655427831671989e-09\n",
      "Epoch: 120 | Loss: 8.521401636329789e-09\n",
      "Epoch: 125 | Loss: 8.437205547123425e-09\n",
      "Epoch: 130 | Loss: 8.378316081462165e-09\n",
      "Epoch: 135 | Loss: 8.317160644314844e-09\n",
      "Epoch: 140 | Loss: 8.27038274916863e-09\n",
      "Epoch: 145 | Loss: 8.237762556747681e-09\n",
      "Epoch: 150 | Loss: 8.198239245831482e-09\n",
      "Epoch: 155 | Loss: 8.185061179472444e-09\n",
      "Epoch: 160 | Loss: 8.167474255692501e-09\n",
      "Epoch: 165 | Loss: 8.108607238443129e-09\n",
      "Epoch: 170 | Loss: 8.083019654178993e-09\n",
      "Epoch: 175 | Loss: 8.037157949517209e-09\n",
      "Epoch: 180 | Loss: 8.030040277582204e-09\n",
      "Epoch: 185 | Loss: 7.982182676674222e-09\n",
      "Epoch: 190 | Loss: 7.92746701623347e-09\n",
      "Epoch: 195 | Loss: 7.904384300440346e-09\n",
      "Epoch: 200 | Loss: 7.883532870012087e-09\n",
      "Epoch: 205 | Loss: 7.8683303859353e-09\n",
      "Epoch: 210 | Loss: 7.859161497550842e-09\n",
      "Epoch: 215 | Loss: 7.850893617027764e-09\n",
      "Epoch: 220 | Loss: 7.843115105554958e-09\n",
      "Epoch: 225 | Loss: 7.836514774229502e-09\n",
      "Epoch: 230 | Loss: 7.829027560992898e-09\n",
      "Epoch: 235 | Loss: 7.82265090655569e-09\n",
      "Epoch: 240 | Loss: 7.815899637706717e-09\n",
      "Epoch: 245 | Loss: 7.808930183811877e-09\n",
      "Epoch: 250 | Loss: 7.800353487373745e-09\n",
      "Epoch: 255 | Loss: 7.793565849718358e-09\n",
      "Epoch: 260 | Loss: 7.785765099563572e-09\n",
      "Epoch: 265 | Loss: 7.77863725563308e-09\n",
      "Epoch: 270 | Loss: 7.771236648065143e-09\n",
      "Epoch: 275 | Loss: 7.764708257437455e-09\n",
      "Epoch: 280 | Loss: 7.758704937774455e-09\n",
      "Epoch: 285 | Loss: 7.751321266369155e-09\n",
      "Epoch: 290 | Loss: 7.745233558441863e-09\n",
      "Epoch: 295 | Loss: 7.739261905221165e-09\n",
      "time=482.9697120189667\n",
      "error=7.734805411520295e-09\n",
      "best loss=7.734805411520295e-09\n",
      "best epoch=299\n",
      "48\n",
      "Epoch: 0 | Loss: 2.0283705110528366e-07\n",
      "Epoch: 5 | Loss: 3.060201075609048e-08\n",
      "Epoch: 10 | Loss: 2.9585458188789106e-08\n",
      "Epoch: 15 | Loss: 2.8885395440920162e-08\n",
      "Epoch: 20 | Loss: 2.8325814819444395e-08\n",
      "Epoch: 25 | Loss: 2.784734100913653e-08\n",
      "Epoch: 30 | Loss: 2.741568989399272e-08\n",
      "Epoch: 35 | Loss: 2.701902575471654e-08\n",
      "Epoch: 40 | Loss: 2.664558297838225e-08\n",
      "Epoch: 45 | Loss: 2.6292119976771855e-08\n",
      "Epoch: 50 | Loss: 2.5954039294896987e-08\n",
      "Epoch: 55 | Loss: 2.5630556875076988e-08\n",
      "Epoch: 60 | Loss: 2.5318695741203065e-08\n",
      "Epoch: 65 | Loss: 2.501871764122376e-08\n",
      "Epoch: 70 | Loss: 2.4728203361801484e-08\n",
      "Epoch: 75 | Loss: 2.4448071710221563e-08\n",
      "Epoch: 80 | Loss: 2.417549620347409e-08\n",
      "Epoch: 85 | Loss: 2.391224834135796e-08\n",
      "Epoch: 90 | Loss: 2.3655809737629145e-08\n",
      "Epoch: 95 | Loss: 2.340704172189212e-08\n",
      "Epoch: 100 | Loss: 2.312362330578246e-08\n",
      "Epoch: 105 | Loss: 2.2738044787302985e-08\n",
      "Epoch: 110 | Loss: 2.2453893749554317e-08\n",
      "Epoch: 115 | Loss: 2.2162999522596003e-08\n",
      "Epoch: 120 | Loss: 2.1855749144483628e-08\n",
      "Epoch: 125 | Loss: 2.1613013975777175e-08\n",
      "Epoch: 130 | Loss: 2.1322186249982392e-08\n",
      "Epoch: 135 | Loss: 2.105063060302233e-08\n",
      "Epoch: 140 | Loss: 2.0788634113905805e-08\n",
      "Epoch: 145 | Loss: 2.0501591315132592e-08\n",
      "Epoch: 150 | Loss: 2.0259725625859472e-08\n",
      "Epoch: 155 | Loss: 1.9997512858834156e-08\n",
      "Epoch: 160 | Loss: 1.9606222586550025e-08\n",
      "Epoch: 165 | Loss: 1.9395099924864936e-08\n",
      "Epoch: 170 | Loss: 1.920061863532925e-08\n",
      "Epoch: 175 | Loss: 1.9026192742041334e-08\n",
      "Epoch: 180 | Loss: 1.883333958603241e-08\n",
      "Epoch: 185 | Loss: 1.8632482514505447e-08\n",
      "Epoch: 190 | Loss: 1.8472519913913258e-08\n",
      "Epoch: 195 | Loss: 1.8290253230000688e-08\n",
      "Epoch: 200 | Loss: 1.814524561488613e-08\n",
      "Epoch: 205 | Loss: 1.8046612085260647e-08\n",
      "Epoch: 210 | Loss: 1.7924199873534447e-08\n",
      "Epoch: 215 | Loss: 1.7833737992309042e-08\n",
      "Epoch: 220 | Loss: 1.7757166864226235e-08\n",
      "Epoch: 225 | Loss: 1.7672905228702567e-08\n",
      "Epoch: 230 | Loss: 1.755463666113122e-08\n",
      "Epoch: 235 | Loss: 1.747960410462093e-08\n",
      "Epoch: 240 | Loss: 1.7402363279161147e-08\n",
      "Epoch: 245 | Loss: 1.73332931011293e-08\n",
      "Epoch: 250 | Loss: 1.7216240302339696e-08\n",
      "Epoch: 255 | Loss: 1.7125438124021254e-08\n",
      "Epoch: 260 | Loss: 1.7043465068913636e-08\n",
      "Epoch: 265 | Loss: 1.6893714629727883e-08\n",
      "Epoch: 270 | Loss: 1.6816882703511135e-08\n",
      "Epoch: 275 | Loss: 1.6727789773427463e-08\n",
      "Epoch: 280 | Loss: 1.6646529924624258e-08\n",
      "Epoch: 285 | Loss: 1.6548683505058552e-08\n",
      "Epoch: 290 | Loss: 1.6450758689236025e-08\n",
      "Epoch: 295 | Loss: 1.639888366400474e-08\n",
      "time=441.8924820423126\n",
      "error=1.6348916824168993e-08\n",
      "best loss=1.6348916824168993e-08\n",
      "best epoch=299\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy import interpolate\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import LBFGS\n",
    "\n",
    "interp_mode = \"bilinear\"\n",
    "align_corners = True\n",
    "\n",
    "\n",
    "#n_grids = [12,16,20,24,28,32,36,40]\n",
    "#n_grids = [12,16]\n",
    "#n_grids = [6,10,14,20,30,40,50]\n",
    "#n_grids = [6,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,200,300,400,500,600,700,800,900,1000]\n",
    "#n_grids = [10,20,30,40,50,60,70,80,90,100]\n",
    "n_grids = [6,8,12,16,20,24,28,32,36,40,44,48]\n",
    "#n_grids = [20]\n",
    "#n_grids = [200]\n",
    "theta_start = 0\n",
    "theta_end = np.pi/2\n",
    "r_start = 3\n",
    "r_end = 4\n",
    "\n",
    "a = 0.0\n",
    "M = 1\n",
    "\n",
    "errors = []\n",
    "times = []\n",
    "ii = 0\n",
    "\n",
    "\n",
    "for n_grid in n_grids:\n",
    "    print(n_grid)\n",
    "\n",
    "    thetas = torch.linspace(theta_start,theta_end,steps=n_grid, dtype=torch.double)\n",
    "    rs = torch.linspace(r_start,r_end,steps=n_grid, dtype=torch.double)\n",
    "    theta_h = (theta_end - theta_start)/(n_grid-1)\n",
    "    r_h  = (r_end - r_start)/(n_grid-1)\n",
    "\n",
    "    RS, THETAS = torch.meshgrid(rs, thetas)\n",
    "    # Transpose here is very important! Becareful of meshgrid and reshape stuff!\n",
    "    #RS = torch.transpose(RS,0,1)\n",
    "    #THETAS = torch.transpose(THETAS,0,1)\n",
    "    z = torch.transpose(torch.stack([RS.reshape(-1,), THETAS.reshape(-1,)]),0,1)\n",
    "    \n",
    "    #print(z)\n",
    "\n",
    "    # t' = t + f1(r,theta)\n",
    "    def f1(f1_free, n_grid):\n",
    "        # f1_free has shape (n_grid, n_grid-1).\n",
    "        # Along theta, zero derivative at theta=pi/2\n",
    "        f1_ = torch.zeros(n_grid,n_grid, dtype=torch.double)\n",
    "        f1_[:,:-1] = f1_free\n",
    "        f1_[:,-1] = f1_free[:,-1]\n",
    "        return f1_\n",
    "\n",
    "    # r' = r + f2(r,theta)\n",
    "    def f2(f2_free, n_grid):\n",
    "        # f2_free has shape (n_grid, n_grid-1).\n",
    "        # Along theta, zero derivative at theta=pi/2\n",
    "        f2_ = torch.zeros(n_grid,n_grid, dtype=torch.double)\n",
    "        f2_[:,:-1] = f2_free\n",
    "        f2_[:,-1] = f2_free[:,-1]\n",
    "        return f2_\n",
    "\n",
    "    # theta' = theta + f3(r,theta)\n",
    "    def f3(f3_free, n_grid):\n",
    "        # f3_free has shape (n_grid, n_grid-2)\n",
    "        # Along theta, zero at theta=0 and theta=pi/2\n",
    "        f3_ = torch.zeros(n_grid,n_grid, dtype=torch.double)\n",
    "        f3_[:,1:-1] = f3_free\n",
    "        return f3_\n",
    "\n",
    "    # phi' = phi + f4(r,theta)\n",
    "    def f4(f4_free, n_grid):\n",
    "        # f4_free has shape (n_grid, n_grid-1).\n",
    "        # Along theta, zero derivative at theta=pi/2\n",
    "        f4_ = torch.zeros(n_grid,n_grid, dtype=torch.double)\n",
    "        f4_[:,:-1] = f4_free\n",
    "        f4_[:,-1] = f4_free[:,-1]\n",
    "        return f4_\n",
    "    \n",
    "    \n",
    "    def interp_free(f_free, n_grid, mode=\"0\"):\n",
    "        n_grid_old = f_free.shape[0]\n",
    "        if mode == \"0\":\n",
    "            f_ = f1(f_free, n_grid_old)\n",
    "        else:\n",
    "            f_ = f3(f_free, n_grid_old)\n",
    "        f_free_std = f_.unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "        if mode == \"0\":\n",
    "            f_free_new = F.interpolate(f_free_std, size=(n_grid,n_grid), mode=interp_mode, align_corners=align_corners)[0,0,:,:-1]\n",
    "        else:\n",
    "            f_free_new = F.interpolate(f_free_std, size=(n_grid,n_grid), mode=interp_mode, align_corners=align_corners)[0,0,:,1:-1]\n",
    "        return f_free_new#torch.transpose(f_free_new,0,1)#\n",
    "\n",
    "    def interp_free_test(f_free, n_grid, mode=\"0\"):\n",
    "        if mode == \"0\":\n",
    "            f_ = f1(f_free, n_grid)\n",
    "        else:\n",
    "            f_ = f3(f_free, n_grid)\n",
    "        f_free_std = f_.unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "        if mode == \"0\":\n",
    "            f_free_new = F.interpolate(f_free_std, size=(2*n_grid-1,2*n_grid-1), mode=interp_mode, align_corners=align_corners)[0,0,1:-1:2,1:-3:2]\n",
    "        else:\n",
    "            f_free_new = F.interpolate(f_free_std, size=(2*n_grid-1,2*n_grid-1), mode=interp_mode, align_corners=align_corners)[0,0,1:-1:2,3:-3:2]\n",
    "        return f_free_new#torch.transpose(f_free_new,0,1)#\n",
    "\n",
    "    def decompose_free(f_free, n_grid):\n",
    "        f1_free = f_free[:,:n_grid-1]\n",
    "        f2_free = f_free[:,n_grid-1:2*n_grid-2]\n",
    "        f3_free = f_free[:,2*n_grid-2:3*n_grid-4]\n",
    "        f4_free = f_free[:,3*n_grid-4:4*n_grid-5]\n",
    "        return (f1_free,f2_free,f3_free,f4_free)\n",
    "\n",
    "    def compose_free(f1_free,f2_free,f3_free,f4_free):\n",
    "        return torch.cat([f1_free, f2_free, f3_free, f4_free], dim=1)\n",
    "\n",
    "    def interp_f_free(f_free, n_grid, n_grid_old):\n",
    "        f1_free, f2_free, f3_free, f4_free = decompose_free(f_free, n_grid_old)\n",
    "        f1_free_new = interp_free(f1_free, n_grid, mode=\"0\")\n",
    "        f2_free_new = interp_free(f2_free, n_grid, mode=\"0\")\n",
    "        f3_free_new = interp_free(f3_free, n_grid, mode=\"1\")\n",
    "        f4_free_new = interp_free(f4_free, n_grid, mode=\"0\")\n",
    "        f_free_new = compose_free(f1_free_new, f2_free_new, f3_free_new, f4_free_new)\n",
    "        return f_free_new\n",
    "\n",
    "\n",
    "    def interp_f_free_test(f_free, n_grid):\n",
    "        f_free = f_free.reshape(n_grid, 4*n_grid-5)\n",
    "        f1_free, f2_free, f3_free, f4_free = decompose_free(f_free, n_grid)\n",
    "        f1_free_new = interp_free_test(f1_free, n_grid, mode=\"0\")\n",
    "        f2_free_new = interp_free_test(f2_free, n_grid, mode=\"0\")\n",
    "        f3_free_new = interp_free_test(f3_free, n_grid, mode=\"1\")\n",
    "        f4_free_new = interp_free_test(f4_free, n_grid, mode=\"0\")\n",
    "        #print(f1_free_new.shape, f2_free_new.shape, f3_free_new.shape, f4_free_new.shape)\n",
    "        f_free_new = compose_free(f1_free_new, f2_free_new, f3_free_new, f4_free_new)\n",
    "        return f_free_new\n",
    "\n",
    "\n",
    "    def r_derivative(f, n_grid):\n",
    "        f_aug = torch.zeros(n_grid+2,n_grid,dtype=torch.double)\n",
    "        f_aug[1:-1] = f\n",
    "        f_aug[0] = 2*f[0] - f[1]\n",
    "        f_aug[-1] = 2*f[-1] - f[-2]\n",
    "        f_r = (f_aug[2:] - f_aug[:-2])/(2*r_h)\n",
    "        return f_r\n",
    "\n",
    "    def theta_derivative(f, n_grid):\n",
    "        f_aug = torch.zeros(n_grid,n_grid+2,dtype=torch.double)\n",
    "        f_aug[:,1:-1] = f\n",
    "        f_aug[:,0] = 2*f[:,0] - f[:,1]\n",
    "        f_aug[:,-1] = 2*f[:,-1] - f[:,-2]\n",
    "        f_theta = (f_aug[:,2:] - f_aug[:,:-2])/(2*theta_h)\n",
    "        return f_theta\n",
    "\n",
    "    def w(f1,f2,f3,f4, n_grid):\n",
    "        f1_r = r_derivative(f1, n_grid).reshape(-1,)\n",
    "        f2_r = r_derivative(f2, n_grid).reshape(-1,)\n",
    "        f3_r = r_derivative(f3, n_grid).reshape(-1,)\n",
    "        f4_r = r_derivative(f4, n_grid).reshape(-1,)\n",
    "        f1_theta = theta_derivative(f1, n_grid).reshape(-1,)\n",
    "        f2_theta = theta_derivative(f2, n_grid).reshape(-1,)\n",
    "        f3_theta = theta_derivative(f3, n_grid).reshape(-1,)\n",
    "        f4_theta = theta_derivative(f4, n_grid).reshape(-1,)\n",
    "        ones = torch.ones(f1_r.shape[0], dtype=torch.double)\n",
    "\n",
    "        stack1 = torch.stack([ones, f1_r, f1_theta, 0*ones])\n",
    "        stack2 = torch.stack([0*ones, 1+f2_r, f2_theta, 0*ones])\n",
    "        stack3 = torch.stack([0*ones, f3_r, 1+f3_theta, 0*ones])\n",
    "        stack4 = torch.stack([0*ones, f4_r, f4_theta, ones])\n",
    "        w_ = torch.stack([stack1, stack2, stack3, stack4])\n",
    "        w_ = w_.permute(2,0,1)\n",
    "        return w_\n",
    "\n",
    "    def w_inv_invt(w):\n",
    "        w_inv = torch.linalg.inv(w)\n",
    "        w_invt = w_inv.permute(0,2,1)\n",
    "        return w_inv, w_invt\n",
    "\n",
    "    def gp(g, w):\n",
    "        w_inv, w_invt = w_inv_invt(w)\n",
    "        gp_ = torch.matmul(torch.matmul(w_invt, g), w_inv)\n",
    "        return gp_\n",
    "\n",
    "    def zp(z, f2, f3):\n",
    "        f2 = f2.reshape(-1,)\n",
    "        f3 = f3.reshape(-1,)\n",
    "        rp = z[:,0] + f2\n",
    "        thetap = z[:,1] + f3\n",
    "        zp_ = torch.transpose(torch.stack([rp, thetap]),0,1)\n",
    "        return zp_\n",
    "\n",
    "    def g(x_, a=0.0):\n",
    "        r = x_[:,0]\n",
    "        theta = x_[:,1]\n",
    "        bs = x_.shape[0]\n",
    "        Sigma = r**2 + a**2*np.cos(theta)**2\n",
    "        Delta = r**2 - 2*M*r + a**2\n",
    "        one = torch.ones(bs, dtype=torch.double)\n",
    "        g01 = g02 = g10 = g12 = g13 = g20 = g21 = g23 = g31 = g32 = 0*one\n",
    "        g00 = -(1-2*M*r/Sigma)\n",
    "        g03 = g30 = -2*M*a*r*torch.sin(theta)**2/Sigma\n",
    "        g11 = Sigma/Delta\n",
    "        g22 = Sigma\n",
    "        g33 = (r**2+a**2+2*M*a**2*r*torch.sin(theta)**2/Sigma)*torch.sin(theta)**2\n",
    "        #print(g00.shape, g01.shape, g02.shape, g03.shape)\n",
    "        stack1 = torch.stack([g00, g01, g02, g03])\n",
    "        stack2 = torch.stack([g10, g11, g12, g13])\n",
    "        stack3 = torch.stack([g20, g21, g22, g23])\n",
    "        stack4 = torch.stack([g30, g31, g32, g33])\n",
    "        gs = torch.stack([stack1, stack2, stack3, stack4]).permute(2,0,1)\n",
    "        return gs\n",
    "\n",
    "    def gp_space_target(zp):\n",
    "        bs = zp.shape[0]\n",
    "        one = torch.ones(bs, dtype=torch.double)\n",
    "        g11 = one\n",
    "        g12 = g13 = g21 = g23 = g31 = g32 = 0*one\n",
    "        g22 = zp[:,0]**2\n",
    "        g33 = zp[:,0]**2*torch.sin(zp[:,1])**2\n",
    "        stack1 = torch.stack([g11,g12,g13])\n",
    "        stack2 = torch.stack([g21,g22,g23])\n",
    "        stack3 = torch.stack([g31,g32,g33])\n",
    "        gs = torch.stack([stack1, stack2, stack3]).permute(2,0,1)\n",
    "        return gs\n",
    "\n",
    "    def error(f_free, mode=\"train\"):\n",
    "        f_free = f_free.reshape(n_grid, 4*n_grid-5)\n",
    "        f1_free, f2_free, f3_free, f4_free = decompose_free(f_free, n_grid)\n",
    "        f1_ = f1(f1_free, n_grid)\n",
    "        f2_ = f2(f2_free, n_grid)\n",
    "        f3_ = f3(f3_free, n_grid)\n",
    "        f4_ = f4(f4_free, n_grid)\n",
    "        g_ = g(z, a=a)\n",
    "        w_ = w(f1_,f2_,f3_,f4_, n_grid)\n",
    "        zp_ = zp(z,f2_,f3_)\n",
    "        gp_space = gp(g_, w_)[:,1:,1:].reshape(n_grid,n_grid,3,3)\n",
    "        #print(gp_space): inconsistent here\n",
    "        \n",
    "        gp_space_target_ = gp_space_target(zp_).reshape(n_grid,n_grid,3,3)\n",
    "        if mode == \"train\":\n",
    "            error_ = torch.mean((gp_space-gp_space_target_)[1:-1,:]**2)\n",
    "        else:\n",
    "            error_ = torch.mean((gp_space-gp_space_target_)[1:-1,1:-1]**2)\n",
    "        return error_\n",
    "    \n",
    "    # Initialize next grid with the former solution\n",
    "    if ii == 0:\n",
    "    #if True:\n",
    "        f_free = torch.zeros((n_grid,4*n_grid-5), dtype=torch.double)\n",
    "        #f_free[:,:n_grid-1] = 1*M*(2*np.sqrt(rs/(2*M)) + 0.5*np.log((np.sqrt(rs/(2*M))-1)/(np.sqrt(rs/(2*M))+1)))[:,np.newaxis]\n",
    "        #f_free[:,:n_grid-1] = 2*M*(2*np.sqrt(rs/(2*M)) + 1*np.log((np.sqrt(rs/(2*M))-1)/(np.sqrt(rs/(2*M))+1)))[:,np.newaxis]\n",
    "        #f_free[:,:n_grid-1] = - 2*M*(2*np.sqrt(rs/(2*M)) - 1*np.log((np.sqrt(rs/(2*M))-1)/(np.sqrt(rs/(2*M))+1)))[:,np.newaxis]\n",
    "        f_free[:,:n_grid-1] = 0.001*(rs[:,np.newaxis] + thetas[np.newaxis,:-1])\n",
    "        f_free[:,n_grid-1:2*(n_grid-1)] = 0.001*(rs[:,np.newaxis] + thetas[np.newaxis,:-1])\n",
    "        f_free[:,2*(n_grid-1):3*n_grid-4] = 0.001*(rs[:,np.newaxis] + thetas[np.newaxis,1:-1])\n",
    "        f_free[:,3*n_grid-4:4*n_grid-5] = 0.001*(rs[:,np.newaxis] + thetas[np.newaxis,:-1])\n",
    "        f_free = f_free.reshape(-1,)\n",
    "        f_free = torch.nn.Parameter(f_free, requires_grad=True)\n",
    "        #print(f_free.shape)\n",
    "    else:\n",
    "        #best_free = torch.load('./results_grid/a_%.3f_grid_%d'%(1.0,50))\n",
    "        #f_free_old = best_free.reshape(50, 4*50-5)\n",
    "        #f_free = interp_f_free(f_free_old, n_grid, 50).reshape(-1,)\n",
    "        f_free_old = best_free.reshape(n_grid_old, 4*n_grid_old-5)\n",
    "        #print(f_free_old.shape)\n",
    "        #print(n_grid, n_grid_old)\n",
    "        f_free = interp_f_free(f_free_old, n_grid, n_grid_old).reshape(-1,)\n",
    "        f_free = torch.nn.Parameter(f_free, requires_grad=True)\n",
    "        #print(interp_f_free(f_free_old, n_grid, n_grid_old).shape)\n",
    "    \n",
    "    # \"Testing\"\n",
    "    #rs_test, thetas_test = grid(n_grid)\n",
    "    rs_test = (rs[1:] + rs[:-1])/2\n",
    "    thetas_test = (thetas[1:] + thetas[:-1])/2\n",
    "    f_free_test = interp_f_free_test(f_free, n_grid).reshape(-1,)\n",
    "    #plt.matshow(f_free_test.reshape(n_grid-1,4*(n_grid-1)-5))\n",
    "    RS_test, THETAS_test = torch.meshgrid(rs_test, thetas_test)\n",
    "    # Transpose here is very important! Becareful of meshgrid and reshape stuff!\n",
    "    #RS_test = torch.transpose(RS_test,0,1)\n",
    "    #THETAS_test = torch.transpose(THETAS_test,0,1)\n",
    "    z_test = torch.transpose(torch.stack([RS_test.reshape(-1,), THETAS_test.reshape(-1,)]),0,1)\n",
    "\n",
    "    \n",
    "    def error_test(f_free, mode=\"train\"):\n",
    "        #print(f_free.shape)\n",
    "        f_free = f_free.reshape(n_grid-1, 4*(n_grid-1)-5)\n",
    "        f1_free, f2_free, f3_free, f4_free = decompose_free(f_free, n_grid-1)\n",
    "        f1_ = f1(f1_free, n_grid-1)\n",
    "        f2_ = f2(f2_free, n_grid-1)\n",
    "        f3_ = f3(f3_free, n_grid-1)\n",
    "        f4_ = f4(f4_free, n_grid-1)\n",
    "        g_ = g(z_test, a=a)\n",
    "        w_ = w(f1_,f2_,f3_,f4_, n_grid-1)\n",
    "        zp_test = zp(z_test,f2_,f3_)\n",
    "        gp_space = gp(g_, w_)[:,1:,1:].reshape(n_grid-1,n_grid-1,3,3)\n",
    "        \n",
    "        gp_space_target_ = gp_space_target(zp_test).reshape(n_grid-1,n_grid-1,3,3)\n",
    "        if mode == \"train\":\n",
    "            error_ = torch.mean((gp_space-gp_space_target_)[1:-1,:]**2)\n",
    "        else:\n",
    "            error_ = torch.mean((gp_space-gp_space_target_)[1:-1,1:-1]**2)\n",
    "        return error_\n",
    "    \n",
    "    def error_all(f_free, mode=\"train\"):\n",
    "        f_free_test = interp_f_free_test(f_free, n_grid).reshape(-1,)\n",
    "        #print(interp_f_free_test(f_free, n_grid).shape)\n",
    "        return error(f_free, mode=mode) + error_test(f_free_test, mode=mode)\n",
    "    \n",
    "    \n",
    "    # Test and Train at the same time\n",
    "    start = time.time()\n",
    "    lr = 36/n_grid**2\n",
    "    opt = LBFGS({f_free}, lr=lr, max_iter=100, max_eval=None, tolerance_grad=1e-30, tolerance_change=1e-30, history_size=1000, line_search_fn='strong_wolfe')\n",
    "    #opt = LBFGS({f_free}, lr=lr, max_iter=100, max_eval=None, tolerance_grad=1e-30, tolerance_change=1e-30, history_size=1000)\n",
    "    lr = 1e-2*(6/n_grid)**2\n",
    "    #opt = torch.optim.Adam({f_free}, lr=lr, eps=1e-8)\n",
    "    \n",
    "    epochs = 300\n",
    "    switch_epoch = 100\n",
    "    log = 5\n",
    "    best_loss = 1e20\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        if (i+1) % switch_epoch == 0:\n",
    "            for opt_param in opt.param_groups:\n",
    "                lr = lr * 0.5\n",
    "                opt_param['lr'] = lr\n",
    "        \n",
    "        def loss_closure():\n",
    "            opt.zero_grad()\n",
    "            loss = error_all(f_free, mode=\"train\")\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        \n",
    "        def loss_closure2():\n",
    "            opt.zero_grad()\n",
    "            loss = error_all(f_free, mode=\"evaluate\")\n",
    "            loss.backward()\n",
    "            return loss\n",
    "          # -------------------------------------------\n",
    "        loss = loss_closure()\n",
    "        #print(\"loss_0={}\".format(loss.detach().numpy()))\n",
    "        #loss2 = loss_closure2()\n",
    "        opt.step(loss_closure)  # get loss, use to update wts\n",
    "        #loss = loss_closure()\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            #best_loss2 = loss2\n",
    "            best_epoch = i\n",
    "            best_free = f_free.clone()\n",
    "        if i % log == 0:\n",
    "            print(\"Epoch: {}\".format(i) + \" | \" + \"Loss: {}\".format(loss.detach().numpy()))\n",
    "        losses.append(loss.detach().numpy())\n",
    "    end = time.time()\n",
    "    print(\"time={}\".format(end-start))\n",
    "    times.append(end-start)\n",
    "    errors.append(best_loss.detach().numpy())\n",
    "    print(\"error={}\".format(loss.detach().numpy()))\n",
    "    print(\"best loss={}\".format(best_loss.detach().numpy()))\n",
    "    #print(\"best loss2={}\".format(best_loss2.detach().numpy()))\n",
    "    print(\"best epoch={}\".format(best_epoch))\n",
    "    #errors.append(error_all(f_free))\n",
    "    ii = ii + 1\n",
    "    n_grid_old = n_grid\n",
    "    torch.save(best_free, './results_grid/params_grid_bfgs_lrdecay_randominit_seq_a_%.3f_n_%d'%(a,n_grid))\n",
    "np.save('./results_grid/loss_grid_bfgs_lrdecay_randominit_seq_a_%.3f'%a, np.array([n_grids, errors, times]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.00000000e+00, 8.00000000e+00, 1.20000000e+01, 1.60000000e+01,\n",
       "        2.00000000e+01, 2.40000000e+01, 2.80000000e+01, 3.20000000e+01,\n",
       "        3.60000000e+01, 4.00000000e+01, 4.40000000e+01, 4.80000000e+01],\n",
       "       [1.73825595e-06, 8.05063004e-07, 2.43446744e-07, 7.75711281e-08,\n",
       "        5.89721503e-08, 6.91021239e-08, 8.92719999e-08, 1.12255285e-07,\n",
       "        1.22538481e-07, 1.15967855e-08, 7.73480541e-09, 1.63489168e-08],\n",
       "       [3.23636568e+02, 3.73661321e+02, 4.33918956e+02, 4.77884859e+02,\n",
       "        3.75687454e+02, 4.21396048e+02, 4.57302025e+02, 4.90541560e+02,\n",
       "        4.61483179e+02, 5.54326555e+02, 4.82969712e+02, 4.41892482e+02]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('./results_grid/loss_grid_bfgs_lrdecay_randominit_seq_a_%.3f.npy'%a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'error')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAETCAYAAADzrOu5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmdUlEQVR4nO3deXyV1b3v8c8vIyEjYyYIswlUK9EoEcGqR4tVEUqd0NaiKNbW2uEeWj099/bc9vZYS8/trUNVcJ6g1loUJ9QqCghIEBUkREGZEggIJmEIZFr3jyScEBLCZg9P9t7f9+uVF+TJs5/9iz7km7XWs9Yy5xwiIiL+iPG6ABERCX8KExER8ZvCRERE/KYwERERvylMRETEbwoTERHxW5zXBXilb9++bvDgwV6XISISVlatWvWlc65f++NRGyaDBw+mpKTE6zJERMKKmW3u6Li6uURExG8KExER8VvUhYmZTTSz2dXV1V6XIiISMaIuTJxzC5xzM9LT070uRUQkYkTtAPyJmL+6nFkLy6ioqiUnI4mZE/KZXJjrdVkiIp5TmByn+avLueP5NdTWNwJQXlXLHc+vAVCgiEjUi7purhM1a2HZ4SBpVVvfyKyFZR5VJCLSfShMjlNFVa1Px0VEoonC5DjlZCR1eLxvSmKIKxER6X4UJsdp5oR8kuJjjzq+a98h7ny1lIPtusBERKKJBuCPU+sge9unuW49fxgfba3mwXc+55+lO/njFacyemCGt4WKiHjAonUP+KKiIheotbne+XQXt//9YyprDjLjnGH89IIR9OigFSMiEu7MbJVzrqj9cXVzBcA3TurHwp+dwxWnD+SBdzZy6T1L+HBrlddliYiEjMIkQNJ6xHPX5V/nsevPYP+hBqb8ZSl3vbZeYykiEhUUJgF2bn7/w62U+xdtZKJaKSISBRQmQdC2lbKvTSvlUINaKSISmRQmQdTaSrn89AHcv2gjl969hI/UShGRCKQwCbK0HvH84fJTefT6M9h7sIFvq5UiIhFIYRIi56mVIiIRTGESQulJR7ZSptz/Hn9QK0VEIoDCxAOtrZQphbn8Ra0UEYkAChOPpCfFM+uKU3l02n+3UmYtVCtFRMKTwsRj5xX8dyvlvreb56V8vK3K67JERHyiMOkG2rZSqmvr+fZf1EoRkfASEWFiZjFm9jszu8fMvu91PSfqvIL+vP6zb/DtllbKZfcsVStFRMKC52FiZo+Y2U4zW9vu+EVmVmZmG8zs9i4uMwnIBeqBbcGqNRTSk+L5Y0srpaq2Tq0UEQkLnocJ8BhwUdsDZhYL3Ad8CxgFTDWzUWZ2ipm91O6jP5APLHPO/Ry4JcT1B8V5Bf15/adHtlLWbKv2uiwRkQ55HibOuXeBPe0OnwlscM597pyrA+YBk5xza5xzl7b72Elza+SrltdGzK/w6T2bWymPTCuiqraOyX9Zyh8XlqmVIiLdjudh0olcYGubz7e1HOvM88AEM7sHeLezk8xshpmVmFnJrl27AlNpCJxfkMnrP/0Gk0fncu/bG9RKEZFup7uGiXVwrNMtIZ1zB5xz051zP3bO3XeM82Y754qcc0X9+vULSKGhkt4znv+68shWyn+9XkZdQ5PXpYmIdNsw2QYMbPP5AKDCo1q6lbatlHve2sBl9y5hbblaKSLire4aJiuBEWY2xMwSgKuBFwNxYTObaGazq6vD9wdwayvl4e8XsWd/HZPuUytFRLzleZiY2VxgGZBvZtvMbLpzrgG4FVgIlALPOuc+CcT7OecWOOdmpKenB+JynvqXkZm88bNvMGl0jlopIuIpc67ToYiIVlRU5EpKSrwuI2D+WVrJHc+vYff+Oi4o6Mea8hq2Vx8kJyOJmRPymVx4rOcXRESOj5mtcs4VtT/uectEAqO1lVI4MJ2F63ZSUX0QB5RX1XLH82uYv7rc6xJFJIIpTCJIes94tlcfOup4bX0jsxaWeVCRiESLqAuTSBiAP5aKqlqfjouIBELUhUkkDcB3JCcjqcPjZvDexi9DXI2IRIuoC5NIN3NCPknxsUccS4yLoU9KAtc+tIL/+8anNDTqEWIRCSyFSYSZXJjLnVNOITcjCQNyM5K46ztfZ9G/nsd3ThvA3f/8jGvmrGB7tbq9RCRw9GhwlPnH6m386h9rSYiL4Y+Xn8oFozK9LklEwogeDW4R6QPwXfl24QBe+vE4cjOSuPGJEv73gk+0CrGI+C3qwiTSB+CPx9B+KTz/w7FMGzuYR5du4jv3v8cXX+73uiwRCWNRFybSLDEulv+47GvMua6IbV/VcundizWxUUROmMIkyl04KpNXbhvPqJw0fvrXD/nXv33EgboGr8sSkTCjMBFyMpKYe1Mxt50/nL9/sI1L71nCuooar8sSkTASdWES7QPwnYmLjeHn38zn6RvHsO9gA5P/spQnl20iWp/2ExHfRF2YaAD+2MYO68srPxnP2GF9+J8vfMItT31A9YF6r8sSkW4u6sJEutY3JZFHvn8Gv7p4JG+WVnLx3YtZtXmP12WJSDemMJEOxcQYN50zlOduGUtMDFz54HLue3sDTU3q9hKRoylM5JhGD8zg5dvGc9HJWcxaWMZ1j7zPzr0HvS5LRLoZhYl0Ka1HPPdOLeT3U06hZPMeLv7zYt79dJfXZYlIN6IwkeNiZlx9Zh4v3jqO3skJXPfI+9z12nrqtQKxiBCFYaJHg/1zUmYqL/xoHFPPzOP+RRu58sFlbN1zwOuyRMRjURcmejTYf0kJsdw55RTumVrIhsp9XHL3Yl5ds93rskTEQ1EXJhI4E0/N4eXbxjOkbzK3PP0B/z5/DQfrtQKxSDRSmIhf8vr05G8/GMuMc4by1PItTL5vKRt27vW6LBEJMYWJ+C0hLoZ/u3gkj15/Bjv3HmLiPUt5tmSrlmIRiSIKEwmY8/L78+pPxjN6YAa/eO5jfvrXD9l3SCsQi0QDhYkEVGZaD566cQz/48KTWPBRBZfevZg12/TknEikU5hIwMXGGD/+lxHMm3EWhxqamHL/Uh5e8oW6vUQiWNSFieaZhM6ZQ3rzym3j+cZJ/fntS+u48fES9uyv87osEQmCqAsTzTMJrV7JCcy57nT+Y+IoFn/2JRf/eTErPt/tdVkiEmBRFyYSembGtLOH8PwPx5KUEMvUOcv585uf0agViEUihsJEQubk3HQW/Hgck0bn8qc3P+WaOcvZUa0ViEUigcJEQiolMY4/XTWaP15xKh9vq+biuxfz1vpKr8sSET8pTMQTl58+gJduG0dmWg9ueKyE//PSOuoatAKxSLhSmIhnhvVL4R8/HMt1Zw3ioSVfcPkD77F5936vyxKRE6AwEU/1iI/lN5NO5oHvns6mL/dzyd1LePGjCq/LEhEfKUykW7jo5Cxe+cl48rNSuW3uan753MccqNNSLCLhQmEi3caAXj3564xifnTeMJ5dtZXL7l3K+h01XpclIsdBYSLdSlxsDDMnFPDkDWOoOlDPpHuX8vSKzVqKRaSbi7ow0XIq4WHciL68+pPxnDmkN7/6x1pufWY11bX1XpclIp2IujDRcirho19qIo9ffya/vKiA1z7ZwSV3L2b1lq+8LktEOhB1YSLhJSbGuOXcYTx781k4B1c8sIwH3tlIk5ZiEelWFCYSFk4f1ItXfjKeC0dl8vtX1zPtsZV8ue+Q12WJSAuL1oHNoqIiV1JS4nUZ4iPnHE+v2MJvXlpHelI8/++q0ezae4hZC8uoqKolJyOJmRPymVyY63WpIhHJzFY554raH4/zohiRE2VmfLd4EKcP6sWtz3zAtQ+tIC7GaGjp9iqvquWO59cAKFBEQkjdXBKWRmanseDH4+iZEHs4SFrV1jcya2GZR5WJRCeFiYStnglx1NY1dvi1iqraEFcjEt0UJhLWcjKSfDouIsGhMJGwNnNCPknxsUccM2DGOUO8KUgkSilMJKxNLszlzimnkJuRhAF9UxJIiDUee2+zdnEUCSE9GiwRZ9Xmr/j+I+/TNyWBuTOKyU5Xl5dIoHT2aLBaJhJxTh/Uiyemn8nufXVcPXu5BuNFQsCnMDGzt8zst8EqRiRQTsvrxZM3jmHP/uZAKVegiASVry2TYiC2y7O6Ma0aHD1GD8zgqelj+OpAHVc9uIytew54XZJIxPI1TD4DBgajkFDRqsHR5dSBGTx94xhqauu5evZyBYpIkPgaJg8Bl5hZXjCKEQmGrw/I4Jmbitl3qEGBIhIkvobJAmAJsNTMbjWzMWY2yMzy2n8EoVaRE3ZybjpP3ziG/XUNXPXgMrbsVqCIBJJPjwabWRPgaJ4XdqwXOudct15EUo8GR6d1FTVc+9ByesTHMm9GMYP6JHtdkkhYCdSqwU9w7BAR6dZG5aTxzE3FXPvQCq56cDlzZxQzpK8CRcRfmrQoUWn9jhqumbOC+Fhj7k3FDO2X4nVJImFBkxZF2ijISmPuTcU0NDqunr2cjbv2eV2SSFg74TAxswEtcza+Z2aXmdmAQBYmEmz5WanMm1FMk4OrZy9nw04FisiJ8jlMWp7Weg3YDMwHHgP+AWw2s9fMbHAgCxQJphGZqcybMQZ3OFD2el2SSFjydTmVLGAp8E2aw+RJ4A8tf37RcnxJy3kiYWF4/+YWillzoHxaqUAR8ZWvLZP/CeQCvwRGOOemOefucM5NA/KBXwA5wL8HtEqRIBveP4V5M4qJMWPq7OWU7VCgiPjC1zC5BHjdOTfLOXfEfqnOuUbn3B+B14FLA1WgSKgM69ccKHGxxtQ5y1m/o8brkkTChq9hkgWs6uKcVS3niYSdof1SmDfjLBJiY5g6eznrKhQoIsfD1zCpBgZ1cU5ey3kiYWlI32T+enMxSfGxXPvQcj6p0O0s0hVfw2QJcLmZje3oi2Y2Brii5TyRsDWoTzLzZpxFz4Q4rn1oBWvLFSjSsfmryzn7928x5PaXOfv3bzF/dbnXJXnC1zD5Xcuf75jZk2Z2g5l9y8yuN7PHgcUtX//PwJUo4o28Pj2ZN6OY5JZAWbNNgSJHmr+6nDueX0N5VS0OKK+q5Y7n10RloPgUJs65D4DLae7GuhaYA7xE89L03wNqgCudc12Nq4iEhYG9mwMlJTGOax9azsfbqrwuSbqRPyxcT239Ec8iUVvfyKyFZR5V5B2fV/Z1zr1kZoOAScBpQDrN4bIamO+c2x/YEkW8NbB3T/56czFT5yzn2odW8NT0MZw6MMPrsiQA5q8uZ9bCMiqqasnJSGLmhHwmF+Z2eO6+Qw2U7ahhXUUN67bvZd32GiqqDnZ4bkUUbhPtU5iY2SPAGufcn4BnWj5EIt6AXj2ZN+Msps5ezncfWsET08+kMK+X12WJH1q7qFpbFq1dVM45xgztQ+n21uCooXR7DZva7IGT0TOekVlpJCfGsv9Q41HXzslICtn30V34up/JQeBPzrk7gldSaGjVYDkRFVW1TJ2znD376nh8+pmcpkAJW2f//i3KO2hBmEHbH4uD+/RkVE4aI7PSmv/MTiM7vQdmdlQgASTFx3LnlFM6beGEu0DtZ7IJ6B+QikTCUE5GEvNmFDN19nKue/h9Hr/hDE4f1NvrssRHzrlOu6Kcg99OPplR2ankZ6WRktj5j8nWwJi1sIzyqloS42IiOkiOxdenuZ4BvmVm3erXMTMbb2YPmNlDZvae1/VIZMtOT2LejLPol5rIdQ+/T8mmPV6XJMfpYH0jfyvZyqX3LOl0l7/cjCS+VzyI0wf1PmaQtJpcmMvS289nSmEuvXomRGWQgO9hcidQArxtZpeaWaa/BZjZI2a208zWtjt+kZmVmdkGM7v9WNdwzi12zv2A5ifLHve3JpGuZKX3YN6MYjLTenDdI+/z/hcKlO5se3UtsxauZ+zv32Lmcx9T39jEFUUD6BF/5I/ApPhYZk7IP6H3KMhOZUfNQb7aXxeIksOOr91crY8uGPACgJl1dJ4ve8A/BtxL85bAtFwzFrgPuBDYBqw0sxeBWJoDra0bnHM7W/5+DXDjcb6viF8y05oDZeqc5Ux79H0enXYGY4b28bosaeGcY+Wmr3j8vU289skOnHNcMDKTaWMHc9awPpgZZw/re9xPc3UlPysNgPU79nLWsOi7D3wNk8UEeA9459y7HeyBciawwTn3OYCZzQMmOefupJNFJM0sD6h2zmkxJQmZ/mk9mDujmGvmrGDaoyt59PozKFageOpgfSMvfljBY+9tYt32GtKT4rlx3BC+WzyIgb17HnHu5MLcgHVLjcxKBaBsR43CpCvOuXODVEd7ucDWNp9vA8Z08ZrpwKPHOsHMZgAzAPLy8vypT+Sw/qk9mHtTMdfMWc71j67k4WlFjB3W1+uyok5FVS1PLt/MvPe38NWBevIzU5sHw0fnkpQQG/T375eaSO/kBNZH6fYF/swzCaaO+s6O2SJyzv26q4s652YDs6H50eATK03kaP1SE1taKMu54bGVPPz9Mzh7uAIlGI6caNiDbxfmsnHXfl5fV4lzjgtHZTJt7BCKh/burBs+KMyMgqxUSqM0THwdgL+G0DwavA0Y2ObzAUBFCN5X5IT1TUlk7k3FDO6TzA2PrWTJZ196XVLEOXotrIPc+/ZGFpXt5MbxQ3j3F+fx4PeKDo+JhFpBVhqf7thLY1P0/a7qa5hsIjRhshIYYWZDzCwBuBp4MQTvK+KXPimJPHNTMUP6JjP98ZW8++kur0uKKHe+WnrUWlgAvXomcMe3RjKgV88OXhU6Bdmp1NY3smXPga5PjjCezzMxs7nAMiDfzLaZ2XTnXANwK7AQKAWedc59EqD3m2hms6urtQKsBEfv5ATm3lTMsH4p3PhECe8oUPzS2OT4Z2kl0x59n8qaQx2es7264zWyQm1k6xNd26PvOSDP55k456Y657Kdc/HOuQHOuYdbjr/inDvJOTfMOfe7rq7jw/stcM7NSE9PD9QlRY7SKzmBp28cw4j+Kdz0RAlvl+3s+kVyhN37DvGXRRs45w9vM/3xEtZV1JDao+Nh3u6yFtaIzBRijKgcN+kO80xEIlJroHz34RXc/MQqvj92EK+s2RGQOQ2RyjnHB1u+4sllm3llzQ7qGps4a2gffnXJSC4clcnLH2/vcC2sE51oGGg94mMZ3DeZsh3R1zLxfJ6JSCTL6JnA09OLueTuxcxZ/MXh460r1AIKFGD/oQZe+LCCJ5dvpnR7DamJcVwzJo/vFucxvH/q4fParoXVXUN5ZFYaa6Nwq+fuOs8kaMxsIjBx+PDhXpciUSK9ZzyNHazO3bqJUnf6QRhqG3bu46nlm/n7qm3sPdRAQVYq//ntU5g0OofkTtbFCuREw2AoyErl5TXb2X+oodPvIRKd8HdqZsnASUCKc25xV+d3F865BcCCoqKim7yuRaLHjk4GiMurannhw3LOPak/6T3jQ1yVN+obm3hzXSVPLt/Mext3kxAbw8WnZPG9swZxWl4vTx7pDaSC7OZB+LLKvVG1RYHPYWJmA4A/AxNpXivLtV7HzMbRPCnwh865RYErUyS85WQkdbh3RozBT+Z9SFyMceaQ3lwwMpMLR2UetexHuOloB8OzhvVh7vtbmPv+FiprDpGbkcQvLsrnyqKB9E1J9LrkgCloWVZl/XaFSafMLBtYAWTSPO+jP3BWm1NWtBy7ClgUmBJFwt/MCfkdDhz/5+STGdwvmTfWVfJmaSW/eWkdv3lpHQVZqYeD5ZTcdGJiwue39Y52MPz5sx/iXPNvnt84qR+/mzyI8wr6ExtG39fxGtAriZTEONZH2SC8ry2TX9McFhc45xaZ2a9pEybOuXozWwycHcAaRcJeVwPHhXm9+MVFBWzevZ831lXyxrpK/rJoA/e+vYHMtET+ZWQmF47M5KxhfegRH/x1pvxx12vrj5pY2OQgJTGOl28bx6A+yR5VFhpmRn5WKuu3R9fjwb6GycXAi110YW0Bxp9wRUGmAXjxyvEMHA/qk8yN44dy4/ihfLW/jrfLdvJmaSUvrC7nmRVb6JkQyzkj+nHBqEzOL+hP7+SEEFXfscYmx6eVe1m9pYoPt37F6i1VnU4g3H+oIeKDpFVBVioLPqrAORf2Y0DHy9cwyQQ+6+KceqDb3jEagJdw0Ss5gSmnDWDKaQM41NDIso27ebO0kjfX7eS1T3YQY1A0qDcXjsrkglGZDOkb/H92O2sOsnpr1eHw+HhbNQfqmlshvXrGU5jXix01B9l7sOGo13aXiYWhUJCdxtMrtrC9+mDUfN++hskejlyAsSMnATtOrBwR6UhiXCzn5vfn3Pz+/HaSY215DW+s28EbpTv53Sul/O6VUob3T2kZZ+nP6IG9/B6POFjfyNryaj48HB5Vhx8iiI81RmWncWXRQEYPzKAwL4O83j0xs6PGTKB7TSwMhda9TdbvqFGYdGIpcJmZZTnnjgoMMxsBXAQ8FYjiRORoZsYpA9I5ZUA6P/9mPlv3HOCfpZW8WbqThxZ/zgPvbKRvSgLnF/TnwlFZjBvel6SE2A6fsGrtdnPOsWn3AVZv+epweJRur6GhZfXbAb2SKMzL4IZxQxg9MIOv5aR1OnYTDhMLg+2kljAp3b6X8wv8XnUqLJjrYDJVpyebjQGWAJ8DPwXOBf4VSAPOAf4EDAZOD9TCjMFSVFTkSkpKvC5DJKCqa+t559NdvLGukkXrd7L3UAM94mMY1i+FTyv3Ut/43//eE2JjOL+gHwcbmvhwaxVVB+oBSE6I5esDmlsbhXm9GD0wg36pkfPobqiMu+stCvN6cc/UQq9LCSgzW+WcK2p/3NcZ8Ctadit8AHipzZdan4FroHlP9m4bJBqAl0iWnhTPZafmcNmpOdQ1NPH+F3t4s7SSJ5dtPmoWfl1jE699Ukl+ZioTRmVRmJfB6LwMRvRPjchHdkOtICstqlYP9nnSonPuUTNbAvwQKAb6ANXAcuBe51xZYEsMLA3AS7RIiIth3Ii+jBvRl8ff29ThOQYs/Nk5Ia0rWhRkpfJ22U4ONTSSGNe9H+cOhBNaTsU59xnwswDXIiJB0tkM/GgZHPZCQXYqjU2ODTv38bWcyN/ywtf9TEQkDM2ckE9SuwHzaHvCKtQKDm+UFR2TF6NnSUuRKKYnrEJvcJ+eJMbFRM2yKgoTkSjR3ZdujzRxsTGclJnK+ijZdVHdXCIiQVKQlUpplHRzRV2YmNlEM5tdXR19O6GJSGgVZKfx5b5D7Np7yOtSgi7qwsQ5t8A5NyM9PfKfrhARb7XubVIWBV1dURcmIiKhUtBmja5IpzAREQmSPimJ9EtNjIpBeIWJiEgQFWSlqmUiIiL+GZmdxqeV+2hobPK6lKBSmIiIBFFBVip1DU1s2r3f61KCSmEiIhJErcuqRPp8k6gLE80zEZFQGtY/mdgYi/hxk6gLE80zEZFQSoyLZVi/5Ihf8DHqwkREJNQKstIi/vFghYmISJAVZKdSXlVLzcF6r0sJGoWJiEiQjWwZhI/kZVUUJiIiQVaQ3bKsSgTvCa8wEREJsqy0HqQnxVOqlomIiJwoMyM/K1UtExER8c/IrFTKduylqcl5XUpQKExEREKgIDuN/XWNlFfVel1KUChMRERCoHVvk9II7eqKujDRcioi4oWTMlMxI2InL0ZdmGg5FRHxQnJiHIN694zYNbqiLkxERLxSkJUWsWt0KUxEREIkPyuVL3bvp7au0etSAk5hIiISIiOzU3EOPq2MvNaJwkREJEQKIniNLoWJiEiI5PXuSVJ8LKUROAivMBERCZGYmNZlVdQyERERP4zMTmX9jhqci6xlVRQmIiIhVJCVxlcH6tm595DXpQSUwkREJITyI3RZFYWJiEgIta7RFWnLqihMRERCKKNnAtnpPSLu8WCFiYhIiBVkpaqbK9xp1WAR8VpBdhobd+2jrqHJ61ICJurCRKsGi4jXCrJSqW90fP7lPq9LCZioCxMREa+NzG5eViWSJi8qTEREQmxI32TiYy2illVRmIiIhFh8bAzD+0fWsipxXhcgIhKNRmal8t7G3SF7v/mry5m1sIyKqlpyMpKYOSGfyYW5Abu+WiYiIh4oyE5lR81BvtpfF/T3mr+6nDueX0N5VS0OKK+q5Y7n1zB/dXnA3kNhIiLigda9TUIxE37WwjJq64/c3bG2vpFZC8sC9h4KExERDxRkty6rEvxB+IqqWp+OnwiFiYiIB/qlJNInOSEkg/A5GUk+HT8RChMREQ+YtWyUFYKWycwJ+cSaHXEsKT6WmRPyA/YeChMREY8UZKVRVrmXxqbgbpQ1ZmhvHI7khFgMyM1I4s4ppwT0aS49Giwi4pGC7FQO1jexefd+hvZLCdr7PLLkC8yM1356DgN79wzKe6hlIiLikZEtT3QFczn66gP1PLNiC5d+PTtoQQIKExERz4zITCHGoDSIYfLk8k3sr2vk5nOGBe09QGEiIuKZHvGxDOmbzPog7W1ysL6RR5du4tz8fozKSQvKe7RSmIiIeKggOy1oExf/VrKV3fvr+ME3gtsqAYWJiIinXJNjy54DDLn9Zc7+/VsBW+KkobGJ2Ys/Z/TADMYM6R2Qax6LwkRExCPzV5fzZulOgICvmfXK2h1s3VPLLecOw9rNMQkGhYmIiEdmLSyjrvHIrXsDsWaWc477F21kWL9kLhyZ6de1jpfCRETEI8FaM+vdz76kdHsNN58zjJiY4LdKQGEiIuKZYK2Z9cCijWSl9WBSYY5f1/FFRISJmeWZ2Ytm9oiZ3e51PSIix2PmhHyS4mOPOGYGPzr/xJ+++nBrFcs+3830cUNIjIvt+gUB4nmYtATATjNb2+74RWZWZmYbjiMgTgJeds7dAIwKWrEiIgE0uTCXO6ecQm5GEgb0SU7AgL+vKudAXcMJXfOBRRtJ6xHH1DF5Aa21K91hba7HgHuBJ1oPmFkscB9wIbANWGlmLwKxwJ3tXn8DsBr4lZldBTwZgppFRAJicmHuEQsuvrpmOz965gNufnIVD32/yKfWxcZd+1i4bgc/Onc4KYmh/fHuecvEOfcusKfd4TOBDc65z51zdcA8YJJzbo1z7tJ2HzuB64FfO+fOBy4J7XcgIhI43zolm7u+83UWf/YlP35mNQ3tnvY6ltnvfE5CbAzTzh4cvAI74XmYdCIX2Nrm820txzrzGnCbmT0AbOrsJDObYWYlZlaya9eugBQqIhJoVxQN5D8mjuL1dZX84rmPaTqOJeoraw7yj9XlXFE0gL4piSGo8kjdoZurIx09y9bpf03n3Frg8q4u6pybDcwGKCoqCu4GAiIifph29hD2HWrgj69/SnJiHL+Z9LVjTj58ZMkXNDQ1MWN88JdO6Uh3DZNtwMA2nw8AKjyqRUTEEz86bzh7DzXw4Dufk9Ijjl9eVNDhedW19Ty9YguXfD2HvD7BW2b+WLprmKwERpjZEKAcuBq4xtuSRERCy8y4/aIC9h1s4P5FG0lJjONH5w0/6rynlm9m36EGbj5nqAdVNvN8zMTM5gLLgHwz22Zm051zDcCtwEKgFHjWOfdJgN5vopnNrq6uDsTlRESCysz47aSTmTw6h1kLy3hi2aYjvt66zPz4EX05OTfdmyLpBi0T59zUTo6/ArwShPdbACwoKiq6KdDXFhEJhpgYY9YVp7K/rpH/9cInJCfE8Z3TBwDw3KptfLnvELecO9rTGj0PExER6Vp8bAz3TC1k+uMrmfncR6ytqGLhJ5VUVB0kPtaorD7oaX2ed3OJiMjx6REfy+zvFTGwVxKPLt1MRVVzgNQ3Ov7tH2sDthfKiYi6MNGYiYiEs+TEOOoaj57ZEIil6/0RdWHinFvgnJuRnu7dQJWIiD92dNKl5e/S9f6IujAREQl3wVq63h8KExGRMNPR0vVJ8bHMnJDvUUV6mktEJOy0rjI8a2EZFVW15GQkMXNC/hGrD4da1IWJmU0EJg4ffvQsUhGRcNF+6XqvRV03lwbgRUQCL+rCREREAk9hIiIiflOYiIiI36IuTDQDXkQk8My56Nxw0Mx2AZuBdCDQyeLvNU/k9b6+5njP7+q8rr7eF/jSh7q6s2DcK168ZyCuGex7NFD3Z1fn6P703SDnXL+jjjrnovoDmN3drnkir/f1Ncd7flfnHcfXS7z+f9xd/r92l/cMxDWDfY8G6v7s6hzdn4H7iLpurg4s6IbXPJHX+/qa4z2/q/OC8d+vu/Lie+2O9+eJXsOX1wTq/vT1fcOZp99n1HZzSWiYWYlzrsjrOkQ6ovszcNQykWCb7XUBIseg+zNA1DIRERG/qWUiIiJ+U5iIiIjfFCYiIuI3hYmElJlNNrM5ZvaCmX3T63pE2jKzkWb2gJk9Z2a3eF1POFGYiN/M7BEz22lma9sdv8jMysxsg5ndDuCcm++cuwmYBlzlQbkSZXy8P0udcz8ArgT0yLAPFCYSCI8BF7U9YGaxwH3At4BRwFQzG9XmlH9v+bpIsD2GD/enmV0GLAH+Gdoyw5vCRPzmnHsX2NPu8JnABufc5865OmAeMMma3QW86pz7INS1SvTx5f5sOf9F59xY4NrQVhreom7bXgmZXGBrm8+3AWOAHwMXAOlmNtw594AXxUnU6/D+NLNzgSlAIvBK6MsKXwoTCRbr4Jhzzt0N3B3qYkTa6ez+XAQsCm0pkUHdXBIs24CBbT4fAFR4VItIe7o/A0xhIsGyEhhhZkPMLAG4GnjR45pEWun+DDCFifjNzOYCy4B8M9tmZtOdcw3ArcBCoBR41jn3iZd1SnTS/RkaWuhRRET8ppaJiIj4TWEiIiJ+U5iIiIjfFCYiIuI3hYmIiPhNYSIiIn5TmIiIiN8UJiIeM7M3zMy1fJzWyTlzWr7+7VDXJ3I8FCYi3msbIFd2ck7rRk0lQa5F5IRoBryIh8xsGLCB5pDIAQ4654a1OycR2At85ZzLDH2VIl1Ty0TEW60tjpXA34GhZtZ+u9jRQDxqlUg3pjAR8Vbb7qu/tfy9fVeXurik21OYiHirbVAspXlPjSuOcY5It6QwEfGImRlQCNQC65xzTTR3dQ02szPbnKowkW5PYSLinZOAdOCjlv01oF1Xl5n1BEYCFc657aEvUeT4KExEvNNRi2MpsB24ok3LJRa1SqSbU5iIeKc1TFa1HmjT1ZUHjEFdXBImFCYi3uksKNp2dSlMJCxo0qKIB8wsBqim+Re6NOdcY7uvbQMagX1AAdDfObfLi1pFjodaJiLeGAmkAB+2DRI43NX1PDCA5iDZoiCR7k5hIuKN01v+7Kz76m9t/q4uLun21M0lIiJ+U8tERET8pjARERG/KUxERMRvChMREfGbwkRERPymMBEREb8pTERExG8KExER8ZvCRERE/KYwERERv/1/SVwantiie7cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_grids = np.array(n_grids)\n",
    "#n_params = (4*n_grids-5)*n_grids\n",
    "n_params = n_grids**2\n",
    "#n_params = (n_grids-1)**2\n",
    "plt.plot(n_params, errors, marker=\"o\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "#plt.ylim(1e-8,10e-5)\n",
    "plt.xlabel(r\"$N$\",fontsize=20)\n",
    "plt.ylabel(\"error\",fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9201069227039909"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_id = -6\n",
    "end_id = -1\n",
    "\n",
    "(np.log(errors[start_id]) - np.log(errors[end_id]))/(np.log(n_params[start_id])-np.log(n_params[end_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcc3e5b6dc0>]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import scipy\n",
    "\n",
    "start_id = -6\n",
    "end_id = -1\n",
    "loss_w = errors[start_id:end_id]\n",
    "log_ws = np.log(n_params[start_id:end_id])\n",
    "log_loss = np.log(errors[start_id:end_id])\n",
    "\n",
    "reg = LinearRegression().fit(log_ws[:,np.newaxis], log_loss)\n",
    "eps_max = np.min(loss_w)*0.999\n",
    "\n",
    "num_sweep = 10001\n",
    "eps0_sweep = np.linspace(0, eps_max, num=num_sweep)\n",
    "scores = []\n",
    "\n",
    "for i in range(num_sweep):\n",
    "    score = np.abs(scipy.stats.pearsonr(log_ws, np.log(loss_w-eps0_sweep[i]))[0])\n",
    "    scores.append(score)\n",
    "    \n",
    "plt.plot(eps0_sweep, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0\n",
      "-1.9184066707629372\n",
      "0.0033148606783546743\n"
     ]
    }
   ],
   "source": [
    "max_id = np.argmax(scores)\n",
    "eps0 = eps0_sweep[max_id]\n",
    "reg.fit(log_ws[:,np.newaxis], np.log(loss_w-eps0))\n",
    "alpha = reg.coef_[0]\n",
    "A = np.e**reg.intercept_\n",
    "print(max_id)\n",
    "print(eps0)\n",
    "print(alpha)\n",
    "print(A)\n",
    "#[eps0, A, alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 5.549117469329293e-23\n",
       " hess_inv: array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]])\n",
       "      jac: array([1.49027668e-08, 9.45245568e-18, 2.01377313e-19])\n",
       "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
       "     nfev: 236\n",
       "      nit: 0\n",
       "     njev: 56\n",
       "   status: 2\n",
       "  success: False\n",
       "        x: array([ 0.        ,  0.00331486, -1.91840667])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def f(x):\n",
    "    eps0 = x[0]\n",
    "    A = x[1]\n",
    "    alpha = x[2]\n",
    "    return np.mean((loss_w-(A*n_params[start_id:end_id]**alpha+eps0))**2)\n",
    "\n",
    "x0 = np.array([eps0, A, alpha])\n",
    "sol = minimize(f, x0, tol=1e-32, options={'gtol':1e-30})\n",
    "sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
